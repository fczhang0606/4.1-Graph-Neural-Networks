################################################################################################################################
论文阅读：
https://arxiv.org/pdf/1403.6652.pdf

https://www.bilibili.com/video/BV1K5411H7EQ?p=3&vd_source=26c583b46dbb1b1b34ae4743b60cf76f
https://zhuanlan.zhihu.com/p/56380812

https://www.bilibili.com/video/BV1pW4y1z7pb/?spm_id_from=333.788&vd_source=26c583b46dbb1b1b34ae4743b60cf76f
################################################################################################################################


################################################################################################################################
环境搭建：
conda create -n GNN_DeepWalk
conda activate GNN_DeepWalk

conda install python=3.8

https://pytorch.org/get-started/previous-versions/
conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge
################################################################################################################################
conda install ipykernel
conda install platformdirs
pip3 install ipywidgets
pip3 install --upgrade jupyter_core jupyter_client

python -m ipykernel install --user --name GNN_DeepWalk
################################################################################################################################


################################################################################################################################
代码复现：
1\Deep Walk in karateclub

! pip install karateclub

import karateclub
from karateclub import DeepWalk
import networkx as nx
import sklearn
from sklearn.cluster import KMeans

G = nx.karate_club_graph()
G.nodes[1]
G.nodes[23]

model = DeepWalk()
model.fit(G)

G_embedding = model.get_embedding()
G_embedding.shape

num_coms = 2
clusters = KMeans(n_clusters=num_coms).fit_predict(G_embedding)
print(clusters)
################################################################################################################################


################################################################################################################################
2\ZZ's Deep Walk

import torch
import torch.nn as nn
import random

adj_list = [
[1,2,3], 
[0,2,3], 
[0, 1, 3], 
[0, 1, 2], 
[5, 6], 
[4,6], 
[4, 5], 
[1, 3]]
size_vertex = len(adj_list)

w=3  # 单边窗口长度，包括中心词
d=2  # 中间维度
y=200  # 每节点采样次数
t=6  # 每节点采样长度
lr=0.025
v=[0, 1, 2, 3, 4, 5, 6, 7]

model = Model()
for i in range(y) :
    random.shuffle(v)  # 原位打乱列表
    for vi in v :
        wvi = RandomWalk(vi, t)
        skip_gram(wvi, w)

print(model.phi1)
print(model.phi2)
################################################################################################################################
class Model(torch.nn.Module) :

    def __init__(self) :
        super(Model, self).__init__()
        self.phi1 = nn.Parameter(torch.rand((size_vertex, d), requires_grad=True))  # 8to2
        self.phi2 = nn.Parameter(torch.rand((d, size_vertex), requires_grad=True))  # 2to8 softmax

    def forward(self, hot) :
        hid = torch.matmul(hot, self.phi1)
        out = torch.matmul(hid, self.phi2)
        return out
################################################################################################################################
def RandomWalk(node, t) :

    walk = [node]

    for i in range(t-1) :
        node = adj_list[node][random.randint(0, len(adj_list[node])-1)]  # 等概率
        walk.append(node)

    return walk
################################################################################################################################
def skip_gram(wvi, w) :

    for j in range(len(wvi)) :

        for k in range( max(0, j-w), min(j+w, len(wvi)) ) :
        # 双重循环，制作语料

            # 中心词的one-hot
            one_hot = torch.zeros(size_vertex)
            one_hot[wvi[j]] = 1

            out = model(one_hot)  # 预测共现词的概率分布
            # print(out)

            loss = torch.log(torch.sum(torch.exp(out))) - out[wvi[k]]  # 做了j*k次loss回传
            loss.backward()  # 回传优化参数

            for param in model.parameters() :
                param.data.sub_(lr*param.grad)
                param.grad.data.zero_()
################################################################################################################################


################################################################################################################################
3\Wiki            https://www.bilibili.com/video/BV1et4y187Gd/?spm_id_from=333.788&vd_source=26c583b46dbb1b1b34ae4743b60cf76f
################################################################################################################################
# ! pip install networkx
# ! pip install gensim

# ! pip install scikit-learn

# ! pip install matplotlib
# ! pip install numpy
# ! pip install pandas
# ! pip install tqdm
################################################################################################################################
import networkx as nx

import matplotlib.pyplot as plt
# %:magic function
%matplotlib inline
plt.rcParams['font.sans-serif'] = ['SimHei']  # 显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 显示负号

import numpy as np
import pandas as pd
import random
from tqdm import tqdm
################################################################################################################################
https://densitydesign.github.io/strumentalia-seealsology

https://en.wikipedia.org/wiki/Ren%C3%A9_Descartes
    31 03 1596 – 11 02 1650

https://en.wikipedia.org/wiki/Immanuel_Kant
    22 04 1724 – 12 02 1804

https://en.wikipedia.org/wiki/Georg_Wilhelm_Friedrich_Hegel
    27 08 1770 – 14 11 1831

https://en.wikipedia.org/wiki/Arthur_Schopenhauer
    22 02 1788 – 21 09 1860

https://en.wikipedia.org/wiki/Karl_Marx
    05 05 1818 – 14 03 1883

https://en.wikipedia.org/wiki/Sigmund_Freud
    06 05 1856 – 23 09 1939

https://en.wikipedia.org/wiki/Ayn_Rand
    02 02 1905 – 06 03 1982
################################################################################################################################
df = pd.read_csv("seealsology-data.tsv", sep="\t")
df.shape  # (6935, 3)
# df.head()
################################################################################################################################
G = nx.from_pandas_edgelist(df, "source", "target", edge_attr=True, create_using=nx.Graph())
len(G)  # 4865
all_nodes = list(G.nodes())
# all_nodes
################################################################################################################################
def get_randomwalk(node, path_length) :

    random_walk = [node]

    for i in range(path_length - 1) :

        temp = list(G.neighbors(node))
        temp = list(set(temp) - set(random_walk))  # set，无序不重复元素集
        # 当前节点的未访问过的邻居
        if len(temp) == 0 :
            break

        next_node = random.choice(temp)
        random_walk.append(next_node)
        node = next_node

    return random_walk

#
get_randomwalk('sigmund freud', 6)
################################################################################################################################
random_walks = []
r = 20
l = 6

for n in tqdm(all_nodes) :
    for i in range(r) :
        random_walks.append(get_randomwalk(n, l))

#
len(random_walks)
# random_walks[1]
################################################################################################################################
from gensim.models import Word2Vec
model = Word2Vec(
                 vector_size=48,  # Embedding维数
                 window=3,  # 窗口宽度
                 sg=1,  # Skip-Gram
                 hs=0,  # 不加分层softmax
                 negative=10,  # 负采样
                 alpha=0.03,  # 初始学习率
                 min_alpha=0.0007,  # 最小学习率
                 seed=14  # 随机数种子
                )

model.build_vocab(random_walks, progress_per=2)
model.train(random_walks, total_examples=model.corpus_count, epochs=200, report_delay=1)
model.wv.get_vector('sigmund freud').shape
model.wv.get_vector('sigmund freud')
model.wv.similar_by_word('sigmund freud')
################################################################################################################################
X = model.wv.vectors
################################################################################################################################
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
embed_2d = pca.fit_transform(X)
embed_2d.shape

plt.figure(figsize=(14, 14))
plt.scatter(embed_2d[:, 0], embed_2d[:, 1])
plt.show()

term = 'sigmund freud'
term_xd = model.wv[term].reshape(1, -1)
# term_xd.shape  # (1, 48)

term_2d = pca.transform(term_xd)
# term_2d

plt.figure(figsize=(14, 14))
plt.scatter(embed_2d[:, 0], embed_2d[:, 1])
plt.scatter(term_2d[:, 0], term_2d[:, 1], c='r', s=200)
plt.show()
################################################################################################################################
pagerank = nx.pagerank(G)
node_importance = sorted(pagerank.items(), key=lambda x:x[1], reverse=True)

n = 10
terms_chosen = []
for each in node_importance[:n] :
    terms_chosen.append(each[0])
# terms_chosen.extend(['computer vision', 'deep learning'])
terms_chosen

# 词典中词条的索引号
term2index = model.wv.key_to_index
# term_index = np.array(term2index.values())
# index2term = model.wv.index_to_key

plt.figure(figsize=(14, 14))
plt.scatter(embed_2d[:, 0], embed_2d[:, 1])

for item in terms_chosen :
    idx = term2index[item]
    plt.scatter(embed_2d[idx, 0], embed_2d[idx, 1], c='r', s=50)
    plt.annotate(item, xy=(embed_2d[idx, 0], embed_2d[idx, 1]), c='k', fontsize=12)
plt.show()
################################################################################################################################
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, n_iter=1000)
embed_2d = tsne.fit_transform(X)

plt.figure(figsize=(14, 14))
plt.scatter(embed_2d[:, 0], embed_2d[:, 1])
plt.show()

#
plt.figure(figsize=(14, 14))
plt.scatter(embed_2d[:, 0], embed_2d[:, 1])

for item in terms_chosen :
    idx = term2index[item]
    plt.scatter(embed_2d[idx, 0], embed_2d[idx, 1], c='r', s=50)
    plt.annotate(item, xy=(embed_2d[idx, 0], embed_2d[idx, 1]), c='k', fontsize=12)
plt.show()
################################################################################################################################
terms_chosen_mask = np.zeros(X.shape[0])
for item in terms_chosen :
    idx = term2index[item]
    terms_chosen_mask[idx] = 1

df = pd.DataFrame()
df['X'] = embed_2d[:, 0]
df['Y'] = embed_2d[:, 1]
df['item'] = model.wv.index_to_key
df['pagerank'] = pagerank.values()
df['chosen'] = terms_chosen_mask

df.to_csv('tsne_vis_2d.csv', index=False)

df
################################################################################################################################
from sklearn.manifold import TSNE
tsne = TSNE(n_components=3, n_iter=1000)
embed_3d = tsne.fit_transform(X)

df = pd.DataFrame()
df['X'] = embed_3d[:, 0]
df['Y'] = embed_3d[:, 1]
df['Z'] = embed_3d[:, 2]
df['item'] = model.wv.index_to_key
df['pagerank'] = pagerank.values()
df['chosen'] = terms_chosen_mask

df.to_csv('tsne_vis_3d.csv', index=False)

df
################################################################################################################################
# echarts的3d可视化
################################################################################################################################

