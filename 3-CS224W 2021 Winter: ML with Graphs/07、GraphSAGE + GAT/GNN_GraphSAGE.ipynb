{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cb11a9-7b12-49a4-a061-50ca63691e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://codeleading.com/article/65265993022/\n",
    "# https://zhuanlan.zhihu.com/p/360854229\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable  # 利用Variable定义一个计算图，可以实现自动求导\n",
    "from torch.nn import init  # 初始化参数\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from collections import defaultdict  # 替代容器\n",
    "from sklearn.metrics import f1_score  # 模型、参数的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697a3e3a-548e-47a1-9a4d-da7f22a6d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAggregator(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, features, gcn=False, cuda=False) :\n",
    "\n",
    "        super(MeanAggregator, self).__init__()\n",
    "\n",
    "        self.features = features  # 2708*1433，，724*128\n",
    "        self.gcn = gcn  # 是否汇聚本地特征\n",
    "        self.cuda = cuda  # 是否使用cuda\n",
    "\n",
    "\n",
    "    def forward(self, nodes, neighs, num_sample=10) :  # 节点集，每个节点的邻居集，采样数量\n",
    "\n",
    "        # print(nodes)\n",
    "        # print(len(nodes))  # 256/724\n",
    "        # print(neighs)\n",
    "        # print(len(neighs))  # 256/724\n",
    "\n",
    "        _set = set  # 定义操作，创建空集合\n",
    "        if not num_sample is None :  # 采样数量非空\n",
    "            _sample = random.sample  # 定义操作，从指定序列中，不作修改随机截取指定长度的片断\n",
    "            samp_neighs = [_set(_sample(neigh, num_sample, ))  # 有限采样\n",
    "                           if len(neigh)>=num_sample else neigh  # 邻居数目可能不到num\n",
    "                           for neigh in neighs]  # 每个节点的邻居集\n",
    "        else :\n",
    "            samp_neighs = neighs\n",
    "\n",
    "        if self.gcn :\n",
    "            samp_neighs = [samp_neigh + set([nodes[i]])  # 采样邻居集，加上本地\n",
    "                           for i, samp_neigh in enumerate(samp_neighs)]  # 摘出index\n",
    "\n",
    "        # print(samp_neighs)\n",
    "        # print(len(samp_neighs))  # 256/724\n",
    "\n",
    "\n",
    "        # 一个batch内的所有节点的邻居节点，去重，聚集，无序\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))  # *解包，union并集\n",
    "        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}  # 添加索引映射\n",
    "        # print(unique_nodes_list)\n",
    "        # print(len(unique_nodes_list))  # 724/1666\n",
    "\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))  # 本地节点与采样邻居的邻接矩阵\n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]  # i + j\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]  # indices vs. indexes\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        # print(len(row_indices))  # 922/\n",
    "        # print(len(column_indices))  # 922/\n",
    "        # print(mask.shape)  # 256*724/724*1666\n",
    "\n",
    "        if self.cuda :\n",
    "            # print(\"cuda\")\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        # print(num_neigh)\n",
    "        mask = mask.div(num_neigh)  # normalized\n",
    "        # print(mask.shape)  # 256*724/724*1666\n",
    "\n",
    "\n",
    "        # 触发了self.features，程序执行到此处，进行回弹？还是只是显示的问题。\n",
    "        if self.cuda :\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else :\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        # print(len(unique_nodes_list))  # 724？/1666\n",
    "        # print(embed_matrix.shape)  # 724*1433？/1666*1433/724*128\n",
    "\n",
    "\n",
    "        neigh_feats = mask.mm(embed_matrix)  # AH: neight_feats = mask * embed_matrix\n",
    "        # print(neigh_feats.shape)  # 256*1433？/724*1433/256*724 * 724*128 = 256*128\n",
    "\n",
    "        return neigh_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5aecb14-1af9-403f-a4cf-742827bc7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, \n",
    "\n",
    "                 adj_lists,  # 邻接列表\n",
    "                 features,  # 特征矩阵\n",
    "                 feature_dim,  # 原始特征维度\n",
    "\n",
    "                 aggregator,  # 汇聚函数\n",
    "                 embed_dim,  # 映射特征维度\n",
    "\n",
    "                 base_model=None,  # 基础模型\n",
    "                 num_sample=10,  # 采样数量\n",
    "                 gcn=False,  # 是否使用GCN，汇聚本地信息\n",
    "\n",
    "                 cuda=False,  # 是否使用CUDA\n",
    "                ) :  # 没有默认值参数的变量，不能放在有默认值参数变量的后面\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.adj_lists = adj_lists\n",
    "        self.features = features  # enc2初始化？\n",
    "        self.feat_dim = feature_dim  # enc2初始化？\n",
    "\n",
    "        if base_model != None :\n",
    "            self.base_model = base_model  # 哪里使用？\n",
    "        self.num_sample = num_sample\n",
    "        self.gcn = gcn\n",
    "        self.aggregator = aggregator\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "\n",
    "        # 执行卷积的参数矩阵。如果不使用GCN模式，需要执行concat拼接操作，所以向量维度为2倍的feat_dim\n",
    "        self.weight = nn.Parameter(torch.FloatTensor\n",
    "                                   (embed_dim, self.feat_dim if self.gcn else 2*self.feat_dim))  # 读论文\n",
    "        init.xavier_uniform(self.weight)  # 参数初始化\n",
    "\n",
    "\n",
    "    def forward(self, nodes) :\n",
    "\n",
    "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] \n",
    "                                                      for node in nodes], self.num_sample)\n",
    "        # enc2-agg2-enc1-agg1  agg1-enc1-agg2-enc2\n",
    "        # 256*1433？/724*1433/256*128\n",
    "\n",
    "        if not self.gcn :\n",
    "            if self.cuda :\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else :\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)  # concat\n",
    "        else :\n",
    "            combined = neigh_feats\n",
    "\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        # 128*1433 * 1433*256 = 128*256 ？\n",
    "        # 128*1433 * 1433*724 = 128*724\n",
    "        # 128*128 * 128*256 = 128*256\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3328466-baaa-44fa-8faf-6c3924a7ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, enc, num_classes) :\n",
    "\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "\n",
    "        self.enc = enc\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))  # 7*128\n",
    "        init.xavier_uniform(self.weight)\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, nodes) :\n",
    "        embeds = self.enc(nodes)  # enc2(nodes)\n",
    "        scores = self.weight.mm(embeds)  # 7*128 * 128*256 = 7*256\n",
    "        return scores.t()  # 256*7\n",
    "\n",
    "\n",
    "    def loss(self, nodes, labels) :\n",
    "        scores = self.forward(nodes)\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48481803-ee15-40a1-9a01-0226893199d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora() :\n",
    "\n",
    "\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "\n",
    "    feat_data = np.zeros((num_nodes, num_feats))  # 2708*1433\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "\n",
    "    node_map = {}  # paper id - node index\n",
    "    label_map = {}  # class label - label id\n",
    "\n",
    "\n",
    "    with open(\"cora/cora.content\") as fp :\n",
    "        # cora.content: paper_id + word_attributes + class_label\n",
    "\n",
    "        for i, line in enumerate(fp) :\n",
    "            info = line.strip().split()  # strip()移除字符，split()拆分字符，形成list\n",
    "\n",
    "            node_map[info[0]] = i  # paper id - node index\n",
    "\n",
    "            # -1，倒数第一个；数字字符串列表，数字浮点型列表；,为区分，:为切片\n",
    "            feat_data[i,:] = list(map(float, info[1:-1]))\n",
    "\n",
    "            if not info[-1] in label_map :\n",
    "                label_map[info[-1]] = len(label_map)  # 为未知class label赋予label id\n",
    "            labels[i] = label_map[info[-1]]  # node label id\n",
    "\n",
    "\n",
    "    adj_lists = defaultdict(set)  # 新建一个以set为默认value的字典\n",
    "    with open(\"cora/cora.cites\") as fp :\n",
    "        # cora.cites: ID of cited paper \\t ID of citing paper\n",
    "\n",
    "        for i, line in enumerate(fp) :\n",
    "            info = line.strip().split()\n",
    "\n",
    "            paper1 = node_map[info[0]]  # paper id - node index\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)  # undirected graph\n",
    "\n",
    "\n",
    "    return adj_lists, feat_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb426d5-a1ee-4d2b-be67-7114290377d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cora() :\n",
    "\n",
    "\n",
    "    np.random.seed(1)  # 调用一次，有效一次\n",
    "    random.seed(1)  # 调用一次，有效一次\n",
    "\n",
    "    num_nodes = 2708\n",
    "\n",
    "    adj_lists, feat_data, labels = load_cora()  # 执行一次\n",
    "    # print(adj_lists)\n",
    "    # feat_array = np.array(feat_data)\n",
    "    # print(feat_array.shape)  # 2708*1433\n",
    "\n",
    "    features = nn.Embedding(2708, 1433)  # 字典查找表，https://zhuanlan.zhihu.com/p/647536930\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "        # 将一个不可训练的tensor转换成可以训练的类型parameter\n",
    "    # features.cuda()\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)  # gcn = False\n",
    "    enc1 = Encoder(adj_lists, features, 1433, agg1, embed_dim=128, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 5\n",
    "\n",
    "    agg2 = MeanAggregator(lambda nodes:enc1(nodes).t(), cuda=False)  # lambda 参数：表达式\n",
    "    enc2 = Encoder(adj_lists, lambda nodes:enc1(nodes).t(), enc1.embed_dim, \n",
    "                   agg2, embed_dim=128, base_model=enc1, gcn=True, cuda=False)\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    graphsage = SupervisedGraphSage(enc2, 7)\n",
    "    # graphsage.cuda()\n",
    "\n",
    "    rand_indices = np.random.permutation(num_nodes)  # 随机排列序列\n",
    "    # 按照节点分集\n",
    "    train = list(rand_indices[1500:])  # list\n",
    "    val = rand_indices[1000:1500]\n",
    "    test = rand_indices[:1000]\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p:p.requires_grad, graphsage.parameters()), lr=0.7)  # 0.7？\n",
    "\n",
    "\n",
    "    times = []  # 时间戳片\n",
    "    for batch in range(100) :\n",
    "\n",
    "        batch_nodes = train[:256]  # 256个点\n",
    "        random.shuffle(train)  # 每次采样不同\n",
    "\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, Variable(torch.LongTensor(labels[np.array(batch_nodes)])))  # 真正执行\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "        print(batch, loss.item())  #\n",
    "\n",
    "\n",
    "    val_output = graphsage.forward(val)\n",
    "    print(\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print(\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a797953-2c6d-423b-aa86-b89095d7337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74749/3347669318.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)  # 参数初始化\n",
      "/tmp/ipykernel_74749/1300380885.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.9382734298706055\n",
      "1 1.9129709005355835\n",
      "2 1.8952627182006836\n",
      "3 1.8668767213821411\n",
      "4 1.8197790384292603\n",
      "5 1.7842298746109009\n",
      "6 1.7523261308670044\n",
      "7 1.728799819946289\n",
      "8 1.6776642799377441\n",
      "9 1.6426564455032349\n",
      "10 1.543238878250122\n",
      "11 1.533272385597229\n",
      "12 1.4709399938583374\n",
      "13 1.4595310688018799\n",
      "14 1.3914778232574463\n",
      "15 1.3069311380386353\n",
      "16 1.2662317752838135\n",
      "17 1.1955063343048096\n",
      "18 1.027852177619934\n",
      "19 0.9776282906532288\n",
      "20 0.9540759921073914\n",
      "21 0.9288228750228882\n",
      "22 0.8156899213790894\n",
      "23 0.7928718328475952\n",
      "24 0.7569703459739685\n",
      "25 0.7816515564918518\n",
      "26 0.6727970242500305\n",
      "27 0.6390024423599243\n",
      "28 0.6474947333335876\n",
      "29 0.6191361546516418\n",
      "30 0.5655132532119751\n",
      "31 0.7349564433097839\n",
      "32 0.9486786127090454\n",
      "33 2.2249741554260254\n",
      "34 0.646219789981842\n",
      "35 0.5213156342506409\n",
      "36 0.498539000749588\n",
      "37 0.48807981610298157\n",
      "38 0.46968531608581543\n",
      "39 0.49841806292533875\n",
      "40 0.501276433467865\n",
      "41 0.42151889204978943\n",
      "42 0.37135791778564453\n",
      "43 0.4268933832645416\n",
      "44 0.419485867023468\n",
      "45 0.3656945824623108\n",
      "46 0.34003549814224243\n",
      "47 0.31999656558036804\n",
      "48 0.3522442877292633\n",
      "49 0.3893895745277405\n",
      "50 0.3187280595302582\n",
      "51 0.3786151111125946\n",
      "52 0.34364938735961914\n",
      "53 0.4121556282043457\n",
      "54 0.30252978205680847\n",
      "55 0.35940611362457275\n",
      "56 0.35438448190689087\n",
      "57 0.2949238717556\n",
      "58 0.3213498592376709\n",
      "59 0.2631179392337799\n",
      "60 0.30173414945602417\n",
      "61 0.30890950560569763\n",
      "62 0.36269834637641907\n",
      "63 0.2626371681690216\n",
      "64 0.2420659065246582\n",
      "65 0.24996061623096466\n",
      "66 0.2813286781311035\n",
      "67 0.3058347702026367\n",
      "68 0.26857104897499084\n",
      "69 0.2514624297618866\n",
      "70 0.25613176822662354\n",
      "71 0.20159730315208435\n",
      "72 0.20557816326618195\n",
      "73 0.2019682228565216\n",
      "74 0.20996882021427155\n",
      "75 0.2075689733028412\n",
      "76 0.21581648290157318\n",
      "77 0.21231460571289062\n",
      "78 0.2539060115814209\n",
      "79 0.20651981234550476\n",
      "80 0.2102501094341278\n",
      "81 0.20949141681194305\n",
      "82 0.19954627752304077\n",
      "83 0.20468813180923462\n",
      "84 0.17594724893569946\n",
      "85 0.17053671181201935\n",
      "86 0.21218247711658478\n",
      "87 0.1146562397480011\n",
      "88 0.19966663420200348\n",
      "89 0.1980523020029068\n",
      "90 0.20633016526699066\n",
      "91 0.186315655708313\n",
      "92 0.17073997855186462\n",
      "93 0.19215954840183258\n",
      "94 0.17554064095020294\n",
      "95 0.1953822523355484\n",
      "96 0.13848158717155457\n",
      "97 0.14924627542495728\n",
      "98 0.1862422227859497\n",
      "99 0.18189433217048645\n",
      "Validation F1: 0.842\n",
      "Average batch time: 0.012299103736877441\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "\n",
    "    # print(torch.cuda.is_available())\n",
    "\n",
    "    run_cora()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN_GraphSAGE",
   "language": "python",
   "name": "gnn_graphsage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
