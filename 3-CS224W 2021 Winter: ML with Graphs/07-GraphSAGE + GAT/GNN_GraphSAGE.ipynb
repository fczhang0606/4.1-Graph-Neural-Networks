{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e021c86e-f1c9-453d-b111-cff5a7d0f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/williamleif/graphsage-simple\n",
    "# https://codeleading.com/article/65265993022/\n",
    "# https://zhuanlan.zhihu.com/p/360854229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4455219-5868-4c0e-bd21-66a04a8c6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9cb11a9-7b12-49a4-a061-50ca63691e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable  # 利用Variable定义一个计算图，可以实现自动求导\n",
    "from torch.nn import init  # 初始化参数\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from collections import defaultdict  # 替代容器\n",
    "from sklearn.metrics import f1_score  # 模型、参数的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697a3e3a-548e-47a1-9a4d-da7f22a6d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAggregator(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, features, gcn=False, cuda=False) :\n",
    "\n",
    "        super(MeanAggregator, self).__init__()\n",
    "\n",
    "        self.features = features  # 2708*1433，，724*128\n",
    "        self.gcn = gcn  # 是否汇聚本地特征\n",
    "        self.cuda = cuda  # 是否使用cuda\n",
    "\n",
    "\n",
    "    def forward(self, nodes, neighs, num_sample=10) :  # 节点集，每个节点的邻居集，采样数量\n",
    "\n",
    "        # print(nodes)\n",
    "        # print(len(nodes))  # 256/724\n",
    "        # print(neighs)\n",
    "        # print(len(neighs))  # 256/724\n",
    "\n",
    "        _set = set  # 定义操作，创建空集合\n",
    "        if not num_sample is None :  # 采样数量非空\n",
    "            _sample = random.sample  # 定义操作，从指定序列中，不作修改随机截取指定长度的片断\n",
    "            samp_neighs = [_set(_sample(neigh, num_sample, ))  # 有限采样\n",
    "                           if len(neigh)>=num_sample else neigh  # 邻居数目可能不到num\n",
    "                           for neigh in neighs]  # 每个节点的邻居集\n",
    "        else :\n",
    "            samp_neighs = neighs\n",
    "\n",
    "        if self.gcn :\n",
    "            samp_neighs = [samp_neigh + set([nodes[i]])  # 采样邻居集，加上本地\n",
    "                           for i, samp_neigh in enumerate(samp_neighs)]  # 摘出index\n",
    "\n",
    "        # print(samp_neighs)\n",
    "        # print(len(samp_neighs))  # 256/724\n",
    "\n",
    "\n",
    "        # 一个batch内的所有节点的邻居节点，去重，聚集，无序\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))  # *解包，union并集\n",
    "        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}  # 添加索引映射\n",
    "        # print(unique_nodes_list)\n",
    "        # print(len(unique_nodes_list))  # 724/1666\n",
    "\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))  # 本地节点与采样邻居的邻接矩阵\n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]  # i + j\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]  # indices vs. indexes\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        # print(len(row_indices))  # 922/\n",
    "        # print(len(column_indices))  # 922/\n",
    "        # print(mask.shape)  # 256*724/724*1666\n",
    "\n",
    "        if self.cuda :\n",
    "            # print(\"cuda\")\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        # print(num_neigh)\n",
    "        mask = mask.div(num_neigh)  # normalized\n",
    "        # print(mask.shape)  # 256*724/724*1666\n",
    "\n",
    "\n",
    "        # 触发了self.features，程序执行到此处，进行回弹？还是只是显示的问题。\n",
    "        if self.cuda :\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else :\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        # print(len(unique_nodes_list))  # 724？/1666\n",
    "        # print(embed_matrix.shape)  # 724*1433？/1666*1433/724*128\n",
    "\n",
    "\n",
    "        neigh_feats = mask.mm(embed_matrix)  # AH: neight_feats = mask * embed_matrix\n",
    "        # print(neigh_feats.shape)  # 256*1433？/724*1433/256*724 * 724*128 = 256*128\n",
    "\n",
    "        return neigh_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5aecb14-1af9-403f-a4cf-742827bc7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, \n",
    "\n",
    "                 adj_lists,  # 邻接列表\n",
    "                 features,  # 特征矩阵\n",
    "                 feature_dim,  # 原始特征维度\n",
    "\n",
    "                 aggregator,  # 汇聚函数\n",
    "                 embed_dim,  # 映射特征维度\n",
    "\n",
    "                 base_model=None,  # 基础模型\n",
    "                 num_sample=10,  # 采样数量\n",
    "                 gcn=False,  # 是否使用GCN，汇聚本地信息\n",
    "\n",
    "                 cuda=False,  # 是否使用CUDA\n",
    "                ) :  # 没有默认值参数的变量，不能放在有默认值参数变量的后面\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.adj_lists = adj_lists\n",
    "        self.features = features  # enc2初始化？\n",
    "        self.feat_dim = feature_dim  # enc2初始化？\n",
    "\n",
    "        if base_model != None :\n",
    "            self.base_model = base_model  # 哪里使用？\n",
    "        self.num_sample = num_sample\n",
    "        self.gcn = gcn\n",
    "        self.aggregator = aggregator\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "\n",
    "        # 执行卷积的参数矩阵。如果不使用GCN模式，需要执行concat拼接操作，所以向量维度为2倍的feat_dim\n",
    "        self.weight = nn.Parameter(torch.FloatTensor\n",
    "                                   (embed_dim, self.feat_dim if self.gcn else 2*self.feat_dim))  # 读论文\n",
    "        init.xavier_uniform(self.weight)  # 参数初始化\n",
    "\n",
    "\n",
    "    def forward(self, nodes) :\n",
    "\n",
    "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] \n",
    "                                                      for node in nodes], self.num_sample)\n",
    "        # enc2-agg2-enc1-agg1  agg1-enc1-agg2-enc2\n",
    "        # 256*1433？/724*1433/256*128\n",
    "\n",
    "        if not self.gcn :\n",
    "            if self.cuda :\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else :\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)  # concat\n",
    "        else :\n",
    "            combined = neigh_feats\n",
    "\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        # 128*1433 * 1433*256 = 128*256 ？\n",
    "        # 128*1433 * 1433*724 = 128*724\n",
    "        # 128*128 * 128*256 = 128*256\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3328466-baaa-44fa-8faf-6c3924a7ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, enc, num_classes) :\n",
    "\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "\n",
    "        self.enc = enc\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))  # 7*128\n",
    "        init.xavier_uniform(self.weight)\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, nodes) :\n",
    "        embeds = self.enc(nodes)  # enc2(nodes)\n",
    "        scores = self.weight.mm(embeds)  # 7*128 * 128*256 = 7*256\n",
    "        return scores.t()  # 256*7\n",
    "\n",
    "\n",
    "    def loss(self, nodes, labels) :\n",
    "        scores = self.forward(nodes)\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48481803-ee15-40a1-9a01-0226893199d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora() :\n",
    "\n",
    "\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "\n",
    "    feat_data = np.zeros((num_nodes, num_feats))  # 2708*1433\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "\n",
    "    node_map = {}  # paper id - node index\n",
    "    label_map = {}  # class label - label id\n",
    "\n",
    "\n",
    "    # https://blog.csdn.net/zfhsfdhdfajhsr/article/details/116137598\n",
    "    with open(\"cora/cora.content\") as fp :\n",
    "        # cora.content: paper_id + word_attributes + class_label\n",
    "\n",
    "        for i, line in enumerate(fp) :\n",
    "            info = line.strip().split()  # strip()移除字符，split()拆分字符，形成list\n",
    "\n",
    "            node_map[info[0]] = i  # paper id - node index\n",
    "\n",
    "            # -1，倒数第一个；数字字符串列表，数字浮点型列表；,为区分，:为切片\n",
    "            feat_data[i,:] = list(map(float, info[1:-1]))\n",
    "\n",
    "            if not info[-1] in label_map :\n",
    "                label_map[info[-1]] = len(label_map)  # 为未知class label赋予label id\n",
    "            labels[i] = label_map[info[-1]]  # node label id\n",
    "\n",
    "\n",
    "    adj_lists = defaultdict(set)  # 新建一个以set为默认value的字典\n",
    "    with open(\"cora/cora.cites\") as fp :\n",
    "        # cora.cites: ID of cited paper \\t ID of citing paper\n",
    "\n",
    "        for i, line in enumerate(fp) :\n",
    "            info = line.strip().split()\n",
    "\n",
    "            paper1 = node_map[info[0]]  # paper id - node index\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)  # undirected graph\n",
    "\n",
    "\n",
    "    return adj_lists, feat_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb426d5-a1ee-4d2b-be67-7114290377d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cora() :\n",
    "\n",
    "\n",
    "    np.random.seed(1)  # 调用一次，有效一次\n",
    "    random.seed(1)  # 调用一次，有效一次\n",
    "\n",
    "    num_nodes = 2708\n",
    "\n",
    "    adj_lists, feat_data, labels = load_cora()  # 执行一次\n",
    "    # print(adj_lists)\n",
    "    # feat_array = np.array(feat_data)\n",
    "    # print(feat_array.shape)  # 2708*1433\n",
    "\n",
    "    features = nn.Embedding(2708, 1433)  # 字典查找表，https://zhuanlan.zhihu.com/p/647536930\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "        # 将一个不可训练的tensor转换成可以训练的类型parameter\n",
    "    # features.cuda()\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)  # gcn = False\n",
    "    enc1 = Encoder(adj_lists, features, 1433, agg1, embed_dim=128, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 5\n",
    "\n",
    "    agg2 = MeanAggregator(lambda nodes:enc1(nodes).t(), cuda=False)  # lambda 参数：表达式\n",
    "    enc2 = Encoder(adj_lists, lambda nodes:enc1(nodes).t(), enc1.embed_dim, \n",
    "                   agg2, embed_dim=128, base_model=enc1, gcn=True, cuda=False)\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    graphsage = SupervisedGraphSage(enc2, 7)\n",
    "    # graphsage.cuda()\n",
    "\n",
    "    rand_indices = np.random.permutation(num_nodes)  # 随机排列序列\n",
    "    # 按照节点分集\n",
    "    train = list(rand_indices[1500:])  # list\n",
    "    val = rand_indices[1000:1500]\n",
    "    test = rand_indices[:1000]\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p:p.requires_grad, graphsage.parameters()), lr=0.7)  # 0.7？\n",
    "\n",
    "\n",
    "    times = []  # 时间戳片\n",
    "    for batch in range(100) :\n",
    "\n",
    "        batch_nodes = train[:256]  # 256个点\n",
    "        random.shuffle(train)  # 每次采样不同\n",
    "\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, Variable(torch.LongTensor(labels[np.array(batch_nodes)])))  # 真正执行\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "        print(batch, loss.item())  #\n",
    "\n",
    "\n",
    "    val_output = graphsage.forward(val)\n",
    "    print(\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print(\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a797953-2c6d-423b-aa86-b89095d7337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21141/3347669318.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)  # 参数初始化\n",
      "/tmp/ipykernel_21141/1300380885.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.9431257247924805\n",
      "1 1.920516014099121\n",
      "2 1.8969712257385254\n",
      "3 1.873515248298645\n",
      "4 1.8391609191894531\n",
      "5 1.7935867309570312\n",
      "6 1.7658058404922485\n",
      "7 1.7370555400848389\n",
      "8 1.691222906112671\n",
      "9 1.6536329984664917\n",
      "10 1.548503041267395\n",
      "11 1.5281739234924316\n",
      "12 1.4740568399429321\n",
      "13 1.4711228609085083\n",
      "14 1.376316785812378\n",
      "15 1.2970651388168335\n",
      "16 1.23788321018219\n",
      "17 1.168979287147522\n",
      "18 0.9896806478500366\n",
      "19 0.9294998049736023\n",
      "20 0.8983370065689087\n",
      "21 0.8824247717857361\n",
      "22 0.7854024767875671\n",
      "23 0.7549806237220764\n",
      "24 0.7054868936538696\n",
      "25 0.745578944683075\n",
      "26 0.6273682117462158\n",
      "27 0.6444839239120483\n",
      "28 0.7085258364677429\n",
      "29 0.828799307346344\n",
      "30 0.7955332398414612\n",
      "31 1.0029040575027466\n",
      "32 0.6404502987861633\n",
      "33 0.5842044353485107\n",
      "34 0.4937381148338318\n",
      "35 0.4280731678009033\n",
      "36 0.4637112021446228\n",
      "37 0.49667835235595703\n",
      "38 0.492444783449173\n",
      "39 0.5240227580070496\n",
      "40 0.48932594060897827\n",
      "41 0.3839634358882904\n",
      "42 0.3418525159358978\n",
      "43 0.40775567293167114\n",
      "44 0.39959582686424255\n",
      "45 0.35198497772216797\n",
      "46 0.3248589038848877\n",
      "47 0.29564836621284485\n",
      "48 0.3293103575706482\n",
      "49 0.38528528809547424\n",
      "50 0.3190351724624634\n",
      "51 0.3840221166610718\n",
      "52 0.3482644855976105\n",
      "53 0.40022480487823486\n",
      "54 0.2989453971385956\n",
      "55 0.3702351152896881\n",
      "56 0.3870967626571655\n",
      "57 0.3035328686237335\n",
      "58 0.32832732796669006\n",
      "59 0.2569013833999634\n",
      "60 0.2972260117530823\n",
      "61 0.2854379117488861\n",
      "62 0.3474458158016205\n",
      "63 0.24665316939353943\n",
      "64 0.24139516055583954\n",
      "65 0.24399885535240173\n",
      "66 0.2690184712409973\n",
      "67 0.30842268466949463\n",
      "68 0.25497955083847046\n",
      "69 0.23490531742572784\n",
      "70 0.2505265772342682\n",
      "71 0.19806720316410065\n",
      "72 0.21439653635025024\n",
      "73 0.19674499332904816\n",
      "74 0.20391741394996643\n",
      "75 0.2104070484638214\n",
      "76 0.21213501691818237\n",
      "77 0.19929683208465576\n",
      "78 0.23640115559101105\n",
      "79 0.18999546766281128\n",
      "80 0.20283696055412292\n",
      "81 0.20099130272865295\n",
      "82 0.19549055397510529\n",
      "83 0.20302937924861908\n",
      "84 0.1773964762687683\n",
      "85 0.16367599368095398\n",
      "86 0.20860624313354492\n",
      "87 0.11074955761432648\n",
      "88 0.2005676031112671\n",
      "89 0.2113191783428192\n",
      "90 0.19620023667812347\n",
      "91 0.1829860806465149\n",
      "92 0.17608016729354858\n",
      "93 0.19659794867038727\n",
      "94 0.17413441836833954\n",
      "95 0.18478140234947205\n",
      "96 0.1332346498966217\n",
      "97 0.1387828141450882\n",
      "98 0.17258992791175842\n",
      "99 0.17361357808113098\n",
      "Validation F1: 0.8459999999999999\n",
      "Average batch time: 0.006552224159240723\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "\n",
    "    # print(torch.cuda.is_available())\n",
    "\n",
    "    run_cora()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN_GraphSAGE",
   "language": "python",
   "name": "gnn_graphsage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
