{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e021c86e-f1c9-453d-b111-cff5a7d0f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/williamleif/graphsage-simple\n",
    "# https://codeleading.com/article/65265993022/\n",
    "# https://zhuanlan.zhihu.com/p/360854229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4455219-5868-4c0e-bd21-66a04a8c6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9cb11a9-7b12-49a4-a061-50ca63691e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable  # 利用Variable定义一个计算图，可以实现自动求导\n",
    "from torch.nn import init  # 初始化参数\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from collections import defaultdict  # 替代容器\n",
    "from sklearn.metrics import f1_score  # 模型、参数的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697a3e3a-548e-47a1-9a4d-da7f22a6d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAggregator(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, features, gcn=False, cuda=False) :\n",
    "\n",
    "        super(MeanAggregator, self).__init__()\n",
    "\n",
    "        self.features = features  # 2708*1433，，724*128\n",
    "        self.gcn = gcn  # 是否汇聚本地特征\n",
    "        self.cuda = cuda  # 是否使用cuda\n",
    "\n",
    "\n",
    "    def forward(self, nodes, neighs, num_sample=10) :  # 节点集，每个节点的邻居集，采样数量\n",
    "\n",
    "        # print(nodes)\n",
    "        # print(len(nodes))  # 256/724\n",
    "        # print(neighs)\n",
    "        # print(len(neighs))  # 256/724\n",
    "\n",
    "        _set = set  # 定义操作，创建空集合\n",
    "        if not num_sample is None :  # 采样数量非空\n",
    "            _sample = random.sample  # 定义操作，从指定序列中，不作修改随机截取指定长度的片断\n",
    "            samp_neighs = [_set(_sample(neigh, num_sample, ))  # 有限采样\n",
    "                           if len(neigh)>=num_sample else neigh  # 邻居数目可能不到num\n",
    "                           for neigh in neighs]  # 每个节点的邻居集\n",
    "        else :\n",
    "            samp_neighs = neighs\n",
    "\n",
    "        if self.gcn :\n",
    "            samp_neighs = [samp_neigh + set([nodes[i]])  # 采样邻居集，加上本地\n",
    "                           for i, samp_neigh in enumerate(samp_neighs)]  # 摘出index\n",
    "\n",
    "        # print(samp_neighs)\n",
    "        # print(len(samp_neighs))  # 256/724\n",
    "\n",
    "\n",
    "        # 一个batch内的所有节点的邻居节点，去重，聚集，无序\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))  # *解包，union并集\n",
    "        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}  # 添加索引映射\n",
    "        # print(unique_nodes_list)\n",
    "        # print(len(unique_nodes_list))  # 724/1666\n",
    "\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))  # 本地节点与采样邻居的邻接矩阵\n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]  # i + j\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]  # indices vs. indexes\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        # print(len(row_indices))  # 922/\n",
    "        # print(len(column_indices))  # 922/\n",
    "        # print(mask.shape)  # 256*724/724*1666\n",
    "\n",
    "        if self.cuda :\n",
    "            # print(\"cuda\")\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        # print(num_neigh)\n",
    "        mask = mask.div(num_neigh)  # normalized\n",
    "        # print(mask.shape)  # 256*724/724*1666\n",
    "\n",
    "\n",
    "        # 触发了self.features，程序执行到此处，进行回弹？还是只是显示的问题。\n",
    "        if self.cuda :\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else :\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        # print(len(unique_nodes_list))  # 724？/1666\n",
    "        # print(embed_matrix.shape)  # 724*1433？/1666*1433/724*128\n",
    "\n",
    "\n",
    "        neigh_feats = mask.mm(embed_matrix)  # AH: neight_feats = mask * embed_matrix\n",
    "        # print(neigh_feats.shape)  # 256*1433？/724*1433/256*724 * 724*128 = 256*128\n",
    "\n",
    "        return neigh_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5aecb14-1af9-403f-a4cf-742827bc7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, \n",
    "\n",
    "                 adj_lists,  # 邻接列表\n",
    "                 features,  # 特征矩阵\n",
    "                 feature_dim,  # 原始特征维度\n",
    "\n",
    "                 aggregator,  # 汇聚函数\n",
    "                 embed_dim,  # 映射特征维度\n",
    "\n",
    "                 base_model=None,  # 基础模型\n",
    "                 num_sample=10,  # 采样数量\n",
    "                 gcn=False,  # 是否使用GCN，汇聚本地信息\n",
    "\n",
    "                 cuda=False,  # 是否使用CUDA\n",
    "                ) :  # 没有默认值参数的变量，不能放在有默认值参数变量的后面\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.adj_lists = adj_lists\n",
    "        self.features = features  # enc2初始化？\n",
    "        self.feat_dim = feature_dim  # enc2初始化？\n",
    "\n",
    "        if base_model != None :\n",
    "            self.base_model = base_model  # 哪里使用？\n",
    "        self.num_sample = num_sample\n",
    "        self.gcn = gcn\n",
    "        self.aggregator = aggregator\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "\n",
    "        # 执行卷积的参数矩阵。如果不使用GCN模式，需要执行concat拼接操作，所以向量维度为2倍的feat_dim\n",
    "        self.weight = nn.Parameter(torch.FloatTensor\n",
    "                                   (embed_dim, self.feat_dim if self.gcn else 2*self.feat_dim))  # 读论文\n",
    "        init.xavier_uniform(self.weight)  # 参数初始化\n",
    "\n",
    "\n",
    "    def forward(self, nodes) :\n",
    "\n",
    "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] \n",
    "                                                      for node in nodes], self.num_sample)\n",
    "        # enc2-agg2-enc1-agg1  agg1-enc1-agg2-enc2\n",
    "        # 256*1433？/724*1433/256*128\n",
    "\n",
    "        if not self.gcn :\n",
    "            if self.cuda :\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else :\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)  # concat\n",
    "        else :\n",
    "            combined = neigh_feats\n",
    "\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        # 128*1433 * 1433*256 = 128*256 ？\n",
    "        # 128*1433 * 1433*724 = 128*724\n",
    "        # 128*128 * 128*256 = 128*256\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3328466-baaa-44fa-8faf-6c3924a7ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, enc, num_classes) :\n",
    "\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "\n",
    "        self.enc = enc\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))  # 7*128\n",
    "        init.xavier_uniform(self.weight)\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, nodes) :\n",
    "        embeds = self.enc(nodes)  # enc2(nodes)\n",
    "        scores = self.weight.mm(embeds)  # 7*128 * 128*256 = 7*256\n",
    "        return scores.t()  # 256*7\n",
    "\n",
    "\n",
    "    def loss(self, nodes, labels) :\n",
    "        scores = self.forward(nodes)\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48481803-ee15-40a1-9a01-0226893199d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora() :\n",
    "\n",
    "\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "\n",
    "    feat_data = np.zeros((num_nodes, num_feats))  # 2708*1433\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "\n",
    "    node_map = {}  # paper id - node index\n",
    "    label_map = {}  # class label - label id\n",
    "\n",
    "\n",
    "    # https://blog.csdn.net/zfhsfdhdfajhsr/article/details/116137598\n",
    "    with open(\"cora/cora.content\") as fp :\n",
    "        # cora.content: paper_id + word_attributes + class_label\n",
    "\n",
    "        for i, line in enumerate(fp) :\n",
    "            info = line.strip().split()  # strip()移除字符，split()拆分字符，形成list\n",
    "\n",
    "            node_map[info[0]] = i  # paper id - node index\n",
    "\n",
    "            # -1，倒数第一个；数字字符串列表，数字浮点型列表；,为区分，:为切片\n",
    "            feat_data[i,:] = list(map(float, info[1:-1]))\n",
    "\n",
    "            if not info[-1] in label_map :\n",
    "                label_map[info[-1]] = len(label_map)  # 为未知class label赋予label id\n",
    "            labels[i] = label_map[info[-1]]  # node label id\n",
    "\n",
    "\n",
    "    adj_lists = defaultdict(set)  # 新建一个以set为默认value的字典\n",
    "    with open(\"cora/cora.cites\") as fp :\n",
    "        # cora.cites: ID of cited paper \\t ID of citing paper\n",
    "\n",
    "        for i, line in enumerate(fp) :\n",
    "            info = line.strip().split()\n",
    "\n",
    "            paper1 = node_map[info[0]]  # paper id - node index\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)  # undirected graph\n",
    "\n",
    "\n",
    "    return adj_lists, feat_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb426d5-a1ee-4d2b-be67-7114290377d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cora() :\n",
    "\n",
    "\n",
    "    np.random.seed(1)  # 调用一次，有效一次\n",
    "    random.seed(1)  # 调用一次，有效一次\n",
    "\n",
    "    num_nodes = 2708\n",
    "\n",
    "    adj_lists, feat_data, labels = load_cora()  # 执行一次\n",
    "    # print(adj_lists)\n",
    "    # feat_array = np.array(feat_data)\n",
    "    # print(feat_array.shape)  # 2708*1433\n",
    "\n",
    "    features = nn.Embedding(2708, 1433)  # 字典查找表，https://zhuanlan.zhihu.com/p/647536930\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "        # 将一个不可训练的tensor转换成可以训练的类型parameter\n",
    "    # features.cuda()\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)  # gcn = False\n",
    "    enc1 = Encoder(adj_lists, features, 1433, agg1, embed_dim=128, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 5\n",
    "\n",
    "    agg2 = MeanAggregator(lambda nodes:enc1(nodes).t(), cuda=False)  # lambda 参数：表达式\n",
    "    enc2 = Encoder(adj_lists, lambda nodes:enc1(nodes).t(), enc1.embed_dim, \n",
    "                   agg2, embed_dim=128, base_model=enc1, gcn=True, cuda=False)\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    graphsage = SupervisedGraphSage(enc2, 7)\n",
    "    # graphsage.cuda()\n",
    "\n",
    "    rand_indices = np.random.permutation(num_nodes)  # 随机排列序列\n",
    "    # 按照节点分集\n",
    "    train = list(rand_indices[1500:])  # list\n",
    "    val = rand_indices[1000:1500]\n",
    "    test = rand_indices[:1000]\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p:p.requires_grad, graphsage.parameters()), lr=0.7)  # 0.7？\n",
    "\n",
    "\n",
    "    times = []  # 时间戳片\n",
    "    for batch in range(100) :\n",
    "\n",
    "        batch_nodes = train[:256]  # 256个点\n",
    "        random.shuffle(train)  # 每次采样不同\n",
    "\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, Variable(torch.LongTensor(labels[np.array(batch_nodes)])))  # 真正执行\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "        print(batch, loss.item())  #\n",
    "\n",
    "\n",
    "    val_output = graphsage.forward(val)\n",
    "    print(\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print(\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a797953-2c6d-423b-aa86-b89095d7337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20385/3347669318.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)  # 参数初始化\n",
      "/tmp/ipykernel_20385/1300380885.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.9399844408035278\n",
      "1 1.9190994501113892\n",
      "2 1.8981761932373047\n",
      "3 1.8728034496307373\n",
      "4 1.8336786031723022\n",
      "5 1.8083218336105347\n",
      "6 1.7743821144104004\n",
      "7 1.7518820762634277\n",
      "8 1.6999285221099854\n",
      "9 1.6662955284118652\n",
      "10 1.5780115127563477\n",
      "11 1.566594123840332\n",
      "12 1.5161126852035522\n",
      "13 1.5002799034118652\n",
      "14 1.4308321475982666\n",
      "15 1.3524351119995117\n",
      "16 1.3052356243133545\n",
      "17 1.2509088516235352\n",
      "18 1.080136775970459\n",
      "19 1.0265235900878906\n",
      "20 1.0069183111190796\n",
      "21 0.9543110728263855\n",
      "22 0.8498784303665161\n",
      "23 0.8094907402992249\n",
      "24 0.7725906372070312\n",
      "25 0.8070815801620483\n",
      "26 0.6877274513244629\n",
      "27 0.6956753730773926\n",
      "28 0.7658761143684387\n",
      "29 0.9236629605293274\n",
      "30 0.8031381368637085\n",
      "31 1.0974193811416626\n",
      "32 0.6815934181213379\n",
      "33 0.6214292049407959\n",
      "34 0.515097975730896\n",
      "35 0.4499916136264801\n",
      "36 0.4676463007926941\n",
      "37 0.4968079626560211\n",
      "38 0.49022993445396423\n",
      "39 0.5285108685493469\n",
      "40 0.48567667603492737\n",
      "41 0.39763736724853516\n",
      "42 0.3527314066886902\n",
      "43 0.40881189703941345\n",
      "44 0.40927833318710327\n",
      "45 0.3531739115715027\n",
      "46 0.3350268006324768\n",
      "47 0.313998281955719\n",
      "48 0.33169683814048767\n",
      "49 0.382842481136322\n",
      "50 0.31788167357444763\n",
      "51 0.3719431459903717\n",
      "52 0.3277972936630249\n",
      "53 0.4060099124908447\n",
      "54 0.2964138686656952\n",
      "55 0.3692268133163452\n",
      "56 0.41492798924446106\n",
      "57 0.41654279828071594\n",
      "58 0.4020342230796814\n",
      "59 0.35013046860694885\n",
      "60 0.3120410144329071\n",
      "61 0.30245256423950195\n",
      "62 0.3467726409435272\n",
      "63 0.24201029539108276\n",
      "64 0.23973175883293152\n",
      "65 0.2491835355758667\n",
      "66 0.2705053389072418\n",
      "67 0.31651198863983154\n",
      "68 0.2538436949253082\n",
      "69 0.23872259259223938\n",
      "70 0.2497573047876358\n",
      "71 0.1947021484375\n",
      "72 0.20463860034942627\n",
      "73 0.193184494972229\n",
      "74 0.20241259038448334\n",
      "75 0.2124413251876831\n",
      "76 0.20413006842136383\n",
      "77 0.19914953410625458\n",
      "78 0.23596453666687012\n",
      "79 0.18925154209136963\n",
      "80 0.196351557970047\n",
      "81 0.20008276402950287\n",
      "82 0.2052394598722458\n",
      "83 0.20543040335178375\n",
      "84 0.18536318838596344\n",
      "85 0.16654781997203827\n",
      "86 0.20439736545085907\n",
      "87 0.10971108824014664\n",
      "88 0.20388366281986237\n",
      "89 0.20415517687797546\n",
      "90 0.19899757206439972\n",
      "91 0.18115176260471344\n",
      "92 0.16485793888568878\n",
      "93 0.18301069736480713\n",
      "94 0.16706804931163788\n",
      "95 0.1908426135778427\n",
      "96 0.13286951184272766\n",
      "97 0.14631661772727966\n",
      "98 0.1626579761505127\n",
      "99 0.1702905148267746\n",
      "Validation F1: 0.8399999999999999\n",
      "Average batch time: 0.006551012992858886\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "\n",
    "    # print(torch.cuda.is_available())\n",
    "\n",
    "    run_cora()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN_GraphSAGE",
   "language": "python",
   "name": "gnn_graphsage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
