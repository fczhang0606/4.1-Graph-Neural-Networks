{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2295bb7b-cff7-49fe-92db-a35c158bfb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "# git clone git@github.com:Diego999/pyGAT.git\n",
    "################################################################################################################################\n",
    "# 环境搭建：\n",
    "# conda create -n GNN_GAT\n",
    "# conda activate GNN_GAT\n",
    "\n",
    "# conda install python=3.8\n",
    "\n",
    "# https://pytorch.org/get-started/previous-versions/\n",
    "# conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "################################################################################################################################\n",
    "# conda install ipykernel\n",
    "# conda install platformdirs\n",
    "# pip3 install ipywidgets\n",
    "# pip3 install --upgrade jupyter_core jupyter_client\n",
    "\n",
    "# python -m ipykernel install --user --name GNN_GAT\n",
    "################################################################################################################################\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "# pip install scipy\n",
    "\n",
    "# sudo apt install graphviz\n",
    "# pip install graphviz\n",
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ac99a1-81a2-4b77-9617-b52874075264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898c36fc-45e3-46e3-83fa-18a859fb42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "\n",
    "        self.in_features = in_features  # I\n",
    "        self.out_features = out_features  # O\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))  # I*O\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))  # 2O*1\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "\n",
    "    def forward(self, adj, h):  #\n",
    "\n",
    "        Wh = torch.mm(h, self.W)  # h(N*I) * W(I*O) = Wh(N*O)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)  # 全关联系数矩阵\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj>0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "\n",
    "        # Wh(N*O) * a(1/2)(O*1) = Wh(1/2)(N*1)，每个节点的加与被加关联值\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])  # https://blog.csdn.net/didi_ya/article/details/121158666\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])  # https://blog.csdn.net/weixin_48780159/article/details/119032864\n",
    "\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T  # e(N*N)\n",
    "\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + str(self.in_features) + '->' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593ad783-f027-4f49-a93d-673207380cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 多头\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)  # \n",
    "\n",
    "        # 两层\n",
    "        self.out_att = GraphAttentionLayer(nhid*nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "\n",
    "    def forward(self, adj, x):\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(adj, x) for att in self.attentions], dim=1)\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(adj, x))\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2625d45-1c61-4398-aacb-e020709a0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"./cora/\", dataset=\"cora\"):\n",
    "\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T>adj) - adj.multiply(adj.T>adj)\n",
    "\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize_adj(mx):\n",
    "\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def normalize_features(mx):\n",
    "\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b7e354-d6f3-45c1-8b46-ee8e2084460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhfc/anaconda3/envs/GNN_GAT/lib/python3.8/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# seed\n",
    "random.seed(72)\n",
    "np.random.seed(72)\n",
    "torch.manual_seed(72)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(72)\n",
    "\n",
    "\n",
    "# load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "\n",
    "# model and optimizer\n",
    "model = GAT(nfeat=features.shape[1], \n",
    "            nhid=8, \n",
    "            nclass=int(labels.max()) + 1, \n",
    "            dropout=0.6, \n",
    "            nheads=8, \n",
    "            alpha=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.005, \n",
    "                       weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# cuda\n",
    "if torch.cuda.is_available():\n",
    "    adj = adj.cuda()\n",
    "    features = features.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "# \n",
    "adj, features, labels = Variable(adj), Variable(features), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a498c3fc-c68d-4618-8260-c5a840c67790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(adj, features)\n",
    "\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # evaluate\n",
    "    # deactivates dropout during validation run.\n",
    "    model.eval()\n",
    "    output = model(adj, features)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch+1), \n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()), \n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()), \n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()), \n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()), \n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f28ecd-75d2-4572-a5c1-72f23037f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test():\n",
    "\n",
    "    model.eval()\n",
    "    output = model(adj, features)\n",
    "\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "\n",
    "    print(\"Test set results:\", \n",
    "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ea81d6-3a96-4235-a47b-b0bfececfb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9486 acc_train: 0.1714 loss_val: 1.9411 acc_val: 0.2533 time: 0.4625s\n",
      "Epoch: 0002 loss_train: 1.9442 acc_train: 0.2071 loss_val: 1.9323 acc_val: 0.4800 time: 0.3920s\n",
      "Epoch: 0003 loss_train: 1.9291 acc_train: 0.3357 loss_val: 1.9231 acc_val: 0.5900 time: 0.3973s\n",
      "Epoch: 0004 loss_train: 1.9233 acc_train: 0.3429 loss_val: 1.9140 acc_val: 0.6267 time: 0.4066s\n",
      "Epoch: 0005 loss_train: 1.9044 acc_train: 0.4714 loss_val: 1.9050 acc_val: 0.6333 time: 0.4121s\n",
      "Epoch: 0006 loss_train: 1.8991 acc_train: 0.5286 loss_val: 1.8959 acc_val: 0.6533 time: 0.4318s\n",
      "Epoch: 0007 loss_train: 1.8786 acc_train: 0.5286 loss_val: 1.8865 acc_val: 0.6500 time: 0.4275s\n",
      "Epoch: 0008 loss_train: 1.8726 acc_train: 0.5643 loss_val: 1.8770 acc_val: 0.6433 time: 0.4054s\n",
      "Epoch: 0009 loss_train: 1.8614 acc_train: 0.5500 loss_val: 1.8675 acc_val: 0.6433 time: 0.4609s\n",
      "Epoch: 0010 loss_train: 1.8347 acc_train: 0.6214 loss_val: 1.8576 acc_val: 0.6500 time: 0.3901s\n",
      "Epoch: 0011 loss_train: 1.8332 acc_train: 0.5643 loss_val: 1.8474 acc_val: 0.6467 time: 0.4191s\n",
      "Epoch: 0012 loss_train: 1.8213 acc_train: 0.5643 loss_val: 1.8370 acc_val: 0.6333 time: 0.4343s\n",
      "Epoch: 0013 loss_train: 1.8124 acc_train: 0.5714 loss_val: 1.8265 acc_val: 0.6333 time: 0.4290s\n",
      "Epoch: 0014 loss_train: 1.7915 acc_train: 0.6000 loss_val: 1.8158 acc_val: 0.6333 time: 0.3833s\n",
      "Epoch: 0015 loss_train: 1.8043 acc_train: 0.5286 loss_val: 1.8049 acc_val: 0.6333 time: 0.4091s\n",
      "Epoch: 0016 loss_train: 1.7524 acc_train: 0.5786 loss_val: 1.7939 acc_val: 0.6400 time: 0.3879s\n",
      "Epoch: 0017 loss_train: 1.7383 acc_train: 0.6143 loss_val: 1.7824 acc_val: 0.6400 time: 0.3966s\n",
      "Epoch: 0018 loss_train: 1.7253 acc_train: 0.5643 loss_val: 1.7707 acc_val: 0.6333 time: 0.3997s\n",
      "Epoch: 0019 loss_train: 1.7342 acc_train: 0.5357 loss_val: 1.7588 acc_val: 0.6333 time: 0.4416s\n",
      "Epoch: 0020 loss_train: 1.7211 acc_train: 0.6357 loss_val: 1.7468 acc_val: 0.6333 time: 0.3878s\n",
      "Epoch: 0021 loss_train: 1.6777 acc_train: 0.5714 loss_val: 1.7346 acc_val: 0.6333 time: 0.3907s\n",
      "Epoch: 0022 loss_train: 1.7102 acc_train: 0.5929 loss_val: 1.7224 acc_val: 0.6367 time: 0.3894s\n",
      "Epoch: 0023 loss_train: 1.6810 acc_train: 0.6286 loss_val: 1.7100 acc_val: 0.6367 time: 0.3804s\n",
      "Epoch: 0024 loss_train: 1.6765 acc_train: 0.5857 loss_val: 1.6977 acc_val: 0.6367 time: 0.3989s\n",
      "Epoch: 0025 loss_train: 1.6440 acc_train: 0.6071 loss_val: 1.6852 acc_val: 0.6367 time: 0.3860s\n",
      "Epoch: 0026 loss_train: 1.6765 acc_train: 0.5643 loss_val: 1.6729 acc_val: 0.6433 time: 0.3822s\n",
      "Epoch: 0027 loss_train: 1.6317 acc_train: 0.5357 loss_val: 1.6604 acc_val: 0.6433 time: 0.3771s\n",
      "Epoch: 0028 loss_train: 1.5592 acc_train: 0.6214 loss_val: 1.6476 acc_val: 0.6433 time: 0.4532s\n",
      "Epoch: 0029 loss_train: 1.6432 acc_train: 0.5214 loss_val: 1.6350 acc_val: 0.6433 time: 0.4287s\n",
      "Epoch: 0030 loss_train: 1.5299 acc_train: 0.5786 loss_val: 1.6223 acc_val: 0.6467 time: 0.4085s\n",
      "Epoch: 0031 loss_train: 1.5689 acc_train: 0.5857 loss_val: 1.6095 acc_val: 0.6500 time: 0.3940s\n",
      "Epoch: 0032 loss_train: 1.5580 acc_train: 0.5714 loss_val: 1.5968 acc_val: 0.6533 time: 0.4190s\n",
      "Epoch: 0033 loss_train: 1.5393 acc_train: 0.5929 loss_val: 1.5840 acc_val: 0.6533 time: 0.4408s\n",
      "Epoch: 0034 loss_train: 1.5146 acc_train: 0.5714 loss_val: 1.5714 acc_val: 0.6533 time: 0.3791s\n",
      "Epoch: 0035 loss_train: 1.5206 acc_train: 0.6000 loss_val: 1.5587 acc_val: 0.6533 time: 0.3954s\n",
      "Epoch: 0036 loss_train: 1.5191 acc_train: 0.6357 loss_val: 1.5460 acc_val: 0.6600 time: 0.3909s\n",
      "Epoch: 0037 loss_train: 1.4972 acc_train: 0.6000 loss_val: 1.5336 acc_val: 0.6633 time: 0.4115s\n",
      "Epoch: 0038 loss_train: 1.4177 acc_train: 0.6357 loss_val: 1.5212 acc_val: 0.6667 time: 0.3924s\n",
      "Epoch: 0039 loss_train: 1.4027 acc_train: 0.6643 loss_val: 1.5087 acc_val: 0.6667 time: 0.4266s\n",
      "Epoch: 0040 loss_train: 1.4252 acc_train: 0.6500 loss_val: 1.4962 acc_val: 0.6733 time: 0.3770s\n",
      "Epoch: 0041 loss_train: 1.4261 acc_train: 0.6571 loss_val: 1.4837 acc_val: 0.6733 time: 0.4622s\n",
      "Epoch: 0042 loss_train: 1.3590 acc_train: 0.6500 loss_val: 1.4712 acc_val: 0.6767 time: 0.4677s\n",
      "Epoch: 0043 loss_train: 1.4586 acc_train: 0.6357 loss_val: 1.4589 acc_val: 0.6900 time: 0.3930s\n",
      "Epoch: 0044 loss_train: 1.3878 acc_train: 0.6429 loss_val: 1.4468 acc_val: 0.6933 time: 0.4435s\n",
      "Epoch: 0045 loss_train: 1.3164 acc_train: 0.7286 loss_val: 1.4346 acc_val: 0.7100 time: 0.4455s\n",
      "Epoch: 0046 loss_train: 1.3489 acc_train: 0.6786 loss_val: 1.4226 acc_val: 0.7133 time: 0.4089s\n",
      "Epoch: 0047 loss_train: 1.3440 acc_train: 0.7071 loss_val: 1.4107 acc_val: 0.7167 time: 0.4013s\n",
      "Epoch: 0048 loss_train: 1.3608 acc_train: 0.6357 loss_val: 1.3989 acc_val: 0.7200 time: 0.3846s\n",
      "Epoch: 0049 loss_train: 1.3652 acc_train: 0.6143 loss_val: 1.3874 acc_val: 0.7267 time: 0.3950s\n",
      "Epoch: 0050 loss_train: 1.3285 acc_train: 0.6643 loss_val: 1.3760 acc_val: 0.7333 time: 0.3944s\n",
      "Epoch: 0051 loss_train: 1.2941 acc_train: 0.7071 loss_val: 1.3646 acc_val: 0.7367 time: 0.4156s\n",
      "Epoch: 0052 loss_train: 1.3407 acc_train: 0.6571 loss_val: 1.3534 acc_val: 0.7367 time: 0.3966s\n",
      "Epoch: 0053 loss_train: 1.3046 acc_train: 0.6786 loss_val: 1.3424 acc_val: 0.7467 time: 0.4701s\n",
      "Epoch: 0054 loss_train: 1.2617 acc_train: 0.7143 loss_val: 1.3318 acc_val: 0.7467 time: 0.4355s\n",
      "Epoch: 0055 loss_train: 1.1889 acc_train: 0.7071 loss_val: 1.3212 acc_val: 0.7567 time: 0.4009s\n",
      "Epoch: 0056 loss_train: 1.2461 acc_train: 0.7429 loss_val: 1.3107 acc_val: 0.7567 time: 0.4018s\n",
      "Epoch: 0057 loss_train: 1.2935 acc_train: 0.7071 loss_val: 1.3003 acc_val: 0.7667 time: 0.3926s\n",
      "Epoch: 0058 loss_train: 1.1554 acc_train: 0.7286 loss_val: 1.2900 acc_val: 0.7733 time: 0.3774s\n",
      "Epoch: 0059 loss_train: 1.1982 acc_train: 0.7214 loss_val: 1.2797 acc_val: 0.7900 time: 0.3895s\n",
      "Epoch: 0060 loss_train: 1.1557 acc_train: 0.7429 loss_val: 1.2697 acc_val: 0.7933 time: 0.3919s\n",
      "Epoch: 0061 loss_train: 1.2083 acc_train: 0.7357 loss_val: 1.2598 acc_val: 0.7967 time: 0.3846s\n",
      "Epoch: 0062 loss_train: 1.2278 acc_train: 0.7071 loss_val: 1.2500 acc_val: 0.8000 time: 0.4347s\n",
      "Epoch: 0063 loss_train: 1.2203 acc_train: 0.7214 loss_val: 1.2407 acc_val: 0.8000 time: 0.4284s\n",
      "Epoch: 0064 loss_train: 1.1906 acc_train: 0.7143 loss_val: 1.2315 acc_val: 0.8033 time: 0.4188s\n",
      "Epoch: 0065 loss_train: 1.1154 acc_train: 0.7357 loss_val: 1.2223 acc_val: 0.8000 time: 0.4006s\n",
      "Epoch: 0066 loss_train: 1.0684 acc_train: 0.7571 loss_val: 1.2129 acc_val: 0.8000 time: 0.3945s\n",
      "Epoch: 0067 loss_train: 1.1675 acc_train: 0.7643 loss_val: 1.2037 acc_val: 0.8033 time: 0.3780s\n",
      "Epoch: 0068 loss_train: 1.1185 acc_train: 0.7929 loss_val: 1.1944 acc_val: 0.8067 time: 0.3910s\n",
      "Epoch: 0069 loss_train: 1.0533 acc_train: 0.7786 loss_val: 1.1853 acc_val: 0.8100 time: 0.3874s\n",
      "Epoch: 0070 loss_train: 1.2053 acc_train: 0.6857 loss_val: 1.1767 acc_val: 0.8100 time: 0.3804s\n",
      "Epoch: 0071 loss_train: 1.0947 acc_train: 0.7143 loss_val: 1.1684 acc_val: 0.8167 time: 0.4427s\n",
      "Epoch: 0072 loss_train: 1.0573 acc_train: 0.7429 loss_val: 1.1601 acc_val: 0.8167 time: 0.3845s\n",
      "Epoch: 0073 loss_train: 1.0358 acc_train: 0.8071 loss_val: 1.1518 acc_val: 0.8200 time: 0.3998s\n",
      "Epoch: 0074 loss_train: 1.1066 acc_train: 0.7357 loss_val: 1.1439 acc_val: 0.8200 time: 0.4005s\n",
      "Epoch: 0075 loss_train: 1.0822 acc_train: 0.7500 loss_val: 1.1364 acc_val: 0.8200 time: 0.3956s\n",
      "Epoch: 0076 loss_train: 1.0803 acc_train: 0.7786 loss_val: 1.1289 acc_val: 0.8267 time: 0.4114s\n",
      "Epoch: 0077 loss_train: 1.1827 acc_train: 0.7429 loss_val: 1.1216 acc_val: 0.8300 time: 0.4068s\n",
      "Epoch: 0078 loss_train: 1.1355 acc_train: 0.7571 loss_val: 1.1147 acc_val: 0.8300 time: 0.4320s\n",
      "Epoch: 0079 loss_train: 1.0312 acc_train: 0.7429 loss_val: 1.1081 acc_val: 0.8333 time: 0.4204s\n",
      "Epoch: 0080 loss_train: 1.0619 acc_train: 0.7571 loss_val: 1.1020 acc_val: 0.8367 time: 0.4227s\n",
      "Epoch: 0081 loss_train: 1.0515 acc_train: 0.7714 loss_val: 1.0960 acc_val: 0.8333 time: 0.4008s\n",
      "Epoch: 0082 loss_train: 1.0257 acc_train: 0.7357 loss_val: 1.0901 acc_val: 0.8333 time: 0.3958s\n",
      "Epoch: 0083 loss_train: 1.0761 acc_train: 0.7714 loss_val: 1.0844 acc_val: 0.8367 time: 0.4397s\n",
      "Epoch: 0084 loss_train: 1.0065 acc_train: 0.7929 loss_val: 1.0788 acc_val: 0.8267 time: 0.3815s\n",
      "Epoch: 0085 loss_train: 1.0251 acc_train: 0.7786 loss_val: 1.0732 acc_val: 0.8333 time: 0.3879s\n",
      "Epoch: 0086 loss_train: 1.0041 acc_train: 0.7714 loss_val: 1.0678 acc_val: 0.8333 time: 0.3853s\n",
      "Epoch: 0087 loss_train: 1.0692 acc_train: 0.7714 loss_val: 1.0624 acc_val: 0.8300 time: 0.3851s\n",
      "Epoch: 0088 loss_train: 0.9925 acc_train: 0.7571 loss_val: 1.0569 acc_val: 0.8267 time: 0.3759s\n",
      "Epoch: 0089 loss_train: 1.0239 acc_train: 0.7286 loss_val: 1.0513 acc_val: 0.8267 time: 0.4016s\n",
      "Epoch: 0090 loss_train: 1.0134 acc_train: 0.7571 loss_val: 1.0458 acc_val: 0.8233 time: 0.3774s\n",
      "Epoch: 0091 loss_train: 1.0053 acc_train: 0.8000 loss_val: 1.0400 acc_val: 0.8233 time: 0.4152s\n",
      "Epoch: 0092 loss_train: 0.9716 acc_train: 0.7786 loss_val: 1.0340 acc_val: 0.8233 time: 0.3905s\n",
      "Epoch: 0093 loss_train: 0.9756 acc_train: 0.7857 loss_val: 1.0281 acc_val: 0.8233 time: 0.3997s\n",
      "Epoch: 0094 loss_train: 1.0617 acc_train: 0.7357 loss_val: 1.0223 acc_val: 0.8200 time: 0.4632s\n",
      "Epoch: 0095 loss_train: 0.9764 acc_train: 0.7714 loss_val: 1.0164 acc_val: 0.8200 time: 0.4019s\n",
      "Epoch: 0096 loss_train: 0.9796 acc_train: 0.8000 loss_val: 1.0105 acc_val: 0.8233 time: 0.3880s\n",
      "Epoch: 0097 loss_train: 0.8986 acc_train: 0.8000 loss_val: 1.0045 acc_val: 0.8233 time: 0.4176s\n",
      "Epoch: 0098 loss_train: 0.8934 acc_train: 0.7786 loss_val: 0.9984 acc_val: 0.8233 time: 0.4600s\n",
      "Epoch: 0099 loss_train: 0.9208 acc_train: 0.7786 loss_val: 0.9924 acc_val: 0.8233 time: 0.4578s\n",
      "Epoch: 0100 loss_train: 0.9414 acc_train: 0.7714 loss_val: 0.9866 acc_val: 0.8300 time: 0.4223s\n",
      "Epoch: 0101 loss_train: 0.9544 acc_train: 0.7786 loss_val: 0.9812 acc_val: 0.8300 time: 0.4212s\n",
      "Epoch: 0102 loss_train: 0.9704 acc_train: 0.7857 loss_val: 0.9758 acc_val: 0.8333 time: 0.3918s\n",
      "Epoch: 0103 loss_train: 1.0031 acc_train: 0.7357 loss_val: 0.9703 acc_val: 0.8333 time: 0.4023s\n",
      "Epoch: 0104 loss_train: 0.9438 acc_train: 0.7929 loss_val: 0.9649 acc_val: 0.8267 time: 0.4669s\n",
      "Epoch: 0105 loss_train: 0.9811 acc_train: 0.7500 loss_val: 0.9596 acc_val: 0.8267 time: 0.4623s\n",
      "Epoch: 0106 loss_train: 0.9506 acc_train: 0.8071 loss_val: 0.9544 acc_val: 0.8267 time: 0.4336s\n",
      "Epoch: 0107 loss_train: 0.9402 acc_train: 0.8000 loss_val: 0.9495 acc_val: 0.8300 time: 0.4462s\n",
      "Epoch: 0108 loss_train: 0.9251 acc_train: 0.7643 loss_val: 0.9449 acc_val: 0.8300 time: 0.4757s\n",
      "Epoch: 0109 loss_train: 0.8768 acc_train: 0.7857 loss_val: 0.9407 acc_val: 0.8333 time: 0.4246s\n",
      "Epoch: 0110 loss_train: 0.8493 acc_train: 0.8143 loss_val: 0.9366 acc_val: 0.8333 time: 0.3919s\n",
      "Epoch: 0111 loss_train: 0.9298 acc_train: 0.7857 loss_val: 0.9329 acc_val: 0.8333 time: 0.3898s\n",
      "Epoch: 0112 loss_train: 0.9367 acc_train: 0.7214 loss_val: 0.9294 acc_val: 0.8333 time: 0.3760s\n",
      "Epoch: 0113 loss_train: 0.8607 acc_train: 0.8071 loss_val: 0.9259 acc_val: 0.8333 time: 0.3858s\n",
      "Epoch: 0114 loss_train: 0.9376 acc_train: 0.7643 loss_val: 0.9224 acc_val: 0.8333 time: 0.3877s\n",
      "Epoch: 0115 loss_train: 0.8670 acc_train: 0.7857 loss_val: 0.9190 acc_val: 0.8400 time: 0.4028s\n",
      "Epoch: 0116 loss_train: 0.9259 acc_train: 0.7429 loss_val: 0.9159 acc_val: 0.8400 time: 0.3901s\n",
      "Epoch: 0117 loss_train: 0.9003 acc_train: 0.7714 loss_val: 0.9128 acc_val: 0.8467 time: 0.4072s\n",
      "Epoch: 0118 loss_train: 0.8755 acc_train: 0.8000 loss_val: 0.9098 acc_val: 0.8500 time: 0.3840s\n",
      "Epoch: 0119 loss_train: 0.9182 acc_train: 0.8143 loss_val: 0.9069 acc_val: 0.8467 time: 0.4426s\n",
      "Epoch: 0120 loss_train: 0.9159 acc_train: 0.7571 loss_val: 0.9041 acc_val: 0.8433 time: 0.3917s\n",
      "Epoch: 0121 loss_train: 0.9226 acc_train: 0.7786 loss_val: 0.9015 acc_val: 0.8433 time: 0.4113s\n",
      "Epoch: 0122 loss_train: 0.8389 acc_train: 0.8000 loss_val: 0.8988 acc_val: 0.8433 time: 0.4076s\n",
      "Epoch: 0123 loss_train: 0.8445 acc_train: 0.7857 loss_val: 0.8962 acc_val: 0.8433 time: 0.3953s\n",
      "Epoch: 0124 loss_train: 0.8931 acc_train: 0.7357 loss_val: 0.8938 acc_val: 0.8433 time: 0.3925s\n",
      "Epoch: 0125 loss_train: 0.9116 acc_train: 0.7643 loss_val: 0.8914 acc_val: 0.8400 time: 0.4017s\n",
      "Epoch: 0126 loss_train: 0.7693 acc_train: 0.7786 loss_val: 0.8890 acc_val: 0.8367 time: 0.3981s\n",
      "Epoch: 0127 loss_train: 0.8049 acc_train: 0.7857 loss_val: 0.8863 acc_val: 0.8367 time: 0.4545s\n",
      "Epoch: 0128 loss_train: 0.8896 acc_train: 0.7429 loss_val: 0.8839 acc_val: 0.8333 time: 0.4524s\n",
      "Epoch: 0129 loss_train: 0.8546 acc_train: 0.7857 loss_val: 0.8815 acc_val: 0.8333 time: 0.4406s\n",
      "Epoch: 0130 loss_train: 0.8011 acc_train: 0.8071 loss_val: 0.8790 acc_val: 0.8333 time: 0.4347s\n",
      "Epoch: 0131 loss_train: 0.9604 acc_train: 0.7214 loss_val: 0.8766 acc_val: 0.8333 time: 0.3934s\n",
      "Epoch: 0132 loss_train: 0.8980 acc_train: 0.7357 loss_val: 0.8744 acc_val: 0.8333 time: 0.4463s\n",
      "Epoch: 0133 loss_train: 0.9257 acc_train: 0.7286 loss_val: 0.8722 acc_val: 0.8367 time: 0.4586s\n",
      "Epoch: 0134 loss_train: 0.8356 acc_train: 0.7786 loss_val: 0.8699 acc_val: 0.8367 time: 0.3831s\n",
      "Epoch: 0135 loss_train: 0.8726 acc_train: 0.7929 loss_val: 0.8676 acc_val: 0.8367 time: 0.4532s\n",
      "Epoch: 0136 loss_train: 0.8841 acc_train: 0.7286 loss_val: 0.8653 acc_val: 0.8367 time: 0.4360s\n",
      "Epoch: 0137 loss_train: 0.8003 acc_train: 0.8214 loss_val: 0.8630 acc_val: 0.8367 time: 0.3981s\n",
      "Epoch: 0138 loss_train: 0.8566 acc_train: 0.8071 loss_val: 0.8608 acc_val: 0.8367 time: 0.4056s\n",
      "Epoch: 0139 loss_train: 0.7980 acc_train: 0.8357 loss_val: 0.8586 acc_val: 0.8333 time: 0.3937s\n",
      "Epoch: 0140 loss_train: 0.8612 acc_train: 0.7857 loss_val: 0.8564 acc_val: 0.8333 time: 0.3923s\n",
      "Epoch: 0141 loss_train: 0.8380 acc_train: 0.7643 loss_val: 0.8543 acc_val: 0.8367 time: 0.3944s\n",
      "Epoch: 0142 loss_train: 0.7430 acc_train: 0.7929 loss_val: 0.8522 acc_val: 0.8367 time: 0.4084s\n",
      "Epoch: 0143 loss_train: 0.8552 acc_train: 0.7500 loss_val: 0.8504 acc_val: 0.8367 time: 0.4461s\n",
      "Epoch: 0144 loss_train: 0.8800 acc_train: 0.8214 loss_val: 0.8483 acc_val: 0.8367 time: 0.4619s\n",
      "Epoch: 0145 loss_train: 0.8759 acc_train: 0.8000 loss_val: 0.8461 acc_val: 0.8367 time: 0.4587s\n",
      "Epoch: 0146 loss_train: 0.7234 acc_train: 0.8714 loss_val: 0.8437 acc_val: 0.8333 time: 0.4078s\n",
      "Epoch: 0147 loss_train: 0.7927 acc_train: 0.8214 loss_val: 0.8415 acc_val: 0.8333 time: 0.4529s\n",
      "Epoch: 0148 loss_train: 0.7044 acc_train: 0.8429 loss_val: 0.8391 acc_val: 0.8333 time: 0.4178s\n",
      "Epoch: 0149 loss_train: 0.8759 acc_train: 0.7929 loss_val: 0.8366 acc_val: 0.8333 time: 0.4036s\n",
      "Epoch: 0150 loss_train: 0.8741 acc_train: 0.7571 loss_val: 0.8341 acc_val: 0.8333 time: 0.4176s\n",
      "Epoch: 0151 loss_train: 0.7801 acc_train: 0.7643 loss_val: 0.8314 acc_val: 0.8333 time: 0.4063s\n",
      "Epoch: 0152 loss_train: 0.8016 acc_train: 0.8143 loss_val: 0.8294 acc_val: 0.8367 time: 0.3852s\n",
      "Epoch: 0153 loss_train: 0.8512 acc_train: 0.7714 loss_val: 0.8273 acc_val: 0.8367 time: 0.3814s\n",
      "Epoch: 0154 loss_train: 0.8688 acc_train: 0.7571 loss_val: 0.8257 acc_val: 0.8333 time: 0.4047s\n",
      "Epoch: 0155 loss_train: 0.7478 acc_train: 0.8214 loss_val: 0.8238 acc_val: 0.8333 time: 0.3995s\n",
      "Epoch: 0156 loss_train: 0.8529 acc_train: 0.7571 loss_val: 0.8221 acc_val: 0.8333 time: 0.3904s\n",
      "Epoch: 0157 loss_train: 0.7588 acc_train: 0.8500 loss_val: 0.8204 acc_val: 0.8333 time: 0.4404s\n",
      "Epoch: 0158 loss_train: 0.8180 acc_train: 0.7500 loss_val: 0.8188 acc_val: 0.8333 time: 0.3981s\n",
      "Epoch: 0159 loss_train: 0.8438 acc_train: 0.7857 loss_val: 0.8171 acc_val: 0.8333 time: 0.3967s\n",
      "Epoch: 0160 loss_train: 0.8159 acc_train: 0.8000 loss_val: 0.8150 acc_val: 0.8333 time: 0.3967s\n",
      "Epoch: 0161 loss_train: 0.8272 acc_train: 0.7643 loss_val: 0.8131 acc_val: 0.8333 time: 0.4374s\n",
      "Epoch: 0162 loss_train: 0.7508 acc_train: 0.7857 loss_val: 0.8112 acc_val: 0.8367 time: 0.4255s\n",
      "Epoch: 0163 loss_train: 0.7901 acc_train: 0.7929 loss_val: 0.8094 acc_val: 0.8367 time: 0.4020s\n",
      "Epoch: 0164 loss_train: 0.6643 acc_train: 0.8500 loss_val: 0.8076 acc_val: 0.8367 time: 0.3820s\n",
      "Epoch: 0165 loss_train: 0.8141 acc_train: 0.8143 loss_val: 0.8057 acc_val: 0.8333 time: 0.4017s\n",
      "Epoch: 0166 loss_train: 0.7663 acc_train: 0.8000 loss_val: 0.8037 acc_val: 0.8333 time: 0.4329s\n",
      "Epoch: 0167 loss_train: 0.7951 acc_train: 0.8000 loss_val: 0.8018 acc_val: 0.8333 time: 0.3871s\n",
      "Epoch: 0168 loss_train: 0.7667 acc_train: 0.7857 loss_val: 0.7999 acc_val: 0.8333 time: 0.3992s\n",
      "Epoch: 0169 loss_train: 0.6842 acc_train: 0.8714 loss_val: 0.7982 acc_val: 0.8367 time: 0.4053s\n",
      "Epoch: 0170 loss_train: 0.7225 acc_train: 0.8143 loss_val: 0.7966 acc_val: 0.8333 time: 0.4500s\n",
      "Epoch: 0171 loss_train: 0.7458 acc_train: 0.7929 loss_val: 0.7949 acc_val: 0.8300 time: 0.4467s\n",
      "Epoch: 0172 loss_train: 0.7744 acc_train: 0.7786 loss_val: 0.7933 acc_val: 0.8300 time: 0.4118s\n",
      "Epoch: 0173 loss_train: 0.7713 acc_train: 0.8000 loss_val: 0.7917 acc_val: 0.8300 time: 0.4105s\n",
      "Epoch: 0174 loss_train: 0.7283 acc_train: 0.8429 loss_val: 0.7900 acc_val: 0.8300 time: 0.4155s\n",
      "Epoch: 0175 loss_train: 0.7983 acc_train: 0.8071 loss_val: 0.7887 acc_val: 0.8267 time: 0.3841s\n",
      "Epoch: 0176 loss_train: 0.7997 acc_train: 0.8357 loss_val: 0.7873 acc_val: 0.8267 time: 0.4342s\n",
      "Epoch: 0177 loss_train: 0.8320 acc_train: 0.7714 loss_val: 0.7860 acc_val: 0.8267 time: 0.3977s\n",
      "Epoch: 0178 loss_train: 0.7078 acc_train: 0.8071 loss_val: 0.7848 acc_val: 0.8267 time: 0.3893s\n",
      "Epoch: 0179 loss_train: 0.8097 acc_train: 0.7786 loss_val: 0.7837 acc_val: 0.8267 time: 0.4226s\n",
      "Epoch: 0180 loss_train: 0.7335 acc_train: 0.8000 loss_val: 0.7826 acc_val: 0.8267 time: 0.4188s\n",
      "Epoch: 0181 loss_train: 0.7941 acc_train: 0.7643 loss_val: 0.7818 acc_val: 0.8267 time: 0.4133s\n",
      "Epoch: 0182 loss_train: 0.7049 acc_train: 0.8000 loss_val: 0.7811 acc_val: 0.8267 time: 0.4519s\n",
      "Epoch: 0183 loss_train: 0.8021 acc_train: 0.7714 loss_val: 0.7804 acc_val: 0.8233 time: 0.4810s\n",
      "Epoch: 0184 loss_train: 0.8606 acc_train: 0.7643 loss_val: 0.7799 acc_val: 0.8233 time: 0.3930s\n",
      "Epoch: 0185 loss_train: 0.7546 acc_train: 0.8000 loss_val: 0.7794 acc_val: 0.8233 time: 0.4248s\n",
      "Epoch: 0186 loss_train: 0.7196 acc_train: 0.8286 loss_val: 0.7787 acc_val: 0.8233 time: 0.4017s\n",
      "Epoch: 0187 loss_train: 0.7568 acc_train: 0.8214 loss_val: 0.7781 acc_val: 0.8200 time: 0.4220s\n",
      "Epoch: 0188 loss_train: 0.7602 acc_train: 0.8000 loss_val: 0.7774 acc_val: 0.8200 time: 0.3888s\n",
      "Epoch: 0189 loss_train: 0.7453 acc_train: 0.7929 loss_val: 0.7765 acc_val: 0.8200 time: 0.4488s\n",
      "Epoch: 0190 loss_train: 0.6419 acc_train: 0.8714 loss_val: 0.7751 acc_val: 0.8233 time: 0.4006s\n",
      "Epoch: 0191 loss_train: 0.7869 acc_train: 0.7857 loss_val: 0.7738 acc_val: 0.8233 time: 0.3984s\n",
      "Epoch: 0192 loss_train: 0.7603 acc_train: 0.7714 loss_val: 0.7725 acc_val: 0.8233 time: 0.4169s\n",
      "Epoch: 0193 loss_train: 0.8327 acc_train: 0.7143 loss_val: 0.7712 acc_val: 0.8267 time: 0.3942s\n",
      "Epoch: 0194 loss_train: 0.7196 acc_train: 0.7857 loss_val: 0.7701 acc_val: 0.8267 time: 0.4103s\n",
      "Epoch: 0195 loss_train: 0.7381 acc_train: 0.8357 loss_val: 0.7689 acc_val: 0.8267 time: 0.3760s\n",
      "Epoch: 0196 loss_train: 0.7206 acc_train: 0.8143 loss_val: 0.7676 acc_val: 0.8267 time: 0.4460s\n",
      "Epoch: 0197 loss_train: 0.7266 acc_train: 0.7929 loss_val: 0.7663 acc_val: 0.8267 time: 0.3857s\n",
      "Epoch: 0198 loss_train: 0.7332 acc_train: 0.8143 loss_val: 0.7648 acc_val: 0.8267 time: 0.3916s\n",
      "Epoch: 0199 loss_train: 0.8355 acc_train: 0.7571 loss_val: 0.7635 acc_val: 0.8267 time: 0.4321s\n",
      "Epoch: 0200 loss_train: 0.6640 acc_train: 0.8500 loss_val: 0.7620 acc_val: 0.8267 time: 0.4227s\n",
      "Epoch: 0201 loss_train: 0.7863 acc_train: 0.7714 loss_val: 0.7606 acc_val: 0.8267 time: 0.4108s\n",
      "Epoch: 0202 loss_train: 0.6653 acc_train: 0.8286 loss_val: 0.7593 acc_val: 0.8233 time: 0.3851s\n",
      "Epoch: 0203 loss_train: 0.7057 acc_train: 0.8143 loss_val: 0.7580 acc_val: 0.8233 time: 0.4275s\n",
      "Epoch: 0204 loss_train: 0.6455 acc_train: 0.8214 loss_val: 0.7565 acc_val: 0.8233 time: 0.3899s\n",
      "Epoch: 0205 loss_train: 0.7071 acc_train: 0.7929 loss_val: 0.7547 acc_val: 0.8233 time: 0.3868s\n",
      "Epoch: 0206 loss_train: 0.6931 acc_train: 0.8286 loss_val: 0.7529 acc_val: 0.8267 time: 0.3749s\n",
      "Epoch: 0207 loss_train: 0.7210 acc_train: 0.8286 loss_val: 0.7515 acc_val: 0.8233 time: 0.4488s\n",
      "Epoch: 0208 loss_train: 0.7219 acc_train: 0.8500 loss_val: 0.7501 acc_val: 0.8233 time: 0.4000s\n",
      "Epoch: 0209 loss_train: 0.7836 acc_train: 0.7786 loss_val: 0.7486 acc_val: 0.8233 time: 0.3814s\n",
      "Epoch: 0210 loss_train: 0.7828 acc_train: 0.7857 loss_val: 0.7471 acc_val: 0.8233 time: 0.4183s\n",
      "Epoch: 0211 loss_train: 0.7224 acc_train: 0.8143 loss_val: 0.7455 acc_val: 0.8233 time: 0.3886s\n",
      "Epoch: 0212 loss_train: 0.8090 acc_train: 0.7786 loss_val: 0.7440 acc_val: 0.8267 time: 0.3956s\n",
      "Epoch: 0213 loss_train: 0.7317 acc_train: 0.7643 loss_val: 0.7430 acc_val: 0.8267 time: 0.4084s\n",
      "Epoch: 0214 loss_train: 0.6803 acc_train: 0.8357 loss_val: 0.7419 acc_val: 0.8267 time: 0.4440s\n",
      "Epoch: 0215 loss_train: 0.5930 acc_train: 0.8500 loss_val: 0.7412 acc_val: 0.8267 time: 0.3912s\n",
      "Epoch: 0216 loss_train: 0.7846 acc_train: 0.7571 loss_val: 0.7405 acc_val: 0.8300 time: 0.4092s\n",
      "Epoch: 0217 loss_train: 0.7115 acc_train: 0.8143 loss_val: 0.7400 acc_val: 0.8300 time: 0.3995s\n",
      "Epoch: 0218 loss_train: 0.7720 acc_train: 0.8071 loss_val: 0.7394 acc_val: 0.8300 time: 0.3942s\n",
      "Epoch: 0219 loss_train: 0.5945 acc_train: 0.8714 loss_val: 0.7387 acc_val: 0.8300 time: 0.4439s\n",
      "Epoch: 0220 loss_train: 0.7166 acc_train: 0.7929 loss_val: 0.7382 acc_val: 0.8267 time: 0.3817s\n",
      "Epoch: 0221 loss_train: 0.7719 acc_train: 0.7643 loss_val: 0.7376 acc_val: 0.8233 time: 0.4118s\n",
      "Epoch: 0222 loss_train: 0.7635 acc_train: 0.8071 loss_val: 0.7372 acc_val: 0.8200 time: 0.3838s\n",
      "Epoch: 0223 loss_train: 0.7585 acc_train: 0.7714 loss_val: 0.7370 acc_val: 0.8200 time: 0.3898s\n",
      "Epoch: 0224 loss_train: 0.7693 acc_train: 0.7786 loss_val: 0.7367 acc_val: 0.8200 time: 0.4237s\n",
      "Epoch: 0225 loss_train: 0.8105 acc_train: 0.7571 loss_val: 0.7365 acc_val: 0.8200 time: 0.3819s\n",
      "Epoch: 0226 loss_train: 0.7462 acc_train: 0.8000 loss_val: 0.7362 acc_val: 0.8167 time: 0.3880s\n",
      "Epoch: 0227 loss_train: 0.6815 acc_train: 0.8286 loss_val: 0.7359 acc_val: 0.8167 time: 0.4204s\n",
      "Epoch: 0228 loss_train: 0.7996 acc_train: 0.7714 loss_val: 0.7354 acc_val: 0.8167 time: 0.4603s\n",
      "Epoch: 0229 loss_train: 0.6984 acc_train: 0.7714 loss_val: 0.7349 acc_val: 0.8167 time: 0.4157s\n",
      "Epoch: 0230 loss_train: 0.6491 acc_train: 0.8571 loss_val: 0.7345 acc_val: 0.8167 time: 0.4181s\n",
      "Epoch: 0231 loss_train: 0.7753 acc_train: 0.7643 loss_val: 0.7342 acc_val: 0.8167 time: 0.4196s\n",
      "Epoch: 0232 loss_train: 0.7865 acc_train: 0.7643 loss_val: 0.7337 acc_val: 0.8167 time: 0.3828s\n",
      "Epoch: 0233 loss_train: 0.7490 acc_train: 0.8143 loss_val: 0.7332 acc_val: 0.8167 time: 0.3989s\n",
      "Epoch: 0234 loss_train: 0.7199 acc_train: 0.7714 loss_val: 0.7329 acc_val: 0.8167 time: 0.3941s\n",
      "Epoch: 0235 loss_train: 0.7596 acc_train: 0.7857 loss_val: 0.7325 acc_val: 0.8167 time: 0.3854s\n",
      "Epoch: 0236 loss_train: 0.6963 acc_train: 0.8286 loss_val: 0.7323 acc_val: 0.8167 time: 0.3811s\n",
      "Epoch: 0237 loss_train: 0.6964 acc_train: 0.8429 loss_val: 0.7320 acc_val: 0.8167 time: 0.3846s\n",
      "Epoch: 0238 loss_train: 0.7389 acc_train: 0.8143 loss_val: 0.7317 acc_val: 0.8167 time: 0.4013s\n",
      "Epoch: 0239 loss_train: 0.7211 acc_train: 0.7929 loss_val: 0.7317 acc_val: 0.8200 time: 0.4027s\n",
      "Epoch: 0240 loss_train: 0.6056 acc_train: 0.8571 loss_val: 0.7314 acc_val: 0.8200 time: 0.4604s\n",
      "Epoch: 0241 loss_train: 0.7617 acc_train: 0.8214 loss_val: 0.7311 acc_val: 0.8200 time: 0.3897s\n",
      "Epoch: 0242 loss_train: 0.6639 acc_train: 0.8000 loss_val: 0.7306 acc_val: 0.8200 time: 0.4060s\n",
      "Epoch: 0243 loss_train: 0.6949 acc_train: 0.8000 loss_val: 0.7301 acc_val: 0.8200 time: 0.3906s\n",
      "Epoch: 0244 loss_train: 0.7375 acc_train: 0.8071 loss_val: 0.7292 acc_val: 0.8200 time: 0.4042s\n",
      "Epoch: 0245 loss_train: 0.5728 acc_train: 0.8714 loss_val: 0.7282 acc_val: 0.8200 time: 0.3832s\n",
      "Epoch: 0246 loss_train: 0.7155 acc_train: 0.7786 loss_val: 0.7272 acc_val: 0.8200 time: 0.4366s\n",
      "Epoch: 0247 loss_train: 0.7381 acc_train: 0.8143 loss_val: 0.7264 acc_val: 0.8200 time: 0.4611s\n",
      "Epoch: 0248 loss_train: 0.8251 acc_train: 0.7714 loss_val: 0.7254 acc_val: 0.8200 time: 0.4185s\n",
      "Epoch: 0249 loss_train: 0.7357 acc_train: 0.8143 loss_val: 0.7252 acc_val: 0.8200 time: 0.3892s\n",
      "Epoch: 0250 loss_train: 0.8537 acc_train: 0.6929 loss_val: 0.7251 acc_val: 0.8200 time: 0.4232s\n",
      "Epoch: 0251 loss_train: 0.6413 acc_train: 0.8214 loss_val: 0.7251 acc_val: 0.8233 time: 0.4608s\n",
      "Epoch: 0252 loss_train: 0.6055 acc_train: 0.8571 loss_val: 0.7250 acc_val: 0.8233 time: 0.4469s\n",
      "Epoch: 0253 loss_train: 0.7173 acc_train: 0.8000 loss_val: 0.7245 acc_val: 0.8233 time: 0.3984s\n",
      "Epoch: 0254 loss_train: 0.6906 acc_train: 0.8071 loss_val: 0.7239 acc_val: 0.8233 time: 0.3907s\n",
      "Epoch: 0255 loss_train: 0.6574 acc_train: 0.8286 loss_val: 0.7231 acc_val: 0.8233 time: 0.4538s\n",
      "Epoch: 0256 loss_train: 0.7481 acc_train: 0.7786 loss_val: 0.7222 acc_val: 0.8233 time: 0.4454s\n",
      "Epoch: 0257 loss_train: 0.6660 acc_train: 0.7929 loss_val: 0.7216 acc_val: 0.8233 time: 0.3951s\n",
      "Epoch: 0258 loss_train: 0.6197 acc_train: 0.8429 loss_val: 0.7207 acc_val: 0.8233 time: 0.3848s\n",
      "Epoch: 0259 loss_train: 0.6830 acc_train: 0.7929 loss_val: 0.7200 acc_val: 0.8233 time: 0.4317s\n",
      "Epoch: 0260 loss_train: 0.7832 acc_train: 0.7571 loss_val: 0.7194 acc_val: 0.8233 time: 0.4370s\n",
      "Epoch: 0261 loss_train: 0.7103 acc_train: 0.8143 loss_val: 0.7187 acc_val: 0.8233 time: 0.4416s\n",
      "Epoch: 0262 loss_train: 0.7377 acc_train: 0.8000 loss_val: 0.7180 acc_val: 0.8233 time: 0.4146s\n",
      "Epoch: 0263 loss_train: 0.6052 acc_train: 0.8429 loss_val: 0.7172 acc_val: 0.8233 time: 0.4616s\n",
      "Epoch: 0264 loss_train: 0.6707 acc_train: 0.8286 loss_val: 0.7164 acc_val: 0.8200 time: 0.3902s\n",
      "Epoch: 0265 loss_train: 0.7050 acc_train: 0.8071 loss_val: 0.7155 acc_val: 0.8200 time: 0.4177s\n",
      "Epoch: 0266 loss_train: 0.7121 acc_train: 0.7929 loss_val: 0.7147 acc_val: 0.8200 time: 0.4643s\n",
      "Epoch: 0267 loss_train: 0.6561 acc_train: 0.8143 loss_val: 0.7142 acc_val: 0.8200 time: 0.4504s\n",
      "Epoch: 0268 loss_train: 0.6257 acc_train: 0.8214 loss_val: 0.7138 acc_val: 0.8200 time: 0.4251s\n",
      "Epoch: 0269 loss_train: 0.5943 acc_train: 0.8429 loss_val: 0.7136 acc_val: 0.8200 time: 0.3935s\n",
      "Epoch: 0270 loss_train: 0.7043 acc_train: 0.7857 loss_val: 0.7134 acc_val: 0.8233 time: 0.4179s\n",
      "Epoch: 0271 loss_train: 0.6700 acc_train: 0.8071 loss_val: 0.7132 acc_val: 0.8233 time: 0.4327s\n",
      "Epoch: 0272 loss_train: 0.5818 acc_train: 0.8429 loss_val: 0.7131 acc_val: 0.8233 time: 0.3834s\n",
      "Epoch: 0273 loss_train: 0.6943 acc_train: 0.7714 loss_val: 0.7129 acc_val: 0.8233 time: 0.4106s\n",
      "Epoch: 0274 loss_train: 0.7156 acc_train: 0.8214 loss_val: 0.7126 acc_val: 0.8233 time: 0.4313s\n",
      "Epoch: 0275 loss_train: 0.6456 acc_train: 0.8643 loss_val: 0.7122 acc_val: 0.8233 time: 0.4000s\n",
      "Epoch: 0276 loss_train: 0.7014 acc_train: 0.8071 loss_val: 0.7118 acc_val: 0.8233 time: 0.3857s\n",
      "Epoch: 0277 loss_train: 0.6104 acc_train: 0.8571 loss_val: 0.7114 acc_val: 0.8233 time: 0.4507s\n",
      "Epoch: 0278 loss_train: 0.6015 acc_train: 0.8500 loss_val: 0.7111 acc_val: 0.8233 time: 0.4600s\n",
      "Epoch: 0279 loss_train: 0.6580 acc_train: 0.8143 loss_val: 0.7108 acc_val: 0.8233 time: 0.3836s\n",
      "Epoch: 0280 loss_train: 0.6462 acc_train: 0.8143 loss_val: 0.7105 acc_val: 0.8233 time: 0.4440s\n",
      "Epoch: 0281 loss_train: 0.5709 acc_train: 0.8571 loss_val: 0.7101 acc_val: 0.8233 time: 0.4530s\n",
      "Epoch: 0282 loss_train: 0.7012 acc_train: 0.8286 loss_val: 0.7093 acc_val: 0.8233 time: 0.4397s\n",
      "Epoch: 0283 loss_train: 0.6527 acc_train: 0.8214 loss_val: 0.7086 acc_val: 0.8233 time: 0.3760s\n",
      "Epoch: 0284 loss_train: 0.6577 acc_train: 0.8214 loss_val: 0.7081 acc_val: 0.8200 time: 0.3846s\n",
      "Epoch: 0285 loss_train: 0.7273 acc_train: 0.7571 loss_val: 0.7075 acc_val: 0.8200 time: 0.3973s\n",
      "Epoch: 0286 loss_train: 0.6313 acc_train: 0.8143 loss_val: 0.7069 acc_val: 0.8200 time: 0.3799s\n",
      "Epoch: 0287 loss_train: 0.6472 acc_train: 0.8286 loss_val: 0.7062 acc_val: 0.8200 time: 0.3952s\n",
      "Epoch: 0288 loss_train: 0.5558 acc_train: 0.8429 loss_val: 0.7056 acc_val: 0.8200 time: 0.3919s\n",
      "Epoch: 0289 loss_train: 0.7970 acc_train: 0.7714 loss_val: 0.7051 acc_val: 0.8200 time: 0.4003s\n",
      "Epoch: 0290 loss_train: 0.7471 acc_train: 0.7786 loss_val: 0.7048 acc_val: 0.8200 time: 0.3899s\n",
      "Epoch: 0291 loss_train: 0.6885 acc_train: 0.8357 loss_val: 0.7043 acc_val: 0.8200 time: 0.3814s\n",
      "Epoch: 0292 loss_train: 0.7509 acc_train: 0.7714 loss_val: 0.7038 acc_val: 0.8200 time: 0.4366s\n",
      "Epoch: 0293 loss_train: 0.6552 acc_train: 0.8214 loss_val: 0.7033 acc_val: 0.8200 time: 0.3833s\n",
      "Epoch: 0294 loss_train: 0.6026 acc_train: 0.7929 loss_val: 0.7029 acc_val: 0.8200 time: 0.4543s\n",
      "Epoch: 0295 loss_train: 0.6817 acc_train: 0.8143 loss_val: 0.7022 acc_val: 0.8200 time: 0.4371s\n",
      "Epoch: 0296 loss_train: 0.6569 acc_train: 0.8286 loss_val: 0.7017 acc_val: 0.8200 time: 0.4387s\n",
      "Epoch: 0297 loss_train: 0.7115 acc_train: 0.8071 loss_val: 0.7013 acc_val: 0.8200 time: 0.4017s\n",
      "Epoch: 0298 loss_train: 0.7532 acc_train: 0.7643 loss_val: 0.7007 acc_val: 0.8200 time: 0.3987s\n",
      "Epoch: 0299 loss_train: 0.7257 acc_train: 0.7857 loss_val: 0.7003 acc_val: 0.8200 time: 0.3807s\n",
      "Epoch: 0300 loss_train: 0.6392 acc_train: 0.8286 loss_val: 0.6999 acc_val: 0.8200 time: 0.4039s\n",
      "Epoch: 0301 loss_train: 0.7234 acc_train: 0.7857 loss_val: 0.6997 acc_val: 0.8200 time: 0.3798s\n",
      "Epoch: 0302 loss_train: 0.5611 acc_train: 0.8571 loss_val: 0.6996 acc_val: 0.8167 time: 0.3836s\n",
      "Epoch: 0303 loss_train: 0.6059 acc_train: 0.8286 loss_val: 0.6995 acc_val: 0.8167 time: 0.4566s\n",
      "Epoch: 0304 loss_train: 0.6094 acc_train: 0.8571 loss_val: 0.6992 acc_val: 0.8133 time: 0.4582s\n",
      "Epoch: 0305 loss_train: 0.6488 acc_train: 0.8286 loss_val: 0.6988 acc_val: 0.8133 time: 0.4565s\n",
      "Epoch: 0306 loss_train: 0.6593 acc_train: 0.8357 loss_val: 0.6985 acc_val: 0.8167 time: 0.4315s\n",
      "Epoch: 0307 loss_train: 0.6323 acc_train: 0.8071 loss_val: 0.6982 acc_val: 0.8167 time: 0.4495s\n",
      "Epoch: 0308 loss_train: 0.6199 acc_train: 0.8357 loss_val: 0.6980 acc_val: 0.8167 time: 0.4439s\n",
      "Epoch: 0309 loss_train: 0.6879 acc_train: 0.7857 loss_val: 0.6980 acc_val: 0.8167 time: 0.4248s\n",
      "Epoch: 0310 loss_train: 0.6580 acc_train: 0.7929 loss_val: 0.6984 acc_val: 0.8167 time: 0.4129s\n",
      "Epoch: 0311 loss_train: 0.6605 acc_train: 0.8000 loss_val: 0.6988 acc_val: 0.8167 time: 0.4661s\n",
      "Epoch: 0312 loss_train: 0.8584 acc_train: 0.7286 loss_val: 0.6993 acc_val: 0.8200 time: 0.3883s\n",
      "Epoch: 0313 loss_train: 0.6329 acc_train: 0.8571 loss_val: 0.7000 acc_val: 0.8200 time: 0.4099s\n",
      "Epoch: 0314 loss_train: 0.6995 acc_train: 0.8000 loss_val: 0.7004 acc_val: 0.8200 time: 0.4013s\n",
      "Epoch: 0315 loss_train: 0.6958 acc_train: 0.7857 loss_val: 0.7005 acc_val: 0.8200 time: 0.3904s\n",
      "Epoch: 0316 loss_train: 0.6863 acc_train: 0.7714 loss_val: 0.7009 acc_val: 0.8167 time: 0.4788s\n",
      "Epoch: 0317 loss_train: 0.7168 acc_train: 0.7714 loss_val: 0.7013 acc_val: 0.8167 time: 0.4778s\n",
      "Epoch: 0318 loss_train: 0.6316 acc_train: 0.8357 loss_val: 0.7014 acc_val: 0.8167 time: 0.4298s\n",
      "Epoch: 0319 loss_train: 0.5862 acc_train: 0.8214 loss_val: 0.7012 acc_val: 0.8167 time: 0.3827s\n",
      "Epoch: 0320 loss_train: 0.5729 acc_train: 0.8571 loss_val: 0.7008 acc_val: 0.8167 time: 0.4089s\n",
      "Epoch: 0321 loss_train: 0.7217 acc_train: 0.7857 loss_val: 0.7005 acc_val: 0.8167 time: 0.4011s\n",
      "Epoch: 0322 loss_train: 0.6497 acc_train: 0.8214 loss_val: 0.7001 acc_val: 0.8167 time: 0.4524s\n",
      "Epoch: 0323 loss_train: 0.5767 acc_train: 0.8071 loss_val: 0.6996 acc_val: 0.8167 time: 0.4569s\n",
      "Epoch: 0324 loss_train: 0.7371 acc_train: 0.7357 loss_val: 0.6992 acc_val: 0.8167 time: 0.4349s\n",
      "Epoch: 0325 loss_train: 0.7052 acc_train: 0.7929 loss_val: 0.6987 acc_val: 0.8167 time: 0.3758s\n",
      "Epoch: 0326 loss_train: 0.6488 acc_train: 0.8000 loss_val: 0.6985 acc_val: 0.8167 time: 0.4320s\n",
      "Epoch: 0327 loss_train: 0.6651 acc_train: 0.8429 loss_val: 0.6982 acc_val: 0.8167 time: 0.4508s\n",
      "Epoch: 0328 loss_train: 0.6490 acc_train: 0.8429 loss_val: 0.6979 acc_val: 0.8167 time: 0.3956s\n",
      "Epoch: 0329 loss_train: 0.7369 acc_train: 0.7929 loss_val: 0.6973 acc_val: 0.8167 time: 0.3862s\n",
      "Epoch: 0330 loss_train: 0.6015 acc_train: 0.8500 loss_val: 0.6971 acc_val: 0.8167 time: 0.3891s\n",
      "Epoch: 0331 loss_train: 0.5465 acc_train: 0.8857 loss_val: 0.6965 acc_val: 0.8167 time: 0.3887s\n",
      "Epoch: 0332 loss_train: 0.6503 acc_train: 0.8000 loss_val: 0.6957 acc_val: 0.8167 time: 0.3974s\n",
      "Epoch: 0333 loss_train: 0.6406 acc_train: 0.8000 loss_val: 0.6952 acc_val: 0.8167 time: 0.3813s\n",
      "Epoch: 0334 loss_train: 0.6718 acc_train: 0.8071 loss_val: 0.6946 acc_val: 0.8167 time: 0.3861s\n",
      "Epoch: 0335 loss_train: 0.6485 acc_train: 0.8357 loss_val: 0.6940 acc_val: 0.8167 time: 0.4713s\n",
      "Epoch: 0336 loss_train: 0.6490 acc_train: 0.8000 loss_val: 0.6936 acc_val: 0.8167 time: 0.3869s\n",
      "Epoch: 0337 loss_train: 0.6901 acc_train: 0.7929 loss_val: 0.6932 acc_val: 0.8167 time: 0.4029s\n",
      "Epoch: 0338 loss_train: 0.7437 acc_train: 0.7571 loss_val: 0.6929 acc_val: 0.8167 time: 0.4307s\n",
      "Epoch: 0339 loss_train: 0.6580 acc_train: 0.7857 loss_val: 0.6925 acc_val: 0.8167 time: 0.4372s\n",
      "Epoch: 0340 loss_train: 0.7243 acc_train: 0.8143 loss_val: 0.6918 acc_val: 0.8167 time: 0.3822s\n",
      "Epoch: 0341 loss_train: 0.6983 acc_train: 0.8000 loss_val: 0.6910 acc_val: 0.8167 time: 0.4088s\n",
      "Epoch: 0342 loss_train: 0.6628 acc_train: 0.8214 loss_val: 0.6906 acc_val: 0.8167 time: 0.3957s\n",
      "Epoch: 0343 loss_train: 0.7344 acc_train: 0.7571 loss_val: 0.6906 acc_val: 0.8167 time: 0.3952s\n",
      "Epoch: 0344 loss_train: 0.7133 acc_train: 0.8429 loss_val: 0.6904 acc_val: 0.8167 time: 0.3720s\n",
      "Epoch: 0345 loss_train: 0.5898 acc_train: 0.8500 loss_val: 0.6903 acc_val: 0.8167 time: 0.4280s\n",
      "Epoch: 0346 loss_train: 0.7069 acc_train: 0.8000 loss_val: 0.6901 acc_val: 0.8167 time: 0.4499s\n",
      "Epoch: 0347 loss_train: 0.7024 acc_train: 0.7857 loss_val: 0.6902 acc_val: 0.8167 time: 0.3892s\n",
      "Epoch: 0348 loss_train: 0.6089 acc_train: 0.8286 loss_val: 0.6902 acc_val: 0.8167 time: 0.4118s\n",
      "Epoch: 0349 loss_train: 0.6818 acc_train: 0.8071 loss_val: 0.6900 acc_val: 0.8167 time: 0.3949s\n",
      "Epoch: 0350 loss_train: 0.5923 acc_train: 0.8429 loss_val: 0.6895 acc_val: 0.8167 time: 0.4612s\n",
      "Epoch: 0351 loss_train: 0.6402 acc_train: 0.8143 loss_val: 0.6892 acc_val: 0.8167 time: 0.4631s\n",
      "Epoch: 0352 loss_train: 0.6207 acc_train: 0.8500 loss_val: 0.6890 acc_val: 0.8167 time: 0.4293s\n",
      "Epoch: 0353 loss_train: 0.5610 acc_train: 0.8357 loss_val: 0.6885 acc_val: 0.8167 time: 0.3834s\n",
      "Epoch: 0354 loss_train: 0.7130 acc_train: 0.7857 loss_val: 0.6881 acc_val: 0.8167 time: 0.3989s\n",
      "Epoch: 0355 loss_train: 0.6926 acc_train: 0.8000 loss_val: 0.6876 acc_val: 0.8167 time: 0.4071s\n",
      "Epoch: 0356 loss_train: 0.5602 acc_train: 0.8500 loss_val: 0.6872 acc_val: 0.8167 time: 0.3917s\n",
      "Epoch: 0357 loss_train: 0.7518 acc_train: 0.7714 loss_val: 0.6869 acc_val: 0.8167 time: 0.4127s\n",
      "Epoch: 0358 loss_train: 0.6158 acc_train: 0.8429 loss_val: 0.6864 acc_val: 0.8167 time: 0.3946s\n",
      "Epoch: 0359 loss_train: 0.7339 acc_train: 0.7929 loss_val: 0.6862 acc_val: 0.8167 time: 0.4490s\n",
      "Epoch: 0360 loss_train: 0.6890 acc_train: 0.7929 loss_val: 0.6857 acc_val: 0.8167 time: 0.3835s\n",
      "Epoch: 0361 loss_train: 0.6814 acc_train: 0.8071 loss_val: 0.6856 acc_val: 0.8167 time: 0.4519s\n",
      "Epoch: 0362 loss_train: 0.7093 acc_train: 0.7500 loss_val: 0.6852 acc_val: 0.8167 time: 0.3767s\n",
      "Epoch: 0363 loss_train: 0.7120 acc_train: 0.7643 loss_val: 0.6851 acc_val: 0.8167 time: 0.3744s\n",
      "Epoch: 0364 loss_train: 0.6380 acc_train: 0.8071 loss_val: 0.6848 acc_val: 0.8167 time: 0.3977s\n",
      "Epoch: 0365 loss_train: 0.6610 acc_train: 0.8000 loss_val: 0.6846 acc_val: 0.8167 time: 0.3863s\n",
      "Epoch: 0366 loss_train: 0.6983 acc_train: 0.8000 loss_val: 0.6843 acc_val: 0.8167 time: 0.3992s\n",
      "Epoch: 0367 loss_train: 0.7842 acc_train: 0.7714 loss_val: 0.6842 acc_val: 0.8167 time: 0.3799s\n",
      "Epoch: 0368 loss_train: 0.6632 acc_train: 0.8143 loss_val: 0.6840 acc_val: 0.8167 time: 0.3965s\n",
      "Epoch: 0369 loss_train: 0.7087 acc_train: 0.7786 loss_val: 0.6838 acc_val: 0.8167 time: 0.4485s\n",
      "Epoch: 0370 loss_train: 0.6460 acc_train: 0.8071 loss_val: 0.6835 acc_val: 0.8167 time: 0.4650s\n",
      "Epoch: 0371 loss_train: 0.6359 acc_train: 0.8143 loss_val: 0.6834 acc_val: 0.8167 time: 0.3855s\n",
      "Epoch: 0372 loss_train: 0.6945 acc_train: 0.7929 loss_val: 0.6832 acc_val: 0.8167 time: 0.4051s\n",
      "Epoch: 0373 loss_train: 0.6275 acc_train: 0.8214 loss_val: 0.6829 acc_val: 0.8167 time: 0.3909s\n",
      "Epoch: 0374 loss_train: 0.7276 acc_train: 0.7786 loss_val: 0.6825 acc_val: 0.8167 time: 0.3872s\n",
      "Epoch: 0375 loss_train: 0.7663 acc_train: 0.7786 loss_val: 0.6821 acc_val: 0.8167 time: 0.3955s\n",
      "Epoch: 0376 loss_train: 0.6982 acc_train: 0.8214 loss_val: 0.6816 acc_val: 0.8167 time: 0.4278s\n",
      "Epoch: 0377 loss_train: 0.7395 acc_train: 0.7929 loss_val: 0.6815 acc_val: 0.8167 time: 0.4374s\n",
      "Epoch: 0378 loss_train: 0.6365 acc_train: 0.8143 loss_val: 0.6813 acc_val: 0.8167 time: 0.4074s\n",
      "Epoch: 0379 loss_train: 0.6780 acc_train: 0.7857 loss_val: 0.6813 acc_val: 0.8167 time: 0.4053s\n",
      "Epoch: 0380 loss_train: 0.6833 acc_train: 0.8214 loss_val: 0.6813 acc_val: 0.8167 time: 0.3866s\n",
      "Epoch: 0381 loss_train: 0.4972 acc_train: 0.8714 loss_val: 0.6813 acc_val: 0.8167 time: 0.4014s\n",
      "Epoch: 0382 loss_train: 0.6601 acc_train: 0.8143 loss_val: 0.6813 acc_val: 0.8200 time: 0.3846s\n",
      "Epoch: 0383 loss_train: 0.7083 acc_train: 0.7857 loss_val: 0.6811 acc_val: 0.8167 time: 0.3868s\n",
      "Epoch: 0384 loss_train: 0.6531 acc_train: 0.8214 loss_val: 0.6811 acc_val: 0.8167 time: 0.4586s\n",
      "Epoch: 0385 loss_train: 0.6940 acc_train: 0.7643 loss_val: 0.6812 acc_val: 0.8200 time: 0.4601s\n",
      "Epoch: 0386 loss_train: 0.7201 acc_train: 0.7500 loss_val: 0.6810 acc_val: 0.8233 time: 0.4395s\n",
      "Epoch: 0387 loss_train: 0.6599 acc_train: 0.8071 loss_val: 0.6811 acc_val: 0.8267 time: 0.3959s\n",
      "Epoch: 0388 loss_train: 0.6477 acc_train: 0.8143 loss_val: 0.6812 acc_val: 0.8267 time: 0.3948s\n",
      "Epoch: 0389 loss_train: 0.6086 acc_train: 0.8286 loss_val: 0.6813 acc_val: 0.8267 time: 0.3858s\n",
      "Epoch: 0390 loss_train: 0.5521 acc_train: 0.8857 loss_val: 0.6809 acc_val: 0.8333 time: 0.3976s\n",
      "Epoch: 0391 loss_train: 0.5106 acc_train: 0.8786 loss_val: 0.6807 acc_val: 0.8333 time: 0.4479s\n",
      "Epoch: 0392 loss_train: 0.5950 acc_train: 0.8286 loss_val: 0.6808 acc_val: 0.8300 time: 0.3923s\n",
      "Epoch: 0393 loss_train: 0.6686 acc_train: 0.8071 loss_val: 0.6806 acc_val: 0.8300 time: 0.4223s\n",
      "Epoch: 0394 loss_train: 0.6563 acc_train: 0.8143 loss_val: 0.6807 acc_val: 0.8300 time: 0.3838s\n",
      "Epoch: 0395 loss_train: 0.6526 acc_train: 0.8071 loss_val: 0.6808 acc_val: 0.8300 time: 0.4159s\n",
      "Epoch: 0396 loss_train: 0.5860 acc_train: 0.8571 loss_val: 0.6809 acc_val: 0.8300 time: 0.4613s\n",
      "Epoch: 0397 loss_train: 0.5804 acc_train: 0.8429 loss_val: 0.6810 acc_val: 0.8300 time: 0.3743s\n",
      "Epoch: 0398 loss_train: 0.6836 acc_train: 0.7857 loss_val: 0.6814 acc_val: 0.8300 time: 0.4298s\n",
      "Epoch: 0399 loss_train: 0.7364 acc_train: 0.8071 loss_val: 0.6816 acc_val: 0.8300 time: 0.4051s\n",
      "Epoch: 0400 loss_train: 0.6676 acc_train: 0.8071 loss_val: 0.6823 acc_val: 0.8267 time: 0.3975s\n",
      "Epoch: 0401 loss_train: 0.6069 acc_train: 0.8571 loss_val: 0.6828 acc_val: 0.8267 time: 0.3990s\n",
      "Epoch: 0402 loss_train: 0.6721 acc_train: 0.8214 loss_val: 0.6832 acc_val: 0.8267 time: 0.4014s\n",
      "Epoch: 0403 loss_train: 0.6528 acc_train: 0.8000 loss_val: 0.6832 acc_val: 0.8267 time: 0.3791s\n",
      "Epoch: 0404 loss_train: 0.6126 acc_train: 0.8214 loss_val: 0.6832 acc_val: 0.8267 time: 0.3832s\n",
      "Epoch: 0405 loss_train: 0.6684 acc_train: 0.8143 loss_val: 0.6831 acc_val: 0.8267 time: 0.3945s\n",
      "Epoch: 0406 loss_train: 0.6435 acc_train: 0.7857 loss_val: 0.6829 acc_val: 0.8267 time: 0.3836s\n",
      "Epoch: 0407 loss_train: 0.6015 acc_train: 0.8286 loss_val: 0.6824 acc_val: 0.8267 time: 0.4568s\n",
      "Epoch: 0408 loss_train: 0.6706 acc_train: 0.8000 loss_val: 0.6819 acc_val: 0.8267 time: 0.4415s\n",
      "Epoch: 0409 loss_train: 0.7640 acc_train: 0.7500 loss_val: 0.6814 acc_val: 0.8267 time: 0.4685s\n",
      "Epoch: 0410 loss_train: 0.5703 acc_train: 0.8429 loss_val: 0.6810 acc_val: 0.8267 time: 0.4663s\n",
      "Epoch: 0411 loss_train: 0.6443 acc_train: 0.8214 loss_val: 0.6805 acc_val: 0.8267 time: 0.3926s\n",
      "Epoch: 0412 loss_train: 0.6008 acc_train: 0.8000 loss_val: 0.6805 acc_val: 0.8267 time: 0.3913s\n",
      "Epoch: 0413 loss_train: 0.6455 acc_train: 0.8286 loss_val: 0.6802 acc_val: 0.8267 time: 0.4640s\n",
      "Epoch: 0414 loss_train: 0.6252 acc_train: 0.8286 loss_val: 0.6800 acc_val: 0.8267 time: 0.3954s\n",
      "Epoch: 0415 loss_train: 0.5026 acc_train: 0.8500 loss_val: 0.6798 acc_val: 0.8267 time: 0.3814s\n",
      "Epoch: 0416 loss_train: 0.6050 acc_train: 0.8071 loss_val: 0.6795 acc_val: 0.8267 time: 0.3909s\n",
      "Epoch: 0417 loss_train: 0.6128 acc_train: 0.8286 loss_val: 0.6795 acc_val: 0.8267 time: 0.3855s\n",
      "Epoch: 0418 loss_train: 0.7943 acc_train: 0.7429 loss_val: 0.6789 acc_val: 0.8267 time: 0.3932s\n",
      "Epoch: 0419 loss_train: 0.6027 acc_train: 0.8357 loss_val: 0.6783 acc_val: 0.8300 time: 0.3947s\n",
      "Epoch: 0420 loss_train: 0.7158 acc_train: 0.7571 loss_val: 0.6778 acc_val: 0.8267 time: 0.4139s\n",
      "Epoch: 0421 loss_train: 0.6316 acc_train: 0.8357 loss_val: 0.6776 acc_val: 0.8267 time: 0.3805s\n",
      "Epoch: 0422 loss_train: 0.5917 acc_train: 0.8500 loss_val: 0.6774 acc_val: 0.8233 time: 0.3839s\n",
      "Epoch: 0423 loss_train: 0.6257 acc_train: 0.8214 loss_val: 0.6772 acc_val: 0.8267 time: 0.3941s\n",
      "Epoch: 0424 loss_train: 0.5872 acc_train: 0.8286 loss_val: 0.6772 acc_val: 0.8267 time: 0.4402s\n",
      "Epoch: 0425 loss_train: 0.6168 acc_train: 0.8286 loss_val: 0.6772 acc_val: 0.8267 time: 0.3894s\n",
      "Epoch: 0426 loss_train: 0.6267 acc_train: 0.8214 loss_val: 0.6771 acc_val: 0.8267 time: 0.4068s\n",
      "Epoch: 0427 loss_train: 0.7143 acc_train: 0.7643 loss_val: 0.6772 acc_val: 0.8267 time: 0.3896s\n",
      "Epoch: 0428 loss_train: 0.5968 acc_train: 0.8000 loss_val: 0.6775 acc_val: 0.8267 time: 0.3897s\n",
      "Epoch: 0429 loss_train: 0.6615 acc_train: 0.8000 loss_val: 0.6776 acc_val: 0.8267 time: 0.3970s\n",
      "Epoch: 0430 loss_train: 0.7126 acc_train: 0.7929 loss_val: 0.6778 acc_val: 0.8267 time: 0.4539s\n",
      "Epoch: 0431 loss_train: 0.6193 acc_train: 0.8071 loss_val: 0.6779 acc_val: 0.8300 time: 0.4201s\n",
      "Epoch: 0432 loss_train: 0.5622 acc_train: 0.8643 loss_val: 0.6779 acc_val: 0.8267 time: 0.4246s\n",
      "Epoch: 0433 loss_train: 0.6177 acc_train: 0.8286 loss_val: 0.6781 acc_val: 0.8267 time: 0.3735s\n",
      "Epoch: 0434 loss_train: 0.6589 acc_train: 0.8071 loss_val: 0.6783 acc_val: 0.8267 time: 0.4194s\n",
      "Epoch: 0435 loss_train: 0.6472 acc_train: 0.8071 loss_val: 0.6784 acc_val: 0.8267 time: 0.3859s\n",
      "Epoch: 0436 loss_train: 0.6134 acc_train: 0.8643 loss_val: 0.6787 acc_val: 0.8267 time: 0.4030s\n",
      "Epoch: 0437 loss_train: 0.6721 acc_train: 0.8000 loss_val: 0.6787 acc_val: 0.8267 time: 0.4010s\n",
      "Epoch: 0438 loss_train: 0.6879 acc_train: 0.8000 loss_val: 0.6786 acc_val: 0.8267 time: 0.3930s\n",
      "Epoch: 0439 loss_train: 0.6706 acc_train: 0.8071 loss_val: 0.6785 acc_val: 0.8233 time: 0.3958s\n",
      "Epoch: 0440 loss_train: 0.5559 acc_train: 0.8429 loss_val: 0.6784 acc_val: 0.8233 time: 0.3751s\n",
      "Epoch: 0441 loss_train: 0.5516 acc_train: 0.8429 loss_val: 0.6784 acc_val: 0.8167 time: 0.3701s\n",
      "Epoch: 0442 loss_train: 0.6998 acc_train: 0.7714 loss_val: 0.6784 acc_val: 0.8200 time: 0.4108s\n",
      "Epoch: 0443 loss_train: 0.5758 acc_train: 0.8571 loss_val: 0.6782 acc_val: 0.8200 time: 0.3939s\n",
      "Epoch: 0444 loss_train: 0.5619 acc_train: 0.8214 loss_val: 0.6781 acc_val: 0.8200 time: 0.4373s\n",
      "Epoch: 0445 loss_train: 0.6674 acc_train: 0.7643 loss_val: 0.6780 acc_val: 0.8200 time: 0.3823s\n",
      "Epoch: 0446 loss_train: 0.6576 acc_train: 0.8071 loss_val: 0.6776 acc_val: 0.8200 time: 0.4065s\n",
      "Epoch: 0447 loss_train: 0.6929 acc_train: 0.8000 loss_val: 0.6774 acc_val: 0.8200 time: 0.3930s\n",
      "Epoch: 0448 loss_train: 0.5962 acc_train: 0.8357 loss_val: 0.6772 acc_val: 0.8233 time: 0.3943s\n",
      "Epoch: 0449 loss_train: 0.6115 acc_train: 0.7929 loss_val: 0.6767 acc_val: 0.8233 time: 0.4523s\n",
      "Epoch: 0450 loss_train: 0.5878 acc_train: 0.8357 loss_val: 0.6763 acc_val: 0.8233 time: 0.4236s\n",
      "Epoch: 0451 loss_train: 0.5931 acc_train: 0.8357 loss_val: 0.6764 acc_val: 0.8233 time: 0.4296s\n",
      "Epoch: 0452 loss_train: 0.6259 acc_train: 0.8000 loss_val: 0.6763 acc_val: 0.8233 time: 0.4097s\n",
      "Epoch: 0453 loss_train: 0.6349 acc_train: 0.8214 loss_val: 0.6761 acc_val: 0.8233 time: 0.4657s\n",
      "Epoch: 0454 loss_train: 0.7092 acc_train: 0.7786 loss_val: 0.6764 acc_val: 0.8233 time: 0.4407s\n",
      "Epoch: 0455 loss_train: 0.7105 acc_train: 0.8000 loss_val: 0.6764 acc_val: 0.8233 time: 0.3995s\n",
      "Epoch: 0456 loss_train: 0.6416 acc_train: 0.7857 loss_val: 0.6766 acc_val: 0.8233 time: 0.3907s\n",
      "Epoch: 0457 loss_train: 0.6513 acc_train: 0.7714 loss_val: 0.6770 acc_val: 0.8233 time: 0.4217s\n",
      "Epoch: 0458 loss_train: 0.6640 acc_train: 0.8071 loss_val: 0.6771 acc_val: 0.8233 time: 0.3969s\n",
      "Epoch: 0459 loss_train: 0.6065 acc_train: 0.8357 loss_val: 0.6772 acc_val: 0.8233 time: 0.4010s\n",
      "Epoch: 0460 loss_train: 0.6681 acc_train: 0.8000 loss_val: 0.6774 acc_val: 0.8200 time: 0.4213s\n",
      "Epoch: 0461 loss_train: 0.5390 acc_train: 0.8571 loss_val: 0.6778 acc_val: 0.8200 time: 0.4016s\n",
      "Epoch: 0462 loss_train: 0.5960 acc_train: 0.8071 loss_val: 0.6782 acc_val: 0.8200 time: 0.3999s\n",
      "Epoch: 0463 loss_train: 0.5936 acc_train: 0.8357 loss_val: 0.6787 acc_val: 0.8200 time: 0.4353s\n",
      "Epoch: 0464 loss_train: 0.6465 acc_train: 0.8286 loss_val: 0.6792 acc_val: 0.8200 time: 0.3912s\n",
      "Epoch: 0465 loss_train: 0.6671 acc_train: 0.8286 loss_val: 0.6795 acc_val: 0.8200 time: 0.3872s\n",
      "Epoch: 0466 loss_train: 0.6467 acc_train: 0.8357 loss_val: 0.6796 acc_val: 0.8167 time: 0.3961s\n",
      "Epoch: 0467 loss_train: 0.6171 acc_train: 0.8357 loss_val: 0.6798 acc_val: 0.8167 time: 0.3908s\n",
      "Epoch: 0468 loss_train: 0.6141 acc_train: 0.8286 loss_val: 0.6802 acc_val: 0.8167 time: 0.4011s\n",
      "Epoch: 0469 loss_train: 0.6116 acc_train: 0.8286 loss_val: 0.6805 acc_val: 0.8167 time: 0.4453s\n",
      "Epoch: 0470 loss_train: 0.6170 acc_train: 0.8286 loss_val: 0.6803 acc_val: 0.8167 time: 0.3788s\n",
      "Epoch: 0471 loss_train: 0.6272 acc_train: 0.8286 loss_val: 0.6800 acc_val: 0.8167 time: 0.3840s\n",
      "Epoch: 0472 loss_train: 0.6073 acc_train: 0.8286 loss_val: 0.6798 acc_val: 0.8167 time: 0.4488s\n",
      "Epoch: 0473 loss_train: 0.5814 acc_train: 0.8143 loss_val: 0.6793 acc_val: 0.8167 time: 0.3834s\n",
      "Epoch: 0474 loss_train: 0.5208 acc_train: 0.8571 loss_val: 0.6789 acc_val: 0.8167 time: 0.4113s\n",
      "Epoch: 0475 loss_train: 0.5497 acc_train: 0.8286 loss_val: 0.6783 acc_val: 0.8167 time: 0.3835s\n",
      "Epoch: 0476 loss_train: 0.6551 acc_train: 0.8071 loss_val: 0.6775 acc_val: 0.8200 time: 0.3935s\n",
      "Epoch: 0477 loss_train: 0.6442 acc_train: 0.8071 loss_val: 0.6765 acc_val: 0.8200 time: 0.4043s\n",
      "Epoch: 0478 loss_train: 0.6380 acc_train: 0.8357 loss_val: 0.6757 acc_val: 0.8200 time: 0.3898s\n",
      "Epoch: 0479 loss_train: 0.6838 acc_train: 0.7857 loss_val: 0.6753 acc_val: 0.8167 time: 0.3913s\n",
      "Epoch: 0480 loss_train: 0.5458 acc_train: 0.9000 loss_val: 0.6749 acc_val: 0.8167 time: 0.4072s\n",
      "Epoch: 0481 loss_train: 0.6155 acc_train: 0.8429 loss_val: 0.6745 acc_val: 0.8167 time: 0.3904s\n",
      "Epoch: 0482 loss_train: 0.6245 acc_train: 0.8357 loss_val: 0.6739 acc_val: 0.8167 time: 0.3931s\n",
      "Epoch: 0483 loss_train: 0.6638 acc_train: 0.8214 loss_val: 0.6732 acc_val: 0.8167 time: 0.4307s\n",
      "Epoch: 0484 loss_train: 0.6266 acc_train: 0.8286 loss_val: 0.6726 acc_val: 0.8167 time: 0.3842s\n",
      "Epoch: 0485 loss_train: 0.6194 acc_train: 0.7857 loss_val: 0.6721 acc_val: 0.8167 time: 0.4008s\n",
      "Epoch: 0486 loss_train: 0.6507 acc_train: 0.7786 loss_val: 0.6716 acc_val: 0.8167 time: 0.4464s\n",
      "Epoch: 0487 loss_train: 0.6043 acc_train: 0.8286 loss_val: 0.6711 acc_val: 0.8167 time: 0.4025s\n",
      "Epoch: 0488 loss_train: 0.6775 acc_train: 0.7714 loss_val: 0.6705 acc_val: 0.8167 time: 0.3812s\n",
      "Epoch: 0489 loss_train: 0.5631 acc_train: 0.8500 loss_val: 0.6700 acc_val: 0.8167 time: 0.4120s\n",
      "Epoch: 0490 loss_train: 0.6007 acc_train: 0.8286 loss_val: 0.6696 acc_val: 0.8167 time: 0.4283s\n",
      "Epoch: 0491 loss_train: 0.6023 acc_train: 0.8429 loss_val: 0.6692 acc_val: 0.8167 time: 0.4580s\n",
      "Epoch: 0492 loss_train: 0.5933 acc_train: 0.8357 loss_val: 0.6688 acc_val: 0.8167 time: 0.4524s\n",
      "Epoch: 0493 loss_train: 0.6135 acc_train: 0.7929 loss_val: 0.6685 acc_val: 0.8200 time: 0.4590s\n",
      "Epoch: 0494 loss_train: 0.6781 acc_train: 0.8143 loss_val: 0.6681 acc_val: 0.8200 time: 0.3972s\n",
      "Epoch: 0495 loss_train: 0.6086 acc_train: 0.8286 loss_val: 0.6679 acc_val: 0.8200 time: 0.4463s\n",
      "Epoch: 0496 loss_train: 0.7272 acc_train: 0.8000 loss_val: 0.6677 acc_val: 0.8167 time: 0.4415s\n",
      "Epoch: 0497 loss_train: 0.6615 acc_train: 0.8214 loss_val: 0.6674 acc_val: 0.8167 time: 0.4410s\n",
      "Epoch: 0498 loss_train: 0.6495 acc_train: 0.8143 loss_val: 0.6672 acc_val: 0.8167 time: 0.4506s\n",
      "Epoch: 0499 loss_train: 0.5936 acc_train: 0.8214 loss_val: 0.6670 acc_val: 0.8167 time: 0.3975s\n",
      "Epoch: 0500 loss_train: 0.5412 acc_train: 0.8357 loss_val: 0.6668 acc_val: 0.8167 time: 0.3873s\n",
      "Epoch: 0501 loss_train: 0.5675 acc_train: 0.8357 loss_val: 0.6667 acc_val: 0.8167 time: 0.4010s\n",
      "Epoch: 0502 loss_train: 0.5793 acc_train: 0.8286 loss_val: 0.6666 acc_val: 0.8167 time: 0.3840s\n",
      "Epoch: 0503 loss_train: 0.6755 acc_train: 0.7857 loss_val: 0.6665 acc_val: 0.8200 time: 0.3964s\n",
      "Epoch: 0504 loss_train: 0.6476 acc_train: 0.8000 loss_val: 0.6668 acc_val: 0.8167 time: 0.4178s\n",
      "Epoch: 0505 loss_train: 0.6141 acc_train: 0.7929 loss_val: 0.6671 acc_val: 0.8200 time: 0.4088s\n",
      "Epoch: 0506 loss_train: 0.5592 acc_train: 0.8357 loss_val: 0.6674 acc_val: 0.8200 time: 0.3933s\n",
      "Epoch: 0507 loss_train: 0.5606 acc_train: 0.8286 loss_val: 0.6676 acc_val: 0.8200 time: 0.4110s\n",
      "Epoch: 0508 loss_train: 0.5423 acc_train: 0.8500 loss_val: 0.6682 acc_val: 0.8200 time: 0.4184s\n",
      "Epoch: 0509 loss_train: 0.6586 acc_train: 0.7929 loss_val: 0.6686 acc_val: 0.8200 time: 0.4566s\n",
      "Epoch: 0510 loss_train: 0.6643 acc_train: 0.7786 loss_val: 0.6689 acc_val: 0.8200 time: 0.4277s\n",
      "Epoch: 0511 loss_train: 0.6119 acc_train: 0.8571 loss_val: 0.6694 acc_val: 0.8167 time: 0.4250s\n",
      "Epoch: 0512 loss_train: 0.6313 acc_train: 0.8214 loss_val: 0.6700 acc_val: 0.8167 time: 0.3829s\n",
      "Epoch: 0513 loss_train: 0.5496 acc_train: 0.8714 loss_val: 0.6705 acc_val: 0.8200 time: 0.3930s\n",
      "Epoch: 0514 loss_train: 0.6286 acc_train: 0.8143 loss_val: 0.6709 acc_val: 0.8200 time: 0.3984s\n",
      "Epoch: 0515 loss_train: 0.6342 acc_train: 0.8500 loss_val: 0.6710 acc_val: 0.8200 time: 0.4042s\n",
      "Epoch: 0516 loss_train: 0.6306 acc_train: 0.8071 loss_val: 0.6710 acc_val: 0.8200 time: 0.4431s\n",
      "Epoch: 0517 loss_train: 0.6919 acc_train: 0.7786 loss_val: 0.6713 acc_val: 0.8200 time: 0.4486s\n",
      "Epoch: 0518 loss_train: 0.6511 acc_train: 0.8071 loss_val: 0.6714 acc_val: 0.8200 time: 0.3830s\n",
      "Epoch: 0519 loss_train: 0.5421 acc_train: 0.8714 loss_val: 0.6714 acc_val: 0.8233 time: 0.4002s\n",
      "Epoch: 0520 loss_train: 0.5842 acc_train: 0.8214 loss_val: 0.6713 acc_val: 0.8233 time: 0.4008s\n",
      "Epoch: 0521 loss_train: 0.6278 acc_train: 0.8357 loss_val: 0.6711 acc_val: 0.8233 time: 0.4058s\n",
      "Epoch: 0522 loss_train: 0.5166 acc_train: 0.8429 loss_val: 0.6711 acc_val: 0.8233 time: 0.4060s\n",
      "Epoch: 0523 loss_train: 0.7019 acc_train: 0.8000 loss_val: 0.6709 acc_val: 0.8233 time: 0.3968s\n",
      "Epoch: 0524 loss_train: 0.5826 acc_train: 0.8214 loss_val: 0.6708 acc_val: 0.8233 time: 0.3804s\n",
      "Epoch: 0525 loss_train: 0.6531 acc_train: 0.7929 loss_val: 0.6708 acc_val: 0.8233 time: 0.4457s\n",
      "Epoch: 0526 loss_train: 0.5452 acc_train: 0.8786 loss_val: 0.6711 acc_val: 0.8233 time: 0.3824s\n",
      "Epoch: 0527 loss_train: 0.5696 acc_train: 0.8071 loss_val: 0.6712 acc_val: 0.8200 time: 0.4033s\n",
      "Epoch: 0528 loss_train: 0.6402 acc_train: 0.8357 loss_val: 0.6711 acc_val: 0.8200 time: 0.3921s\n",
      "Epoch: 0529 loss_train: 0.7079 acc_train: 0.7643 loss_val: 0.6713 acc_val: 0.8200 time: 0.4056s\n",
      "Epoch: 0530 loss_train: 0.6506 acc_train: 0.7643 loss_val: 0.6715 acc_val: 0.8200 time: 0.4403s\n",
      "Epoch: 0531 loss_train: 0.5922 acc_train: 0.8214 loss_val: 0.6714 acc_val: 0.8200 time: 0.4676s\n",
      "Epoch: 0532 loss_train: 0.6622 acc_train: 0.8286 loss_val: 0.6711 acc_val: 0.8200 time: 0.4183s\n",
      "Epoch: 0533 loss_train: 0.7408 acc_train: 0.7857 loss_val: 0.6712 acc_val: 0.8200 time: 0.4091s\n",
      "Epoch: 0534 loss_train: 0.6057 acc_train: 0.8286 loss_val: 0.6710 acc_val: 0.8133 time: 0.3870s\n",
      "Epoch: 0535 loss_train: 0.6186 acc_train: 0.8286 loss_val: 0.6706 acc_val: 0.8133 time: 0.4058s\n",
      "Epoch: 0536 loss_train: 0.6750 acc_train: 0.8214 loss_val: 0.6704 acc_val: 0.8133 time: 0.4394s\n",
      "Epoch: 0537 loss_train: 0.5931 acc_train: 0.8429 loss_val: 0.6699 acc_val: 0.8133 time: 0.4101s\n",
      "Epoch: 0538 loss_train: 0.5746 acc_train: 0.8214 loss_val: 0.6695 acc_val: 0.8133 time: 0.4348s\n",
      "Epoch: 0539 loss_train: 0.7261 acc_train: 0.7857 loss_val: 0.6689 acc_val: 0.8100 time: 0.3801s\n",
      "Epoch: 0540 loss_train: 0.6409 acc_train: 0.8214 loss_val: 0.6683 acc_val: 0.8100 time: 0.3897s\n",
      "Epoch: 0541 loss_train: 0.5150 acc_train: 0.8571 loss_val: 0.6678 acc_val: 0.8100 time: 0.4479s\n",
      "Epoch: 0542 loss_train: 0.5926 acc_train: 0.8286 loss_val: 0.6675 acc_val: 0.8067 time: 0.3866s\n",
      "Epoch: 0543 loss_train: 0.6903 acc_train: 0.8000 loss_val: 0.6670 acc_val: 0.8067 time: 0.4476s\n",
      "Epoch: 0544 loss_train: 0.6128 acc_train: 0.8429 loss_val: 0.6663 acc_val: 0.8067 time: 0.4057s\n",
      "Epoch: 0545 loss_train: 0.7203 acc_train: 0.8000 loss_val: 0.6655 acc_val: 0.8067 time: 0.4284s\n",
      "Epoch: 0546 loss_train: 0.5468 acc_train: 0.8643 loss_val: 0.6646 acc_val: 0.8067 time: 0.3896s\n",
      "Epoch: 0547 loss_train: 0.5378 acc_train: 0.8786 loss_val: 0.6641 acc_val: 0.8067 time: 0.3876s\n",
      "Epoch: 0548 loss_train: 0.6275 acc_train: 0.8071 loss_val: 0.6637 acc_val: 0.8067 time: 0.3860s\n",
      "Epoch: 0549 loss_train: 0.5510 acc_train: 0.8357 loss_val: 0.6634 acc_val: 0.8067 time: 0.3991s\n",
      "Epoch: 0550 loss_train: 0.6566 acc_train: 0.8071 loss_val: 0.6634 acc_val: 0.8067 time: 0.4317s\n",
      "Epoch: 0551 loss_train: 0.7583 acc_train: 0.7071 loss_val: 0.6633 acc_val: 0.8067 time: 0.4249s\n",
      "Epoch: 0552 loss_train: 0.6727 acc_train: 0.7714 loss_val: 0.6634 acc_val: 0.8067 time: 0.4401s\n",
      "Epoch: 0553 loss_train: 0.6436 acc_train: 0.8143 loss_val: 0.6635 acc_val: 0.8067 time: 0.3889s\n",
      "Epoch: 0554 loss_train: 0.6575 acc_train: 0.8143 loss_val: 0.6634 acc_val: 0.8067 time: 0.4082s\n",
      "Epoch: 0555 loss_train: 0.6095 acc_train: 0.8143 loss_val: 0.6634 acc_val: 0.8067 time: 0.3896s\n",
      "Epoch: 0556 loss_train: 0.6435 acc_train: 0.8000 loss_val: 0.6632 acc_val: 0.8033 time: 0.4018s\n",
      "Epoch: 0557 loss_train: 0.6939 acc_train: 0.8286 loss_val: 0.6631 acc_val: 0.8033 time: 0.4230s\n",
      "Epoch: 0558 loss_train: 0.6370 acc_train: 0.8357 loss_val: 0.6630 acc_val: 0.8033 time: 0.3769s\n",
      "Epoch: 0559 loss_train: 0.6658 acc_train: 0.8000 loss_val: 0.6629 acc_val: 0.8033 time: 0.3865s\n",
      "Epoch: 0560 loss_train: 0.6439 acc_train: 0.7786 loss_val: 0.6632 acc_val: 0.8033 time: 0.4329s\n",
      "Epoch: 0561 loss_train: 0.6377 acc_train: 0.8286 loss_val: 0.6636 acc_val: 0.8067 time: 0.3982s\n",
      "Epoch: 0562 loss_train: 0.5051 acc_train: 0.8571 loss_val: 0.6635 acc_val: 0.8033 time: 0.4065s\n",
      "Epoch: 0563 loss_train: 0.5994 acc_train: 0.8643 loss_val: 0.6633 acc_val: 0.8033 time: 0.4147s\n",
      "Epoch: 0564 loss_train: 0.6099 acc_train: 0.8357 loss_val: 0.6626 acc_val: 0.8033 time: 0.3800s\n",
      "Epoch: 0565 loss_train: 0.6357 acc_train: 0.7643 loss_val: 0.6619 acc_val: 0.8033 time: 0.4168s\n",
      "Epoch: 0566 loss_train: 0.5803 acc_train: 0.8214 loss_val: 0.6615 acc_val: 0.8033 time: 0.4147s\n",
      "Epoch: 0567 loss_train: 0.6821 acc_train: 0.8143 loss_val: 0.6612 acc_val: 0.8033 time: 0.4268s\n",
      "Epoch: 0568 loss_train: 0.6455 acc_train: 0.8071 loss_val: 0.6608 acc_val: 0.8033 time: 0.4008s\n",
      "Epoch: 0569 loss_train: 0.5971 acc_train: 0.8286 loss_val: 0.6607 acc_val: 0.8033 time: 0.3902s\n",
      "Epoch: 0570 loss_train: 0.6557 acc_train: 0.7714 loss_val: 0.6609 acc_val: 0.8033 time: 0.3964s\n",
      "Epoch: 0571 loss_train: 0.5411 acc_train: 0.8643 loss_val: 0.6607 acc_val: 0.8033 time: 0.3879s\n",
      "Epoch: 0572 loss_train: 0.6655 acc_train: 0.7929 loss_val: 0.6604 acc_val: 0.8033 time: 0.4132s\n",
      "Epoch: 0573 loss_train: 0.6758 acc_train: 0.7643 loss_val: 0.6599 acc_val: 0.8033 time: 0.4024s\n",
      "Epoch: 0574 loss_train: 0.5520 acc_train: 0.8286 loss_val: 0.6594 acc_val: 0.8033 time: 0.3974s\n",
      "Epoch: 0575 loss_train: 0.6255 acc_train: 0.8143 loss_val: 0.6590 acc_val: 0.8033 time: 0.3878s\n",
      "Epoch: 0576 loss_train: 0.4967 acc_train: 0.8786 loss_val: 0.6590 acc_val: 0.8033 time: 0.3911s\n",
      "Epoch: 0577 loss_train: 0.6916 acc_train: 0.7571 loss_val: 0.6591 acc_val: 0.8033 time: 0.3893s\n",
      "Epoch: 0578 loss_train: 0.5893 acc_train: 0.8286 loss_val: 0.6595 acc_val: 0.8100 time: 0.4003s\n",
      "Epoch: 0579 loss_train: 0.6970 acc_train: 0.8000 loss_val: 0.6597 acc_val: 0.8100 time: 0.3949s\n",
      "Epoch: 0580 loss_train: 0.6117 acc_train: 0.8071 loss_val: 0.6599 acc_val: 0.8100 time: 0.4247s\n",
      "Epoch: 0581 loss_train: 0.5460 acc_train: 0.8643 loss_val: 0.6599 acc_val: 0.8133 time: 0.4027s\n",
      "Epoch: 0582 loss_train: 0.6647 acc_train: 0.8000 loss_val: 0.6600 acc_val: 0.8133 time: 0.3931s\n",
      "Epoch: 0583 loss_train: 0.5970 acc_train: 0.8214 loss_val: 0.6602 acc_val: 0.8133 time: 0.3888s\n",
      "Epoch: 0584 loss_train: 0.5712 acc_train: 0.8429 loss_val: 0.6603 acc_val: 0.8100 time: 0.3983s\n",
      "Epoch: 0585 loss_train: 0.5484 acc_train: 0.8500 loss_val: 0.6603 acc_val: 0.8100 time: 0.4198s\n",
      "Epoch: 0586 loss_train: 0.6291 acc_train: 0.8143 loss_val: 0.6603 acc_val: 0.8133 time: 0.3908s\n",
      "Epoch: 0587 loss_train: 0.5289 acc_train: 0.8500 loss_val: 0.6600 acc_val: 0.8167 time: 0.3944s\n",
      "Epoch: 0588 loss_train: 0.6122 acc_train: 0.8357 loss_val: 0.6595 acc_val: 0.8167 time: 0.3966s\n",
      "Epoch: 0589 loss_train: 0.6320 acc_train: 0.8500 loss_val: 0.6592 acc_val: 0.8167 time: 0.3998s\n",
      "Epoch: 0590 loss_train: 0.5702 acc_train: 0.8571 loss_val: 0.6589 acc_val: 0.8167 time: 0.3791s\n",
      "Epoch: 0591 loss_train: 0.6892 acc_train: 0.7571 loss_val: 0.6588 acc_val: 0.8167 time: 0.4043s\n",
      "Epoch: 0592 loss_train: 0.6606 acc_train: 0.7929 loss_val: 0.6584 acc_val: 0.8167 time: 0.3974s\n",
      "Epoch: 0593 loss_train: 0.6467 acc_train: 0.8214 loss_val: 0.6577 acc_val: 0.8167 time: 0.4242s\n",
      "Epoch: 0594 loss_train: 0.6321 acc_train: 0.7857 loss_val: 0.6574 acc_val: 0.8200 time: 0.4006s\n",
      "Epoch: 0595 loss_train: 0.6116 acc_train: 0.8286 loss_val: 0.6571 acc_val: 0.8200 time: 0.3986s\n",
      "Epoch: 0596 loss_train: 0.6591 acc_train: 0.7857 loss_val: 0.6568 acc_val: 0.8167 time: 0.4181s\n",
      "Epoch: 0597 loss_train: 0.6526 acc_train: 0.7929 loss_val: 0.6572 acc_val: 0.8167 time: 0.4100s\n",
      "Epoch: 0598 loss_train: 0.6244 acc_train: 0.8214 loss_val: 0.6573 acc_val: 0.8167 time: 0.4502s\n",
      "Epoch: 0599 loss_train: 0.7187 acc_train: 0.7357 loss_val: 0.6572 acc_val: 0.8167 time: 0.4328s\n",
      "Epoch: 0600 loss_train: 0.5865 acc_train: 0.8071 loss_val: 0.6573 acc_val: 0.8167 time: 0.4376s\n",
      "Epoch: 0601 loss_train: 0.6138 acc_train: 0.8214 loss_val: 0.6573 acc_val: 0.8167 time: 0.4566s\n",
      "Epoch: 0602 loss_train: 0.6747 acc_train: 0.7929 loss_val: 0.6575 acc_val: 0.8167 time: 0.3907s\n",
      "Epoch: 0603 loss_train: 0.6328 acc_train: 0.7929 loss_val: 0.6576 acc_val: 0.8200 time: 0.3732s\n",
      "Epoch: 0604 loss_train: 0.5684 acc_train: 0.8286 loss_val: 0.6576 acc_val: 0.8200 time: 0.3820s\n",
      "Epoch: 0605 loss_train: 0.6431 acc_train: 0.7929 loss_val: 0.6574 acc_val: 0.8200 time: 0.3955s\n",
      "Epoch: 0606 loss_train: 0.5888 acc_train: 0.8000 loss_val: 0.6572 acc_val: 0.8200 time: 0.4063s\n",
      "Epoch: 0607 loss_train: 0.5493 acc_train: 0.8500 loss_val: 0.6568 acc_val: 0.8200 time: 0.4214s\n",
      "Epoch: 0608 loss_train: 0.5430 acc_train: 0.8571 loss_val: 0.6562 acc_val: 0.8167 time: 0.3824s\n",
      "Epoch: 0609 loss_train: 0.6565 acc_train: 0.7929 loss_val: 0.6557 acc_val: 0.8167 time: 0.4128s\n",
      "Epoch: 0610 loss_train: 0.5741 acc_train: 0.8500 loss_val: 0.6551 acc_val: 0.8133 time: 0.4178s\n",
      "Epoch: 0611 loss_train: 0.6749 acc_train: 0.8071 loss_val: 0.6545 acc_val: 0.8100 time: 0.4048s\n",
      "Epoch: 0612 loss_train: 0.5410 acc_train: 0.8500 loss_val: 0.6540 acc_val: 0.8133 time: 0.4016s\n",
      "Epoch: 0613 loss_train: 0.6403 acc_train: 0.8143 loss_val: 0.6536 acc_val: 0.8133 time: 0.3922s\n",
      "Epoch: 0614 loss_train: 0.5163 acc_train: 0.8857 loss_val: 0.6534 acc_val: 0.8167 time: 0.4744s\n",
      "Epoch: 0615 loss_train: 0.6123 acc_train: 0.8071 loss_val: 0.6531 acc_val: 0.8167 time: 0.3912s\n",
      "Epoch: 0616 loss_train: 0.7370 acc_train: 0.8143 loss_val: 0.6535 acc_val: 0.8200 time: 0.4126s\n",
      "Epoch: 0617 loss_train: 0.6694 acc_train: 0.7643 loss_val: 0.6540 acc_val: 0.8133 time: 0.3963s\n",
      "Epoch: 0618 loss_train: 0.6469 acc_train: 0.8071 loss_val: 0.6544 acc_val: 0.8133 time: 0.3850s\n",
      "Epoch: 0619 loss_train: 0.7458 acc_train: 0.7429 loss_val: 0.6549 acc_val: 0.8133 time: 0.3872s\n",
      "Epoch: 0620 loss_train: 0.6151 acc_train: 0.8357 loss_val: 0.6556 acc_val: 0.8133 time: 0.3865s\n",
      "Epoch: 0621 loss_train: 0.5814 acc_train: 0.8571 loss_val: 0.6563 acc_val: 0.8167 time: 0.4093s\n",
      "Epoch: 0622 loss_train: 0.6448 acc_train: 0.7857 loss_val: 0.6571 acc_val: 0.8133 time: 0.4136s\n",
      "Epoch: 0623 loss_train: 0.5013 acc_train: 0.8714 loss_val: 0.6576 acc_val: 0.8100 time: 0.4214s\n",
      "Epoch: 0624 loss_train: 0.6339 acc_train: 0.8143 loss_val: 0.6581 acc_val: 0.8100 time: 0.4007s\n",
      "Epoch: 0625 loss_train: 0.5244 acc_train: 0.8429 loss_val: 0.6586 acc_val: 0.8100 time: 0.3936s\n",
      "Epoch: 0626 loss_train: 0.5705 acc_train: 0.8357 loss_val: 0.6589 acc_val: 0.8100 time: 0.4166s\n",
      "Epoch: 0627 loss_train: 0.6034 acc_train: 0.8286 loss_val: 0.6588 acc_val: 0.8100 time: 0.4063s\n",
      "Epoch: 0628 loss_train: 0.7508 acc_train: 0.7786 loss_val: 0.6588 acc_val: 0.8100 time: 0.3812s\n",
      "Epoch: 0629 loss_train: 0.6181 acc_train: 0.8000 loss_val: 0.6584 acc_val: 0.8100 time: 0.4070s\n",
      "Epoch: 0630 loss_train: 0.6111 acc_train: 0.8357 loss_val: 0.6580 acc_val: 0.8100 time: 0.4009s\n",
      "Epoch: 0631 loss_train: 0.5693 acc_train: 0.8571 loss_val: 0.6574 acc_val: 0.8100 time: 0.3970s\n",
      "Epoch: 0632 loss_train: 0.5859 acc_train: 0.8143 loss_val: 0.6569 acc_val: 0.8100 time: 0.4258s\n",
      "Epoch: 0633 loss_train: 0.6132 acc_train: 0.8286 loss_val: 0.6566 acc_val: 0.8167 time: 0.4175s\n",
      "Epoch: 0634 loss_train: 0.6587 acc_train: 0.8143 loss_val: 0.6564 acc_val: 0.8167 time: 0.4025s\n",
      "Epoch: 0635 loss_train: 0.6229 acc_train: 0.8143 loss_val: 0.6562 acc_val: 0.8167 time: 0.4413s\n",
      "Epoch: 0636 loss_train: 0.6154 acc_train: 0.8357 loss_val: 0.6558 acc_val: 0.8167 time: 0.4270s\n",
      "Epoch: 0637 loss_train: 0.6162 acc_train: 0.8429 loss_val: 0.6551 acc_val: 0.8167 time: 0.4080s\n",
      "Epoch: 0638 loss_train: 0.6125 acc_train: 0.8071 loss_val: 0.6546 acc_val: 0.8133 time: 0.3978s\n",
      "Epoch: 0639 loss_train: 0.5810 acc_train: 0.8429 loss_val: 0.6541 acc_val: 0.8100 time: 0.4624s\n",
      "Epoch: 0640 loss_train: 0.5068 acc_train: 0.8143 loss_val: 0.6536 acc_val: 0.8100 time: 0.3912s\n",
      "Epoch: 0641 loss_train: 0.5767 acc_train: 0.8286 loss_val: 0.6532 acc_val: 0.8100 time: 0.3933s\n",
      "Epoch: 0642 loss_train: 0.6857 acc_train: 0.7857 loss_val: 0.6527 acc_val: 0.8100 time: 0.4221s\n",
      "Epoch: 0643 loss_train: 0.5349 acc_train: 0.8429 loss_val: 0.6523 acc_val: 0.8100 time: 0.3899s\n",
      "Epoch: 0644 loss_train: 0.5707 acc_train: 0.8429 loss_val: 0.6519 acc_val: 0.8100 time: 0.4083s\n",
      "Epoch: 0645 loss_train: 0.6194 acc_train: 0.8286 loss_val: 0.6513 acc_val: 0.8133 time: 0.3801s\n",
      "Epoch: 0646 loss_train: 0.6100 acc_train: 0.8286 loss_val: 0.6506 acc_val: 0.8133 time: 0.3916s\n",
      "Epoch: 0647 loss_train: 0.6006 acc_train: 0.8143 loss_val: 0.6497 acc_val: 0.8133 time: 0.4638s\n",
      "Epoch: 0648 loss_train: 0.6708 acc_train: 0.7857 loss_val: 0.6488 acc_val: 0.8133 time: 0.4509s\n",
      "Epoch: 0649 loss_train: 0.6369 acc_train: 0.7929 loss_val: 0.6482 acc_val: 0.8167 time: 0.4246s\n",
      "Epoch: 0650 loss_train: 0.4769 acc_train: 0.8714 loss_val: 0.6475 acc_val: 0.8200 time: 0.4193s\n",
      "Epoch: 0651 loss_train: 0.6677 acc_train: 0.8143 loss_val: 0.6466 acc_val: 0.8233 time: 0.4565s\n",
      "Epoch: 0652 loss_train: 0.5193 acc_train: 0.8643 loss_val: 0.6459 acc_val: 0.8233 time: 0.4326s\n",
      "Epoch: 0653 loss_train: 0.5595 acc_train: 0.8286 loss_val: 0.6454 acc_val: 0.8300 time: 0.3907s\n",
      "Epoch: 0654 loss_train: 0.6715 acc_train: 0.7786 loss_val: 0.6453 acc_val: 0.8300 time: 0.3924s\n",
      "Epoch: 0655 loss_train: 0.6001 acc_train: 0.8071 loss_val: 0.6452 acc_val: 0.8300 time: 0.3858s\n",
      "Epoch: 0656 loss_train: 0.5821 acc_train: 0.8143 loss_val: 0.6451 acc_val: 0.8333 time: 0.4415s\n",
      "Epoch: 0657 loss_train: 0.6582 acc_train: 0.7714 loss_val: 0.6451 acc_val: 0.8333 time: 0.4000s\n",
      "Epoch: 0658 loss_train: 0.6321 acc_train: 0.7929 loss_val: 0.6450 acc_val: 0.8300 time: 0.4090s\n",
      "Epoch: 0659 loss_train: 0.5579 acc_train: 0.8286 loss_val: 0.6451 acc_val: 0.8300 time: 0.3886s\n",
      "Epoch: 0660 loss_train: 0.6615 acc_train: 0.8000 loss_val: 0.6453 acc_val: 0.8300 time: 0.4487s\n",
      "Epoch: 0661 loss_train: 0.6187 acc_train: 0.7929 loss_val: 0.6456 acc_val: 0.8267 time: 0.4655s\n",
      "Epoch: 0662 loss_train: 0.5644 acc_train: 0.8429 loss_val: 0.6456 acc_val: 0.8267 time: 0.4518s\n",
      "Epoch: 0663 loss_train: 0.6478 acc_train: 0.8000 loss_val: 0.6457 acc_val: 0.8233 time: 0.3886s\n",
      "Epoch: 0664 loss_train: 0.5744 acc_train: 0.8429 loss_val: 0.6461 acc_val: 0.8233 time: 0.4142s\n",
      "Epoch: 0665 loss_train: 0.5724 acc_train: 0.8500 loss_val: 0.6463 acc_val: 0.8200 time: 0.3913s\n",
      "Epoch: 0666 loss_train: 0.8005 acc_train: 0.7571 loss_val: 0.6469 acc_val: 0.8200 time: 0.4173s\n",
      "Epoch: 0667 loss_train: 0.5411 acc_train: 0.8500 loss_val: 0.6477 acc_val: 0.8200 time: 0.4236s\n",
      "Epoch: 0668 loss_train: 0.5601 acc_train: 0.8429 loss_val: 0.6486 acc_val: 0.8200 time: 0.4170s\n",
      "Epoch: 0669 loss_train: 0.6930 acc_train: 0.7786 loss_val: 0.6496 acc_val: 0.8200 time: 0.4158s\n",
      "Epoch: 0670 loss_train: 0.6320 acc_train: 0.7929 loss_val: 0.6504 acc_val: 0.8200 time: 0.4065s\n",
      "Epoch: 0671 loss_train: 0.5820 acc_train: 0.8357 loss_val: 0.6512 acc_val: 0.8200 time: 0.3995s\n",
      "Epoch: 0672 loss_train: 0.5785 acc_train: 0.8429 loss_val: 0.6522 acc_val: 0.8167 time: 0.3930s\n",
      "Epoch: 0673 loss_train: 0.5444 acc_train: 0.8429 loss_val: 0.6533 acc_val: 0.8200 time: 0.3976s\n",
      "Epoch: 0674 loss_train: 0.6073 acc_train: 0.8286 loss_val: 0.6541 acc_val: 0.8200 time: 0.3887s\n",
      "Epoch: 0675 loss_train: 0.6563 acc_train: 0.7786 loss_val: 0.6547 acc_val: 0.8200 time: 0.3813s\n",
      "Epoch: 0676 loss_train: 0.5313 acc_train: 0.8500 loss_val: 0.6553 acc_val: 0.8200 time: 0.4449s\n",
      "Epoch: 0677 loss_train: 0.6149 acc_train: 0.8429 loss_val: 0.6558 acc_val: 0.8200 time: 0.3877s\n",
      "Epoch: 0678 loss_train: 0.5292 acc_train: 0.8571 loss_val: 0.6563 acc_val: 0.8133 time: 0.4022s\n",
      "Epoch: 0679 loss_train: 0.6520 acc_train: 0.7714 loss_val: 0.6565 acc_val: 0.8167 time: 0.4585s\n",
      "Epoch: 0680 loss_train: 0.5972 acc_train: 0.8357 loss_val: 0.6565 acc_val: 0.8167 time: 0.4313s\n",
      "Epoch: 0681 loss_train: 0.6890 acc_train: 0.8286 loss_val: 0.6566 acc_val: 0.8167 time: 0.4060s\n",
      "Epoch: 0682 loss_train: 0.5691 acc_train: 0.8429 loss_val: 0.6566 acc_val: 0.8200 time: 0.3878s\n",
      "Epoch: 0683 loss_train: 0.6207 acc_train: 0.8214 loss_val: 0.6564 acc_val: 0.8200 time: 0.3862s\n",
      "Epoch: 0684 loss_train: 0.6069 acc_train: 0.8143 loss_val: 0.6561 acc_val: 0.8233 time: 0.4247s\n",
      "Epoch: 0685 loss_train: 0.7025 acc_train: 0.7786 loss_val: 0.6559 acc_val: 0.8233 time: 0.3858s\n",
      "Epoch: 0686 loss_train: 0.7131 acc_train: 0.7929 loss_val: 0.6563 acc_val: 0.8200 time: 0.4247s\n",
      "Epoch: 0687 loss_train: 0.6392 acc_train: 0.8000 loss_val: 0.6565 acc_val: 0.8167 time: 0.3844s\n",
      "Epoch: 0688 loss_train: 0.5385 acc_train: 0.8500 loss_val: 0.6566 acc_val: 0.8167 time: 0.3928s\n",
      "Epoch: 0689 loss_train: 0.6676 acc_train: 0.8071 loss_val: 0.6571 acc_val: 0.8167 time: 0.3832s\n",
      "Epoch: 0690 loss_train: 0.6907 acc_train: 0.7929 loss_val: 0.6575 acc_val: 0.8200 time: 0.4298s\n",
      "Epoch: 0691 loss_train: 0.5872 acc_train: 0.8500 loss_val: 0.6578 acc_val: 0.8200 time: 0.3841s\n",
      "Epoch: 0692 loss_train: 0.6967 acc_train: 0.7643 loss_val: 0.6581 acc_val: 0.8167 time: 0.4111s\n",
      "Epoch: 0693 loss_train: 0.6196 acc_train: 0.7857 loss_val: 0.6582 acc_val: 0.8167 time: 0.4194s\n",
      "Epoch: 0694 loss_train: 0.5932 acc_train: 0.8000 loss_val: 0.6582 acc_val: 0.8200 time: 0.4068s\n",
      "Epoch: 0695 loss_train: 0.5986 acc_train: 0.8071 loss_val: 0.6577 acc_val: 0.8200 time: 0.4269s\n",
      "Epoch: 0696 loss_train: 0.6732 acc_train: 0.8000 loss_val: 0.6572 acc_val: 0.8200 time: 0.4197s\n",
      "Epoch: 0697 loss_train: 0.6201 acc_train: 0.8071 loss_val: 0.6567 acc_val: 0.8200 time: 0.4025s\n",
      "Epoch: 0698 loss_train: 0.5821 acc_train: 0.8214 loss_val: 0.6562 acc_val: 0.8233 time: 0.4344s\n",
      "Epoch: 0699 loss_train: 0.5473 acc_train: 0.8500 loss_val: 0.6561 acc_val: 0.8233 time: 0.3934s\n",
      "Epoch: 0700 loss_train: 0.5620 acc_train: 0.8571 loss_val: 0.6559 acc_val: 0.8233 time: 0.4577s\n",
      "Epoch: 0701 loss_train: 0.6474 acc_train: 0.7714 loss_val: 0.6555 acc_val: 0.8233 time: 0.3820s\n",
      "Epoch: 0702 loss_train: 0.6139 acc_train: 0.7929 loss_val: 0.6554 acc_val: 0.8267 time: 0.4064s\n",
      "Epoch: 0703 loss_train: 0.5747 acc_train: 0.8429 loss_val: 0.6552 acc_val: 0.8267 time: 0.3913s\n",
      "Epoch: 0704 loss_train: 0.6072 acc_train: 0.8143 loss_val: 0.6551 acc_val: 0.8267 time: 0.3951s\n",
      "Epoch: 0705 loss_train: 0.6318 acc_train: 0.8214 loss_val: 0.6552 acc_val: 0.8200 time: 0.3711s\n",
      "Epoch: 0706 loss_train: 0.6297 acc_train: 0.8071 loss_val: 0.6551 acc_val: 0.8200 time: 0.4414s\n",
      "Epoch: 0707 loss_train: 0.6230 acc_train: 0.7857 loss_val: 0.6547 acc_val: 0.8233 time: 0.4292s\n",
      "Epoch: 0708 loss_train: 0.6100 acc_train: 0.7857 loss_val: 0.6545 acc_val: 0.8233 time: 0.4741s\n",
      "Epoch: 0709 loss_train: 0.7085 acc_train: 0.7929 loss_val: 0.6545 acc_val: 0.8233 time: 0.4559s\n",
      "Epoch: 0710 loss_train: 0.5736 acc_train: 0.8714 loss_val: 0.6548 acc_val: 0.8200 time: 0.4372s\n",
      "Epoch: 0711 loss_train: 0.6539 acc_train: 0.8214 loss_val: 0.6550 acc_val: 0.8200 time: 0.4462s\n",
      "Epoch: 0712 loss_train: 0.7292 acc_train: 0.8214 loss_val: 0.6556 acc_val: 0.8233 time: 0.3867s\n",
      "Epoch: 0713 loss_train: 0.6049 acc_train: 0.8571 loss_val: 0.6561 acc_val: 0.8233 time: 0.4121s\n",
      "Epoch: 0714 loss_train: 0.5627 acc_train: 0.8714 loss_val: 0.6563 acc_val: 0.8233 time: 0.4042s\n",
      "Epoch: 0715 loss_train: 0.6424 acc_train: 0.8214 loss_val: 0.6562 acc_val: 0.8233 time: 0.4498s\n",
      "Epoch: 0716 loss_train: 0.5522 acc_train: 0.8571 loss_val: 0.6561 acc_val: 0.8200 time: 0.4029s\n",
      "Epoch: 0717 loss_train: 0.6527 acc_train: 0.8143 loss_val: 0.6561 acc_val: 0.8200 time: 0.3892s\n",
      "Epoch: 0718 loss_train: 0.6350 acc_train: 0.8214 loss_val: 0.6562 acc_val: 0.8200 time: 0.3837s\n",
      "Epoch: 0719 loss_train: 0.6476 acc_train: 0.8214 loss_val: 0.6565 acc_val: 0.8167 time: 0.3987s\n",
      "Epoch: 0720 loss_train: 0.5550 acc_train: 0.8571 loss_val: 0.6567 acc_val: 0.8167 time: 0.4517s\n",
      "Epoch: 0721 loss_train: 0.6243 acc_train: 0.8214 loss_val: 0.6565 acc_val: 0.8167 time: 0.3791s\n",
      "Epoch: 0722 loss_train: 0.5782 acc_train: 0.8214 loss_val: 0.6568 acc_val: 0.8167 time: 0.4060s\n",
      "Epoch: 0723 loss_train: 0.6146 acc_train: 0.8357 loss_val: 0.6577 acc_val: 0.8167 time: 0.3868s\n",
      "Epoch: 0724 loss_train: 0.5951 acc_train: 0.8357 loss_val: 0.6582 acc_val: 0.8167 time: 0.4074s\n",
      "Epoch: 0725 loss_train: 0.5595 acc_train: 0.8500 loss_val: 0.6585 acc_val: 0.8167 time: 0.3907s\n",
      "Epoch: 0726 loss_train: 0.5419 acc_train: 0.8643 loss_val: 0.6590 acc_val: 0.8167 time: 0.4093s\n",
      "Epoch: 0727 loss_train: 0.6312 acc_train: 0.8071 loss_val: 0.6594 acc_val: 0.8167 time: 0.4647s\n",
      "Epoch: 0728 loss_train: 0.6580 acc_train: 0.8071 loss_val: 0.6595 acc_val: 0.8167 time: 0.4028s\n",
      "Epoch: 0729 loss_train: 0.5812 acc_train: 0.8643 loss_val: 0.6598 acc_val: 0.8167 time: 0.3830s\n",
      "Epoch: 0730 loss_train: 0.5194 acc_train: 0.8357 loss_val: 0.6600 acc_val: 0.8167 time: 0.4012s\n",
      "Epoch: 0731 loss_train: 0.6554 acc_train: 0.8143 loss_val: 0.6606 acc_val: 0.8167 time: 0.3804s\n",
      "Epoch: 0732 loss_train: 0.6364 acc_train: 0.8071 loss_val: 0.6606 acc_val: 0.8167 time: 0.4045s\n",
      "Epoch: 0733 loss_train: 0.5967 acc_train: 0.8071 loss_val: 0.6602 acc_val: 0.8167 time: 0.3968s\n",
      "Epoch: 0734 loss_train: 0.6465 acc_train: 0.8143 loss_val: 0.6597 acc_val: 0.8167 time: 0.3914s\n",
      "Epoch: 0735 loss_train: 0.6536 acc_train: 0.8143 loss_val: 0.6591 acc_val: 0.8133 time: 0.3895s\n",
      "Epoch: 0736 loss_train: 0.6785 acc_train: 0.7571 loss_val: 0.6587 acc_val: 0.8133 time: 0.4217s\n",
      "Epoch: 0737 loss_train: 0.6656 acc_train: 0.8143 loss_val: 0.6588 acc_val: 0.8133 time: 0.4000s\n",
      "Epoch: 0738 loss_train: 0.6161 acc_train: 0.8143 loss_val: 0.6585 acc_val: 0.8167 time: 0.3953s\n",
      "Epoch: 0739 loss_train: 0.5423 acc_train: 0.8500 loss_val: 0.6582 acc_val: 0.8167 time: 0.4332s\n",
      "Epoch: 0740 loss_train: 0.4994 acc_train: 0.8786 loss_val: 0.6580 acc_val: 0.8167 time: 0.3923s\n",
      "Epoch: 0741 loss_train: 0.5517 acc_train: 0.8500 loss_val: 0.6577 acc_val: 0.8167 time: 0.3849s\n",
      "Epoch: 0742 loss_train: 0.5647 acc_train: 0.8000 loss_val: 0.6572 acc_val: 0.8167 time: 0.3942s\n",
      "Epoch: 0743 loss_train: 0.6147 acc_train: 0.8286 loss_val: 0.6565 acc_val: 0.8200 time: 0.3777s\n",
      "Epoch: 0744 loss_train: 0.6462 acc_train: 0.8143 loss_val: 0.6556 acc_val: 0.8167 time: 0.3992s\n",
      "Epoch: 0745 loss_train: 0.7202 acc_train: 0.7429 loss_val: 0.6548 acc_val: 0.8167 time: 0.4641s\n",
      "Epoch: 0746 loss_train: 0.6242 acc_train: 0.8357 loss_val: 0.6541 acc_val: 0.8167 time: 0.3830s\n",
      "Epoch: 0747 loss_train: 0.6466 acc_train: 0.8429 loss_val: 0.6534 acc_val: 0.8200 time: 0.4340s\n",
      "Epoch: 0748 loss_train: 0.5924 acc_train: 0.8429 loss_val: 0.6527 acc_val: 0.8200 time: 0.4014s\n",
      "Epoch: 0749 loss_train: 0.6130 acc_train: 0.8286 loss_val: 0.6521 acc_val: 0.8200 time: 0.3782s\n",
      "Epoch: 0750 loss_train: 0.5488 acc_train: 0.8643 loss_val: 0.6514 acc_val: 0.8200 time: 0.3955s\n",
      "Epoch: 0751 loss_train: 0.5113 acc_train: 0.8571 loss_val: 0.6507 acc_val: 0.8200 time: 0.3959s\n",
      "Epoch: 0752 loss_train: 0.6296 acc_train: 0.8000 loss_val: 0.6499 acc_val: 0.8200 time: 0.4228s\n",
      "Epoch: 0753 loss_train: 0.6162 acc_train: 0.8214 loss_val: 0.6490 acc_val: 0.8200 time: 0.3937s\n",
      "Epoch: 0754 loss_train: 0.5583 acc_train: 0.8857 loss_val: 0.6480 acc_val: 0.8200 time: 0.4524s\n",
      "Epoch: 0755 loss_train: 0.6178 acc_train: 0.8429 loss_val: 0.6472 acc_val: 0.8200 time: 0.4355s\n",
      "Epoch: 0756 loss_train: 0.6887 acc_train: 0.7571 loss_val: 0.6464 acc_val: 0.8200 time: 0.4208s\n",
      "Epoch: 0757 loss_train: 0.6007 acc_train: 0.8643 loss_val: 0.6457 acc_val: 0.8200 time: 0.4604s\n",
      "Epoch: 0758 loss_train: 0.6403 acc_train: 0.7786 loss_val: 0.6452 acc_val: 0.8200 time: 0.3936s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 314.3164s\n",
      "Loading 657th epoch\n",
      "Test set results: loss= 0.6549 accuracy= 0.8470\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "t_total = time.time()\n",
    "\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = 10000 + 1\n",
    "best_epoch = 0\n",
    "\n",
    "\n",
    "for epoch in range(10000):\n",
    "\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == 100:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "\n",
    "\n",
    "# Testing\n",
    "compute_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN_GAT",
   "language": "python",
   "name": "gnn_gat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
