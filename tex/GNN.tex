%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%文档类型
\documentclass{article}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%引入宏包
\usepackage[fleqn]{amsmath}  % https://zhuanlan.zhihu.com/p/464170020
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{geometry}
%\geometry{a4paper, landscape}  % 设置A4纸张并转为横向模式
\usepackage{CJKutf8}

\usepackage{booktabs}  % 三线表


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%正文内容
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{ch1: Hypergraph}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解： \\[3pt]
\end{CJK}


%
\begin{align*}
    & V = \{ v_{1}, v_{2}, v_{3}, v_{4} \}  \qquad \qquad \qquad
    E = \{ e_{1}, e_{2}, e_{3} \}  \qquad \qquad \qquad
    G = \{ V, E \}  \\[12pt]
    & D = 
        \begin{bmatrix}
        3  &  &  & \\
        &  1  &  & \\
        &  &  1  & \\
        &  &  &  1
        \end{bmatrix} \qquad \qquad
      A = 
        \begin{bmatrix}
            0 & 1 & 1 & 1 \\
            1 & 0 &   &	  \\
            1 &   & 0 &	  \\
            1 &   &   & 0
        \end{bmatrix} \qquad \qquad
      L = D - A = 
        \begin{bmatrix}
            3 & -1 & -1 & -1 \\
            -1&  1 &    &	 \\
            -1&    &  1 &	 \\
            -1&    &    &  1
        \end{bmatrix}  \\[12pt]
    & L_{sym} = D^{-1/2}LD^{-1/2} = I - D^{-1/2}AD^{-1/2}  \\[3pt]
    & L_{rw} = D^{-1}L = I - D^{-1}A 
\end{align*}

%
\begin{align*}
    & \left | \lambda I - L \right | = 0  \\[3pt]
    & L \overrightarrow{u} = \lambda \overrightarrow{u}  \\[3pt]
    & L = U \Lambda U^{T} = \sum_{1}^{N} \lambda_{k} u_{k} \cdot u_{k}^{T} 
\end{align*}

%
\begin{align*}
    & \forall s \in R^{N} \qquad \qquad 
      p_{k} = |s| \cos \theta_{k} = 
      \frac{u_{k}^{T} \cdot s}{|u_{k}^{T}|} = 
      u_{k}^{T} \cdot s  \\[3pt]
    & p = U^{T} s \qquad \qquad s = U p 
\end{align*}

%
\begin{align*}
    & TV(s) = s^{T} L s = s^{T} U \Lambda U^{T} s = 
      ( U^{T} s )^{T} \Lambda ( U^{T} s ) = 
      p^{T} \Lambda p = \sum_{1}^{N} 
      p_{k}^{2} \lambda_{k} \ge 0  \\[3pt]
    & E(s) = |s|^{2} = ( U p )^{T} \cdot ( U p ) = 
      p^{T} p = \sum_{1}^{N} p_{k}^{2} 
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch2: Hypergraph Learning Architecture}


~ \\[3pt]
\begin{align*}
    & s_{in} \left ( \sum_{1}^{N} p_{k} {u_{k}} \right )
      \to H \to 
      s_{out} \left ( \sum_{1}^{N} p_{k}^{'} {u_{k}} \right )  \\[3pt]
    & s_{in} \left ( \sum_{1}^{N} p_{k} {u_{k}} \right )
      \to H(filter) \to 
      s_{out} \left ( \sum_{1}^{N} h(\lambda_{k}) p_{k} u_{k} \right )  \\[3pt]
\end{align*}


\begin{align*}
    & s_{out} = H s_{in} = \sum_{1}^{N} h(\lambda_{k}) p_{k} u_{k}  \\[3pt]
    & = 
        \begin{bmatrix}
            u_{1} & u_{2} & ... & u_{n}
        \end{bmatrix}
        \begin{bmatrix}
            h \left ( \lambda_{1} \right ) p_{1} \\
            h \left ( \lambda_{2} \right ) p_{2} \\
            ... \\
            h \left ( \lambda_{n} \right ) p_{n}
        \end{bmatrix} 
      = 
        U
        \begin{bmatrix}
            h \left ( \lambda_{1} \right ) &  &  & \\
            &  h \left ( \lambda_{2} \right ) &  & \\
            &  &  ... & \\
            &  &  & h \left ( \lambda_{n} \right )
        \end{bmatrix}
        \begin{bmatrix}
            p_{1} \\
            p_{2} \\
            ... \\
            p_{n}
        \end{bmatrix}  \\[3pt]
    & = 
        U
        \begin{bmatrix}
            h \left ( \lambda_{1} \right ) &  &  & \\
            &  h \left ( \lambda_{2} \right ) &  & \\
            &  &  ... & \\
            &  &  & h \left ( \lambda_{n} \right )
        \end{bmatrix}
        U^{T} \overrightarrow{s_{in}}  \\[3pt]
\end{align*}

\begin{align*}
    & H = 
        U
        \begin{bmatrix}
            h \left ( \lambda_{1} \right ) &  &  & \\
            &  h \left ( \lambda_{2} \right ) &  & \\
            &  &  ... & \\
            &  &  & h \left ( \lambda_{n} \right )
        \end{bmatrix}
        U^{T}
        = 
        U \Lambda_{h} U^{T}  \\[3pt]
    & \Lambda_{h} = 
        \lim_{K \to \infty} \sum_{0}^{K} h_{k} \Lambda^{k} \qquad K \ll N  \\[3pt]
    & H = U
    \left (
    h_{0} \Lambda^{0} +
    h_{1} \Lambda^{1} +
    h_{2} \Lambda^{2} +
    ... +
    h_{K} \Lambda^{K}
    \right )
    U^{T}  \\[3pt]
    &= 
    U \left ( h_{0} \Lambda^{0} \right ) U^{T} +
    U \left ( h_{1} \Lambda^{1} \right ) U^{T} +
    U \left ( h_{2} \Lambda^{2} \right ) U^{T} +
    ... +
    U \left ( h_{K} \Lambda^{K} \right ) U^{T} \\[3pt]
    &=
    h_{0} L^{0} +
    h_{1} L^{1} +
    h_{2} L^{2} +
    ... +
    h_{K} L^{K}
    = 
    \sum_{0}^{K} h_{k} L^{k} \\[3pt]
    & K = 1 \qquad \qquad
    H = 
h_{0} L^{0} +
h_{1} L^{1}
\end{align*}


~ \\[3pt]
(2) Transformation

~ \\[3pt]
Reductive Transformation
\begin{align*}
    & ( E, X, Y ) \Rightarrow A 
    \qquad \qquad 
    \text{hyperedges to edges} \\[3pt]
    & \text{clique expansion + adaptive expansion}
\end{align*}

~ \\[3pt]
Non-reductive Transformation
\begin{align*}
    & \text{star/line/tensor expansion}
\end{align*}


~ \\[3pt]
(3) Message
\begin{align*}
    & \text{whose: v-v \ v-e \ e-v} \\[3pt]
    & \text{what : e-consistent + e-dependent} \\[3pt]
    & \text{how  : fixed-pooling + learnable-pooling} 
\end{align*}


~ \\[3pt]
(4) Training


\end{document}

