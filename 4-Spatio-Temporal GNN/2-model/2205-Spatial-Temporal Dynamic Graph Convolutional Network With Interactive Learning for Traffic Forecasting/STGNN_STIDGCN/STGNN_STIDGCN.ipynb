{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd55038-43bc-4612-b2fe-36a2df8bf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "\n",
    "\n",
    "################################################################\n",
    "# https://github.com/LiuAoyu1998/STIDGCN\n",
    "\n",
    "# .log文件有模型结构\n",
    "################################################################\n",
    "# conda create -n STGNN_STIDGCN\n",
    "# conda activate STGNN_STIDGCN\n",
    "\n",
    "# conda install python=3.8\n",
    "\n",
    "# https://pytorch.org/get-started/previous-versions/\n",
    "# conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "\n",
    "\n",
    "################################################################\n",
    "# pip install pandas\n",
    "# pip install scipy\n",
    "\n",
    "\n",
    "################################################################\n",
    "# conda install ipykernel\n",
    "# conda install platformdirs\n",
    "# pip3 install ipywidgets\n",
    "# pip3 install --upgrade jupyter_core jupyter_client\n",
    "\n",
    "# python -m ipykernel install --user --name STGNN_STIDGCN\n",
    "\n",
    "\n",
    "################################################################\n",
    "# train.py    cuda:0\n",
    "\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b9ec6b-05f9-419d-8d96-8d7b344ebaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # 数据分析\n",
    "import random\n",
    "import time\n",
    "\n",
    "import util\n",
    "from util import *\n",
    "from model import STIDGCN\n",
    "from ranger21 import Ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb32efac-984b-4250-96dc-2861c95b8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--device\",     type=str, default=\"cuda:0\", help=\"\")\n",
    "\n",
    "# (10699/3567/3567, 12, 170, 3)\n",
    "parser.add_argument(\"--dataset\",    type=str, default=\"PEMS08\", help=\"data path\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64,       help=\"batch size\")\n",
    "# Nodes\n",
    "parser.add_argument(\"--input_dim\",  type=int, default=3,        help=\"number of input_dim\")\n",
    "# windows\n",
    "\n",
    "# model parametres\n",
    "parser.add_argument(\"--dropout\",       type=float, default=0.1,    help=\"dropout rate\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001,  help=\"learning rate\")\n",
    "parser.add_argument(\"--weight_decay\",  type=float, default=0.0001, help=\"weight decay rate\")\n",
    "\n",
    "# training parametres\n",
    "parser.add_argument(\"--epochs\",      type=int, default=500, help=\"\")\n",
    "parser.add_argument(\"--print_every\", type=int, default=50,  help=\"\")\n",
    "parser.add_argument(\"--save\",        type=str, default=\"./logs/\" + str(time.strftime(\"%Y-%m-%d-%H:%M:%S\")) + \"-\", help=\"save path\")\n",
    "parser.add_argument(\"--expid\",       type=int, default=1,   help=\"experiment id\")\n",
    "parser.add_argument(\"--es_patience\", type=int, default=100, help=\"quit if no improvement after this many iterations\")\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab5b56f-51c6-4b82-a8cd-a0550cff08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it(seed) :\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda117bf-9c5a-44f2-aed7-969f16814983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer :\n",
    "\n",
    "\n",
    "    def __init__(self, device, num_nodes, input_dim, channels, day_granularity, \n",
    "                 dropout, lrate, wdecay, scaler) :\n",
    "\n",
    "        self.model = STIDGCN(device, num_nodes, input_dim, channels, day_granularity, dropout)\n",
    "        self.model.to(device)\n",
    "\n",
    "        # self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        self.optimizer = Ranger(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "\n",
    "        self.loss   = util.MAE_torch\n",
    "        self.scaler = scaler\n",
    "        self.clip   = 5\n",
    "\n",
    "        print(\"The number of parameters: {}\".format(self.model.param_num()))\n",
    "        # print(self.model)\n",
    "\n",
    "\n",
    "    def train(self, input, real_val) :\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output = self.model(input)\n",
    "        output = output.transpose(1, 3)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "\n",
    "        real = torch.unsqueeze(real_val, dim=1)\n",
    "\n",
    "        loss = self.loss(predict, real, 0.0)\n",
    "        loss.backward()\n",
    "\n",
    "        if self.clip is not None :  # 5\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        mape  = util.MAPE_torch (predict, real, 0.0).item()\n",
    "        rmse  = util.RMSE_torch (predict, real, 0.0).item()\n",
    "        wmape = util.WMAPE_torch(predict, real, 0.0).item()\n",
    "\n",
    "        return loss.item(), mape, rmse, wmape\n",
    "\n",
    "\n",
    "    def eval(self, input, real_val) :\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        output = self.model(input)\n",
    "        output = output.transpose(1, 3)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "\n",
    "        real = torch.unsqueeze(real_val, dim=1)\n",
    "\n",
    "        loss = self.loss(predict, real, 0.0)\n",
    "\n",
    "        mape  = util.MAPE_torch (predict, real, 0.0).item()\n",
    "        rmse  = util.RMSE_torch (predict, real, 0.0).item()\n",
    "        wmape = util.WMAPE_torch(predict, real, 0.0).item()\n",
    "\n",
    "        return loss.item(), mape, rmse, wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d680393-2fd2-464f-95fc-8c012872ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform shuffle on the dataset\n"
     ]
    }
   ],
   "source": [
    "seed_it(6666)\n",
    "\n",
    "# [N, D, T] = 170*3*12\n",
    "# [N, D, T] = 170*96*288\n",
    "dataset  = \"PEMS08\"\n",
    "data_dir = \"data//\" + dataset\n",
    "num_nodes = 170\n",
    "# features = 3\n",
    "# windows = 12\n",
    "channels = 96\n",
    "day_granularity = 288\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "dataloader = util.load_dataset(data_dir, args.batch_size, args.batch_size, args.batch_size)\n",
    "\n",
    "scaler = dataloader[\"scaler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb9622e-1e20-4be5-815f-3c9b1d8dc621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "The number of parameters: 4423029\n",
      "start training...\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "test_log = 999999\n",
    "epochs_since_best_mae = 0\n",
    "path = args.save + args.dataset + \"/\"\n",
    "if not os.path.exists(path) :\n",
    "    os.makedirs(path)\n",
    "\n",
    "# \n",
    "loss        = 9999999\n",
    "his_loss    = []\n",
    "train_time  = []\n",
    "val_time    = []\n",
    "result      = []\n",
    "test_result = []\n",
    "\n",
    "# \n",
    "engine = trainer(device, num_nodes, args.input_dim, channels, day_granularity, \n",
    "                 args.dropout, args.learning_rate, args.weight_decay, scaler)\n",
    "\n",
    "print(\"start training...\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c9e30f-0088-4dd4-b0cd-6d1e077f8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: [64, 12, 170, 3]\n",
    "# STIDGCN(\n",
    "#   (Temb): TemporalEmbedding()\n",
    "#   (start_conv): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1))\n",
    "#   (tree): IDGCN_Tree(\n",
    "#     (IDGCN1): IDGCN(\n",
    "#       (split): Splitting()\n",
    "#       (conv1): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv2): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv3): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv4): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (dgcn): DGCN(\n",
    "#         (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#         (generator): Graph_Generator(\n",
    "#           (fc): Linear(in_features=2, out_features=1, bias=True)\n",
    "#         )\n",
    "#         (gcn): Diffusion_GCN(\n",
    "#           (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#           (dropout): Dropout(p=0.1, inplace=False)\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#     (IDGCN2): IDGCN(\n",
    "#       (split): Splitting()\n",
    "#       (conv1): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv2): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv3): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv4): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (dgcn): DGCN(\n",
    "#         (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#         (generator): Graph_Generator(\n",
    "#           (fc): Linear(in_features=2, out_features=1, bias=True)\n",
    "#         )\n",
    "#         (gcn): Diffusion_GCN(\n",
    "#           (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#           (dropout): Dropout(p=0.1, inplace=False)\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#     (IDGCN3): IDGCN(\n",
    "#       (split): Splitting()\n",
    "#       (conv1): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv2): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv3): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv4): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (dgcn): DGCN(\n",
    "#         (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#         (generator): Graph_Generator(\n",
    "#           (fc): Linear(in_features=2, out_features=1, bias=True)\n",
    "#         )\n",
    "#         (gcn): Diffusion_GCN(\n",
    "#           (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#           (dropout): Dropout(p=0.1, inplace=False)\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#   )\n",
    "#   (glu): GLU(\n",
    "#     (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#     (conv2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#     (conv3): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#     (dropout): Dropout(p=0.1, inplace=False)\n",
    "#   )\n",
    "#   (regression_layer): Conv2d(192, 12, kernel_size=(1, 12), stride=(1, 1))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d15ef4-b60a-492b-a606-658ec69083c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 96, 170, 12])\n",
      "Iter: 000,                    Train Loss: 118.7981,                    Train RMSE: 145.8948,                    Train MAPE: 2.0573,                    Train WMAPE: 0.4951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhfc/github files/4.1-Graph-Neural-Networks/4-Spatio-Temporal GNN/2-model/2205-Spatial-Temporal Dynamic Graph Convolutional Network With Interactive Learning for Traffic Forecasting/STGNN_STIDGCN/ranger21.py:139: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1-beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n",
      "torch.Size([64, 96, 170, 12])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m trainy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(y)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m trainy \u001b[38;5;241m=\u001b[39m trainy\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainy\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     20\u001b[0m train_mape\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mtrainer.train\u001b[0;34m(self, input, real_val)\u001b[0m\n\u001b[1;32m     36\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39minverse_transform(output)\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(predict, real, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[1;32m     41\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip)\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, args.epochs + 1) :\n",
    "\n",
    "    train_loss  = []\n",
    "    train_mape  = []\n",
    "    train_rmse  = []\n",
    "    train_wmape = []\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    for iter, (x, y) in enumerate(dataloader[\"train_loader\"].get_iterator()) :  # 返回批数据\n",
    "\n",
    "        trainx = torch.Tensor(x).to(device)\n",
    "        # print(trainx.shape)  # torch.Size([64, 12, 170, 3])\n",
    "        trainx = trainx.transpose(1, 3)\n",
    "        trainy = torch.Tensor(y).to(device)\n",
    "        trainy = trainy.transpose(1, 3)\n",
    "\n",
    "        metrics = engine.train(trainx, trainy[:, 0, :, :])\n",
    "        train_loss.append(metrics[0])\n",
    "        train_mape.append(metrics[1])\n",
    "        train_rmse.append(metrics[2])\n",
    "        train_wmape.append(metrics[3])\n",
    "\n",
    "        if iter % args.print_every == 0 :\n",
    "            log = \"Iter: {:03d}, \\\n",
    "                   Train Loss: {:.4f}, \\\n",
    "                   Train RMSE: {:.4f}, \\\n",
    "                   Train MAPE: {:.4f}, \\\n",
    "                   Train WMAPE: {:.4f}\"\n",
    "            print(\n",
    "                log.format(\n",
    "                    iter, \n",
    "                    train_loss[-1], \n",
    "                    train_rmse[-1], \n",
    "                    train_mape[-1], \n",
    "                    train_wmape[-1], \n",
    "                ), \n",
    "                flush=True, \n",
    "            )\n",
    "\n",
    "    t2 = time.time()\n",
    "    log = \"Epoch: {:03d}, Training Time: {:.4f} secs\"\n",
    "    print(log.format(i, (t2 - t1)))\n",
    "    train_time.append(t2 - t1)\n",
    "\n",
    "    valid_loss  = []\n",
    "    valid_mape  = []\n",
    "    valid_wmape = []\n",
    "    valid_rmse  = []\n",
    "\n",
    "    # \n",
    "    s1 = time.time()\n",
    "\n",
    "    for iter, (x, y) in enumerate(dataloader[\"val_loader\"].get_iterator()) :\n",
    "\n",
    "        testx = torch.Tensor(x).to(device)\n",
    "        testx = testx.transpose(1, 3)\n",
    "        testy = torch.Tensor(y).to(device)\n",
    "        testy = testy.transpose(1, 3)\n",
    "\n",
    "        metrics = engine.eval(testx, testy[:, 0, :, :])\n",
    "        valid_loss .append(metrics[0])\n",
    "        valid_mape .append(metrics[1])\n",
    "        valid_rmse .append(metrics[2])\n",
    "        valid_wmape.append(metrics[3])\n",
    "\n",
    "    s2 = time.time()\n",
    "\n",
    "    log = \"Epoch: {:03d}, Inference Time: {:.4f} secs\"\n",
    "    print(log.format(i, (s2 - s1)))\n",
    "    val_time.append(s2 - s1)\n",
    "\n",
    "    mtrain_loss  = np.mean(train_loss)\n",
    "    mtrain_mape  = np.mean(train_mape)\n",
    "    mtrain_wmape = np.mean(train_wmape)\n",
    "    mtrain_rmse  = np.mean(train_rmse)\n",
    "\n",
    "    mvalid_loss  = np.mean(valid_loss)\n",
    "    mvalid_mape  = np.mean(valid_mape)\n",
    "    mvalid_wmape = np.mean(valid_wmape)\n",
    "    mvalid_rmse  = np.mean(valid_rmse)\n",
    "\n",
    "    his_loss.append(mvalid_loss)\n",
    "    train_m = dict(\n",
    "        train_loss =np.mean(train_loss), \n",
    "        train_rmse =np.mean(train_rmse), \n",
    "        train_mape =np.mean(train_mape), \n",
    "        train_wmape=np.mean(train_wmape), \n",
    "\n",
    "        valid_loss =np.mean(valid_loss), \n",
    "        valid_rmse =np.mean(valid_rmse), \n",
    "        valid_mape =np.mean(valid_mape), \n",
    "        valid_wmape=np.mean(valid_wmape), \n",
    "    )\n",
    "    train_m = pd.Series(train_m)\n",
    "    result.append(train_m)\n",
    "\n",
    "    log = \"Epoch: {:03d}, \\\n",
    "           Train Loss: {:.4f}, \\\n",
    "           Train RMSE: {:.4f}, \\\n",
    "           Train MAPE: {:.4f}, \\\n",
    "           Train WMAPE: {:.4f}, \"\n",
    "    print(\n",
    "        log.format(i, mtrain_loss, mtrain_rmse, mtrain_mape, mtrain_wmape), \n",
    "        flush=True, \n",
    "    )\n",
    "    log = \"Epoch: {:03d}, \\\n",
    "           Valid Loss: {:.4f}, \\\n",
    "           Valid RMSE: {:.4f}, \\\n",
    "           Valid MAPE: {:.4f}, \\\n",
    "           Valid WMAPE: {:.4f}\"\n",
    "    print(\n",
    "        log.format(i, mvalid_loss, mvalid_rmse, mvalid_mape, mvalid_wmape), \n",
    "        flush=True, \n",
    "    )\n",
    "\n",
    "    if mvalid_loss < loss :\n",
    "        print(\"### Update tasks appear ###\")\n",
    "\n",
    "        if i < 100 :\n",
    "            # It is not necessary to print the results of the test set \n",
    "            # when epoch is less than 100, because the model has not yet converged.\n",
    "            loss = mvalid_loss\n",
    "            torch.save(engine.model.state_dict(), path + \"best_model.pth\")\n",
    "            bestid = i\n",
    "            epochs_since_best_mae = 0\n",
    "            print(\"Updating! Valid Loss:\", mvalid_loss, end=\", \")\n",
    "            print(\"epoch: \", i)\n",
    "\n",
    "        elif i > 100 :\n",
    "            outputs = []\n",
    "            realy = torch.Tensor(dataloader[\"y_test\"]).to(device)\n",
    "            realy = realy.transpose(1, 3)[:, 0, :, :]\n",
    "\n",
    "            for iter, (x, y) in enumerate(dataloader[\"test_loader\"].get_iterator()) :\n",
    "                testx = torch.Tensor(x).to(device)\n",
    "                testx = testx.transpose(1, 3)\n",
    "                with torch.no_grad() :\n",
    "                    preds = engine.model(testx).transpose(1, 3)\n",
    "                outputs.append(preds.squeeze())\n",
    "\n",
    "            yhat = torch.cat(outputs, dim=0)\n",
    "            yhat = yhat[: realy.size(0), ...]\n",
    "\n",
    "            amae   = []\n",
    "            amape  = []\n",
    "            awmape = []\n",
    "            armse  = []\n",
    "            test_m = []\n",
    "\n",
    "            for j in range(12) :\n",
    "\n",
    "                pred = scaler.inverse_transform(yhat[:, :, j])\n",
    "                real = realy[:, :, j]\n",
    "                metrics = util.metric(pred, real)\n",
    "                log = \"Evaluate best model on test data for horizon {:d}, \\\n",
    "                    Test MAE  : {:.4f}, \\\n",
    "                    Test RMSE : {:.4f}, \\\n",
    "                    Test MAPE : {:.4f}, \\\n",
    "                    Test WMAPE: {:.4f}\"\n",
    "                print(\n",
    "                    log.format(\n",
    "                        j + 1, metrics[0], metrics[2], metrics[1], metrics[3]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                test_m = dict(\n",
    "                    test_loss =np.mean(metrics[0]), \n",
    "                    test_rmse =np.mean(metrics[2]), \n",
    "                    test_mape =np.mean(metrics[1]), \n",
    "                    test_wmape=np.mean(metrics[3]), \n",
    "                )\n",
    "                test_m = pd.Series(test_m)\n",
    "\n",
    "                amae  .append(metrics[0])\n",
    "                amape .append(metrics[1])\n",
    "                armse .append(metrics[2])\n",
    "                awmape.append(metrics[3])\n",
    "\n",
    "            log = \"On average over 12 horizons, \\\n",
    "                   Test MAE  : {:.4f}, \\\n",
    "                   Test RMSE : {:.4f}, \\\n",
    "                   Test MAPE : {:.4f}, \\\n",
    "                   Test WMAPE: {:.4f}\"\n",
    "            print(\n",
    "                log.format(\n",
    "                    np.mean(amae), np.mean(armse), np.mean(amape), np.mean(awmape)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if np.mean(amae) < test_log :\n",
    "                test_log = np.mean(amae)\n",
    "                loss = mvalid_loss\n",
    "                torch.save(engine.model.state_dict(), path + \"best_model.pth\")\n",
    "                epochs_since_best_mae = 0\n",
    "                print(\"Test low! Updating! Test Loss :\", np.mean(amae), end=\", \")\n",
    "                print(\"Test low! Updating! Valid Loss:\", mvalid_loss  , end=\", \")\n",
    "                bestid = i\n",
    "                print(\"epoch: \", i)\n",
    "            else :\n",
    "                epochs_since_best_mae += 1\n",
    "                print(\"No update\")\n",
    "\n",
    "    else :\n",
    "        epochs_since_best_mae += 1\n",
    "        print(\"No update\")\n",
    "\n",
    "    train_csv = pd.DataFrame(result)\n",
    "    train_csv.round(8).to_csv(f\"{path}/train.csv\")\n",
    "    if epochs_since_best_mae >= args.es_patience and i >= 300 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c8e17-eb80-4c66-90cd-5c0167cb3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(\"Average Training Time : {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
    "# print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
    "\n",
    "# print(\"Training ends\")\n",
    "# print(\"The epoch of the best result：\", bestid)\n",
    "# print(\"The valid loss of the best model\", str(round(his_loss[bestid - 1], 4)))\n",
    "\n",
    "# engine.model.load_state_dict(torch.load(path + \"best_model.pth\"))\n",
    "# outputs = []\n",
    "# realy = torch.Tensor(dataloader[\"y_test\"]).to(device)\n",
    "# realy = realy.transpose(1, 3)[:, 0, :, :]\n",
    "\n",
    "# for iter, (x, y) in enumerate(dataloader[\"test_loader\"].get_iterator()) :\n",
    "#     testx = torch.Tensor(x).to(device)\n",
    "#     testx = testx.transpose(1, 3)\n",
    "#     with torch.no_grad() :\n",
    "#         preds = engine.model(testx).transpose(1, 3)\n",
    "#     outputs.append(preds.squeeze())\n",
    "\n",
    "# yhat = torch.cat(outputs, dim=0)\n",
    "# yhat = yhat[: realy.size(0), ...]\n",
    "\n",
    "# amae   = []\n",
    "# amape  = []\n",
    "# armse  = []\n",
    "# awmape = []\n",
    "\n",
    "# test_m = []\n",
    "\n",
    "# for i in range(12) :\n",
    "#     pred = scaler.inverse_transform(yhat[:, :, i])\n",
    "#     real = realy[:, :, i]\n",
    "#     metrics = util.metric(pred, real)\n",
    "#     log = \"Evaluate best model on test data for horizon {:d}, \\\n",
    "#            Test MAE: {:.4f}, \\\n",
    "#            Test RMSE: {:.4f}, \\\n",
    "#            Test MAPE: {:.4f}, \\\n",
    "#            Test WMAPE: {:.4f}\"\n",
    "#     print(log.format(i + 1, metrics[0], metrics[2], metrics[1], metrics[3]))\n",
    "\n",
    "#     test_m = dict(\n",
    "#         test_loss =np.mean(metrics[0]), \n",
    "#         test_rmse =np.mean(metrics[2]), \n",
    "#         test_mape =np.mean(metrics[1]), \n",
    "#         test_wmape=np.mean(metrics[3]), \n",
    "#     )\n",
    "#     test_m = pd.Series(test_m)\n",
    "#     test_result.append(test_m)\n",
    "\n",
    "#     amae  .append(metrics[0])\n",
    "#     amape .append(metrics[1])\n",
    "#     armse .append(metrics[2])\n",
    "#     awmape.append(metrics[3])\n",
    "\n",
    "# log = \"On average over 12 horizons, \\\n",
    "#        Test MAE: {:.4f}, \\\n",
    "#        Test RMSE: {:.4f}, \\\n",
    "#        Test MAPE: {:.4f}, \\\n",
    "#        Test WMAPE: {:.4f}\"\n",
    "# print(log.format(np.mean(amae), np.mean(armse), np.mean(amape), np.mean(awmape)))\n",
    "\n",
    "# test_m = dict(\n",
    "#     test_loss =np.mean(amae), \n",
    "#     test_rmse =np.mean(armse), \n",
    "#     test_mape =np.mean(amape), \n",
    "#     test_wmape=np.mean(awmape), \n",
    "# )\n",
    "# test_m = pd.Series(test_m)\n",
    "# test_result.append(test_m)\n",
    "\n",
    "# test_csv = pd.DataFrame(test_result)\n",
    "# test_csv.round(8).to_csv(f\"{path}/test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STGNN_STIDGCN",
   "language": "python",
   "name": "stgnn_stidgcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
