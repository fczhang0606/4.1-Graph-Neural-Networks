{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd55038-43bc-4612-b2fe-36a2df8bf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "\n",
    "\n",
    "################################################################\n",
    "# https://github.com/LiuAoyu1998/STIDGCN\n",
    "\n",
    "# .log文件有模型结构\n",
    "################################################################\n",
    "# conda create -n STGNN_STIDGCN\n",
    "# conda activate STGNN_STIDGCN\n",
    "\n",
    "# conda install python=3.8\n",
    "\n",
    "# https://pytorch.org/get-started/previous-versions/\n",
    "# conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "\n",
    "\n",
    "################################################################\n",
    "# pip install pandas\n",
    "# pip install scipy\n",
    "\n",
    "\n",
    "################################################################\n",
    "# conda install ipykernel\n",
    "# conda install platformdirs\n",
    "# pip3 install ipywidgets\n",
    "# pip3 install --upgrade jupyter_core jupyter_client\n",
    "\n",
    "# python -m ipykernel install --user --name STGNN_STIDGCN\n",
    "\n",
    "\n",
    "################################################################\n",
    "# train.py    cuda:0\n",
    "\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b9ec6b-05f9-419d-8d96-8d7b344ebaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # 数据分析\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "\n",
    "import util\n",
    "from util import *\n",
    "from ranger21 import Ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb32efac-984b-4250-96dc-2861c95b8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda:0\", help=\"\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"PEMS08\", help=\"data path\")\n",
    "parser.add_argument(\"--input_dim\", type=int, default=3, help=\"number of input_dim\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"batch size\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001, help=\"learning rate\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout rate\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.0001, help=\"weight decay rate\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=500, help=\"\")\n",
    "parser.add_argument(\"--print_every\", type=int, default=50, help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--save\", \n",
    "    type=str, \n",
    "    default=\"./logs/\" + str(time.strftime(\"%Y-%m-%d-%H:%M:%S\")) + \"-\", \n",
    "    help=\"save path\", \n",
    ")\n",
    "parser.add_argument(\"--expid\", type=int, default=1, help=\"experiment id\")\n",
    "parser.add_argument(\n",
    "    \"--es_patience\", \n",
    "    type=int, \n",
    "    default=100, \n",
    "    help=\"quit if no improvement after this many iterations\", \n",
    ")\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f469744-f322-4de7-adf1-791b1da76479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module) :  # 常规GLU\n",
    "\n",
    "\n",
    "    def __init__(self, features, dropout=0.1) :\n",
    "\n",
    "        super(GLU, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(features, features, (1, 1))\n",
    "        self.conv2 = nn.Conv2d(features, features, (1, 1))\n",
    "        self.conv3 = nn.Conv2d(features, features, (1, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        out = x1 * torch.sigmoid(x2)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b793f6f3-6a7d-4369-a3cf-3e2138d1c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalEmbedding(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, time, features) :\n",
    "\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        self.time = time  # 时刻step\n",
    "        self.time_day = nn.Parameter(torch.empty(time, features))  # array[][]\n",
    "        nn.init.xavier_uniform_(self.time_day)  # 均匀分布\n",
    "\n",
    "        self.time_week = nn.Parameter(torch.empty(7, features))\n",
    "        nn.init.xavier_uniform_(self.time_week)\n",
    "\n",
    "\n",
    "    def forward(self, x) :  # 原理？\n",
    "\n",
    "        day_emb  = x[..., 1]  # 这是什么操作？\n",
    "        time_day = self.time_day[(day_emb[:, :, :]*self.time).type(torch.LongTensor)]\n",
    "        time_day = time_day.transpose(1, 2).contiguous()\n",
    "\n",
    "        week_emb  = x[..., 2]\n",
    "        time_week = self.time_week[(week_emb[:, :, :]).type(torch.LongTensor)]\n",
    "        time_week = time_week.transpose(1, 2).contiguous()\n",
    "\n",
    "        tem_emb = time_day + time_week\n",
    "\n",
    "        tem_emb = tem_emb.permute(0, 3, 1, 2)\n",
    "\n",
    "        return tem_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83236abd-527c-4d40-8e0c-089b7ddbf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion_GCN(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, channels=128, diffusion_step=1, dropout=0.1) :\n",
    "\n",
    "        super().__init__()\n",
    "        self.diffusion_step = diffusion_step  # k\n",
    "        self.conv    = nn.Conv2d(diffusion_step*channels, channels, (1, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj) :\n",
    "\n",
    "        out = []\n",
    "\n",
    "        for i in range(0, self.diffusion_step) :\n",
    "\n",
    "            if adj.dim() == 3 :\n",
    "                x = torch.einsum(\"bcnt, bnm->bcmt\", x, adj).contiguous()\n",
    "                out.append(x)\n",
    "            elif adj.dim() == 2 :\n",
    "                x = torch.einsum(\"bcnt, nm->bcmt\", x, adj).contiguous()\n",
    "                out.append(x)\n",
    "\n",
    "        x = torch.cat(out, dim=1)  # \n",
    "        x = self.conv(x)\n",
    "        output = self.dropout(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5691eb95-8c55-4394-80a9-a84ab3095426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Generator(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, channels=128, num_nodes=170, diffusion_step=1, dropout=0.1) :\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.memory = nn.Parameter(torch.randn(channels, num_nodes))\n",
    "        nn.init.xavier_uniform_(self.memory)\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        adj_dyn_1 = torch.softmax(  # A1\n",
    "            F.relu(\n",
    "                torch.einsum(\"bcnt, cm->bnm\", x, self.memory).contiguous()\n",
    "                / math.sqrt(x.shape[1])\n",
    "            ), \n",
    "            -1, \n",
    "        )\n",
    "        adj_dyn_2 = torch.softmax(  # A2\n",
    "            F.relu(\n",
    "                torch.einsum(\"bcn, bcm->bnm\", x.sum(-1), x.sum(-1)).contiguous()\n",
    "                / math.sqrt(x.shape[1])\n",
    "            ), \n",
    "            -1, \n",
    "        )\n",
    "\n",
    "        # adj_dyn = (adj_dyn_1 + adj_dyn_2 + adj)/2\n",
    "        adj_f = torch.cat([(adj_dyn_1).unsqueeze(-1)] + [(adj_dyn_2).unsqueeze(-1)], dim=-1)  # 融合A\n",
    "        adj_f = torch.softmax(self.fc(adj_f).squeeze(), -1)\n",
    "\n",
    "        topk_values, topk_indices = torch.topk(adj_f, k=int(adj_f.shape[1]*0.8), dim=-1)  # 构图\n",
    "        mask = torch.zeros_like(adj_f)\n",
    "        mask.scatter_(-1, topk_indices, 1)\n",
    "        adj_f = adj_f * mask\n",
    "\n",
    "        return adj_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e8513d-c443-49b3-83be-211355d644c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCN(nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, channels=128, num_nodes=170, diffusion_step=1, dropout=0.1, emb=None) :\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, (1, 1))\n",
    "        self.generator = Graph_Generator(channels, num_nodes, diffusion_step, dropout)\n",
    "        self.gcn = Diffusion_GCN(channels, diffusion_step, dropout)\n",
    "        self.emb = emb\n",
    "\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        skip = x\n",
    "        x = self.conv(x)\n",
    "        adj_dyn = self.generator(x)\n",
    "        x = self.gcn(x, adj_dyn) \n",
    "        x = x*self.emb + skip\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514598cf-37b6-4162-b37f-b2a86d0284ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitting(nn.Module) :\n",
    "\n",
    "    def __init__(self) :\n",
    "        super(Splitting, self).__init__()\n",
    "\n",
    "    def even(self, x) :\n",
    "        return x[:, :, :, ::2]  # 偶数位\n",
    "\n",
    "    def odd(self, x) :\n",
    "        return x[:, :, :, 1::2]  # 奇数位\n",
    "\n",
    "    def forward(self, x) :\n",
    "        return (self.even(x), self.odd(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3845db37-9aec-4dd8-9c0c-f69615b3a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDGCN(nn.Module) :\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        device, \n",
    "        channels=64, \n",
    "        diffusion_step=1, \n",
    "        splitting=True, \n",
    "        num_nodes=170, \n",
    "        dropout=0.2, \n",
    "        emb = None\n",
    "    ) :\n",
    "        super(IDGCN, self).__init__()\n",
    "\n",
    "        device = device\n",
    "        self.dropout = dropout\n",
    "        self.num_nodes = num_nodes\n",
    "        self.splitting = splitting\n",
    "        self.split = Splitting()\n",
    "\n",
    "        Conv1 = []\n",
    "        Conv2 = []\n",
    "        Conv3 = []\n",
    "        Conv4 = []\n",
    "        pad_l = 3\n",
    "        pad_r = 3\n",
    "\n",
    "        k1 = 5\n",
    "        k2 = 3\n",
    "        Conv1 += [\n",
    "            nn.ReplicationPad2d((pad_l, pad_r, 0, 0)), \n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, k1)), \n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True), \n",
    "            nn.Dropout(self.dropout), \n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, k2)), \n",
    "            nn.Tanh(), \n",
    "        ]\n",
    "        Conv2 += [\n",
    "            nn.ReplicationPad2d((pad_l, pad_r, 0, 0)), \n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, k1)), \n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True), \n",
    "            nn.Dropout(self.dropout), \n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, k2)), \n",
    "            nn.Tanh(), \n",
    "        ]\n",
    "        Conv4 += [\n",
    "            nn.ReplicationPad2d((pad_l, pad_r, 0, 0)), \n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, k1)), \n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True), \n",
    "            nn.Dropout(self.dropout), \n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, k2)), \n",
    "            nn.Tanh(), \n",
    "        ]\n",
    "        Conv3 += [\n",
    "            nn.ReplicationPad2d((pad_l, pad_r, 0, 0)), \n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, k1)), \n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True), \n",
    "            nn.Dropout(self.dropout), \n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, k2)), \n",
    "            nn.Tanh(), \n",
    "        ]\n",
    "\n",
    "        self.conv1 = nn.Sequential(*Conv1)\n",
    "        self.conv2 = nn.Sequential(*Conv2)\n",
    "        self.conv3 = nn.Sequential(*Conv3)\n",
    "        self.conv4 = nn.Sequential(*Conv4)\n",
    "\n",
    "        self.dgcn = DGCN(channels, num_nodes, diffusion_step, dropout, emb)\n",
    "\n",
    "    def forward(self, x) :\n",
    "    \n",
    "            if self.splitting :\n",
    "                (x_even, x_odd) = self.split(x)\n",
    "            else :\n",
    "                (x_even, x_odd) = x\n",
    "    \n",
    "            x1 = self.conv1(x_even)\n",
    "            x1 = self.dgcn(x1)\n",
    "            d = x_odd.mul(torch.tanh(x1))\n",
    "    \n",
    "            x2 = self.conv2(x_odd)\n",
    "            x2 = self.dgcn(x2)\n",
    "            c = x_even.mul(torch.tanh(x2))\n",
    "    \n",
    "            x3 = self.conv3(c)\n",
    "            x3 = self.dgcn(x3)\n",
    "            x_odd_update = d + x3\n",
    "    \n",
    "            x4 = self.conv4(d)\n",
    "            x4 = self.dgcn(x4)\n",
    "            x_even_update = c + x4\n",
    "    \n",
    "            return (x_even_update, x_odd_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ac58ff-536c-49a2-b4d4-24f8b18e2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDGCN_Tree(nn.Module) :\n",
    "\n",
    "    def __init__(\n",
    "        self, device, channels=64, diffusion_step=1, num_nodes=170, dropout=0.1\n",
    "    ) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.memory1 = nn.Parameter(torch.randn(channels, num_nodes, 6))\n",
    "        self.memory2 = nn.Parameter(torch.randn(channels, num_nodes, 3))\n",
    "        self.memory3 = nn.Parameter(torch.randn(channels, num_nodes, 3))\n",
    "\n",
    "        self.IDGCN1 = IDGCN(\n",
    "            device=device, \n",
    "            splitting=True, \n",
    "            channels=channels, \n",
    "            diffusion_step=diffusion_step, \n",
    "            num_nodes=num_nodes, \n",
    "            dropout=dropout, \n",
    "            emb=self.memory1\n",
    "        )\n",
    "        self.IDGCN2 = IDGCN(\n",
    "            device=device, \n",
    "            splitting=True, \n",
    "            channels=channels, \n",
    "            diffusion_step=diffusion_step, \n",
    "            num_nodes=num_nodes, \n",
    "            dropout=dropout, \n",
    "            emb=self.memory2\n",
    "        )\n",
    "        self.IDGCN3 = IDGCN(\n",
    "            device=device, \n",
    "            splitting=True, \n",
    "            channels=channels, \n",
    "            diffusion_step=diffusion_step, \n",
    "            num_nodes=num_nodes, \n",
    "            dropout=dropout, \n",
    "            emb=self.memory2\n",
    "        )\n",
    "\n",
    "\n",
    "    def concat(self, even, odd) :\n",
    "\n",
    "        even = even.permute(3, 1, 2, 0)\n",
    "        odd  = odd.permute(3, 1, 2, 0)\n",
    "        len  = even.shape[0]\n",
    "        _ = []\n",
    "\n",
    "        for i in range(len) :\n",
    "            _.append(even[i].unsqueeze(0))\n",
    "            _.append( odd[i].unsqueeze(0))\n",
    "\n",
    "        return torch.cat(_, 0).permute(3, 1, 2, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        x_even_update1, x_odd_update1 = self.IDGCN1(x)\n",
    "        x_even_update2, x_odd_update2 = self.IDGCN2(x_even_update1)\n",
    "        x_even_update3, x_odd_update3 = self.IDGCN3(x_odd_update1)\n",
    "\n",
    "        concat1 = self.concat(x_even_update2, x_odd_update2)\n",
    "        concat2 = self.concat(x_even_update3, x_odd_update3)\n",
    "        concat0 = self.concat(concat1, concat2)\n",
    "        output  = concat0 + x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0391f4-9f30-4fbe-ba66-d9687dfd2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STIDGCN(nn.Module) :\n",
    "\n",
    "    def __init__(\n",
    "        self, device, input_dim, num_nodes, channels, granularity, dropout=0.1\n",
    "    ) :\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.num_nodes = num_nodes\n",
    "        self.output_len = 12\n",
    "        diffusion_step = 1\n",
    "\n",
    "        self.Temb = TemporalEmbedding(granularity, channels)\n",
    "\n",
    "        self.start_conv = nn.Conv2d(\n",
    "            in_channels=input_dim, out_channels=channels, kernel_size=(1, 1)\n",
    "        )\n",
    "\n",
    "        self.tree = IDGCN_Tree(\n",
    "            device=device, \n",
    "            channels=channels*2, \n",
    "            diffusion_step=diffusion_step, \n",
    "            num_nodes=self.num_nodes, \n",
    "            dropout=dropout, \n",
    "        )\n",
    "\n",
    "        self.glu = GLU(channels*2, dropout)\n",
    "\n",
    "        self.regression_layer = nn.Conv2d(\n",
    "            channels*2, self.output_len, kernel_size=(1, self.output_len)\n",
    "        )\n",
    "\n",
    "\n",
    "    def param_num(self) :\n",
    "        return sum([param.nelement() for param in self.parameters()])\n",
    "\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        x = input\n",
    "\n",
    "        # Encoder\n",
    "        # Data Embedding\n",
    "        time_emb = self.Temb(input.permute(0, 3, 2, 1))\n",
    "        x = torch.cat([self.start_conv(x)] + [time_emb], dim=1)\n",
    "\n",
    "        # IDGCN_Tree\n",
    "        x = self.tree(x)\n",
    "\n",
    "        # Decoder\n",
    "        gcn = self.glu(x) + x\n",
    "        prediction = self.regression_layer(F.relu(gcn))\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c458f4-6eef-4008-b92b-105dab0159bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def MAE_torch(pred, true, mask_value=None) :\n",
    "\n",
    "    if mask_value != None :\n",
    "        mask = torch.gt(true, mask_value)  # ge/gt/le/lt/ne/eq分别是>=/>/<=/</==/!=\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "\n",
    "    return torch.mean(torch.abs(true - pred))\n",
    "\n",
    "\n",
    "# \n",
    "def MAPE_torch(pred, true, mask_value=None) :\n",
    "\n",
    "    if mask_value != None :\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "\n",
    "    return torch.mean(torch.abs(torch.div((true - pred), true)))\n",
    "\n",
    "\n",
    "# \n",
    "def RMSE_torch(pred, true, mask_value=None) :\n",
    "\n",
    "    if mask_value != None :\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "\n",
    "    return torch.sqrt(torch.mean((pred - true) ** 2))\n",
    "\n",
    "\n",
    "# \n",
    "def WMAPE_torch(pred, true, mask_value=None) :\n",
    "\n",
    "    if mask_value != None :\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    loss = torch.sum(torch.abs(pred - true)) / torch.sum(torch.abs(true))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# \n",
    "def metric(pred, real) :\n",
    "\n",
    "    mae   = MAE_torch  (pred, real, 0.0).item()\n",
    "    mape  = MAPE_torch (pred, real, 0.0).item()\n",
    "    rmse  = RMSE_torch (pred, real, 0.0).item()\n",
    "    wmape = WMAPE_torch(pred, real, 0.0).item()\n",
    "\n",
    "    return mae, mape, rmse, wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83476564-291a-4469-9929-46972d132afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer :\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        scaler, \n",
    "        input_dim, \n",
    "        num_nodes, \n",
    "        channels, \n",
    "        dropout, \n",
    "        lrate, \n",
    "        wdecay, \n",
    "        device, \n",
    "        granularity, \n",
    "    ) :\n",
    "        self.model = STIDGCN(device, input_dim, num_nodes, channels, granularity, dropout)\n",
    "        self.model.to(device)\n",
    "        self.optimizer = Ranger(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        # self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        self.loss = util.MAE_torch\n",
    "        self.scaler = scaler\n",
    "        self.clip = 5\n",
    "        print(\"The number of parameters: {}\".format(self.model.param_num()))\n",
    "        print(self.model)\n",
    "\n",
    "\n",
    "    def train(self, input, real_val) :\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(input)\n",
    "        output = output.transpose(1, 3)\n",
    "        real = torch.unsqueeze(real_val, dim=1)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "        loss = self.loss(predict, real, 0.0)\n",
    "        loss.backward()\n",
    "\n",
    "        if self.clip is not None :\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        mape  = util.MAPE_torch (predict, real, 0.0).item()\n",
    "        rmse  = util.RMSE_torch (predict, real, 0.0).item()\n",
    "        wmape = util.WMAPE_torch(predict, real, 0.0).item()\n",
    "\n",
    "        return loss.item(), mape, rmse, wmape\n",
    "\n",
    "\n",
    "    def eval(self, input, real_val) :\n",
    "\n",
    "        self.model.eval()\n",
    "        output = self.model(input)\n",
    "        output = output.transpose(1, 3)\n",
    "        real = torch.unsqueeze(real_val, dim=1)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "        loss = self.loss(predict, real, 0.0)\n",
    "\n",
    "        mape  = util.MAPE_torch (predict, real, 0.0).item()\n",
    "        rmse  = util.RMSE_torch (predict, real, 0.0).item()\n",
    "        wmape = util.WMAPE_torch(predict, real, 0.0).item()\n",
    "\n",
    "        return loss.item(), mape, rmse, wmape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239d583c-1e26-4410-9daa-3c67c26c9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object) :\n",
    "\n",
    "\n",
    "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True) :\n",
    "\n",
    "        self.batch_size  = batch_size\n",
    "        self.current_ind = 0\n",
    "\n",
    "        if pad_with_last_sample :\n",
    "\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            # print(num_padding)  # 53 +10699=64*168\n",
    "            x_padding   = np.repeat(xs[-1:], num_padding, axis=0)  # Repeat the last sample\n",
    "            # print(x_padding)  # (53, 12, 170, 3)\n",
    "            y_padding   = np.repeat(ys[-1:], num_padding, axis=0)  # y也复制这么多？\n",
    "            # print(y_padding)  # (53, 12, 170, 1)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.size = len(xs)  # 10752\n",
    "        self.num_batch = int(self.size // self.batch_size)  # 168\n",
    "\n",
    "\n",
    "    def shuffle(self) :\n",
    "\n",
    "        permutation = np.random.permutation(self.size)  # 类似句柄？\n",
    "        xs, ys  = self.xs[permutation], self.ys[permutation]\n",
    "        self.xs = xs  # 原始数据就被打乱，而不是以乱取\n",
    "        self.ys = ys\n",
    "\n",
    "\n",
    "    def get_iterator(self) :\n",
    "\n",
    "        self.current_ind = 0  # 批号\n",
    "\n",
    "        def _wrapper() :\n",
    "\n",
    "            while self.current_ind < self.num_batch :\n",
    "\n",
    "                start_ind = self.batch_size*self.current_ind  # ？\n",
    "                end_ind = min(self.size, self.batch_size*(self.current_ind+1))\n",
    "                x_i = self.xs[start_ind:end_ind, ...]\n",
    "                y_i = self.ys[start_ind:end_ind, ...]\n",
    "                yield (x_i, y_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aa41139-1a36-4352-ac0b-68d7bcb7cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler :\n",
    "\n",
    "    def __init__(self, mean, std) :\n",
    "        self.mean = mean  # 均值\n",
    "        self.std = std    # 标准差\n",
    "\n",
    "    def transform(self, data) :\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data) :\n",
    "        return (data * self.std) + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3a7a89d-be9a-4d69-96c4-f1b4c1716168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it(seed) :\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d680393-2fd2-464f-95fc-8c012872ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_it(6666)\n",
    "\n",
    "# [N, D, T] = 170*96*288\n",
    "dataset  = \"PEMS08\"\n",
    "data_dir = \"data//\" + dataset\n",
    "num_nodes = 170\n",
    "channels = 96\n",
    "granularity = 288\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "data = {}\n",
    "\n",
    "for category in [\"train\", \"val\", \"test\"] :\n",
    "    cat_data = np.load(os.path.join(data_dir, category+\".npz\"))  # .npz是NumPy库用来存储多个NumPy数组\n",
    "\n",
    "    # 6=3*2\n",
    "    data[\"x_\" + category] = cat_data[\"x\"]\n",
    "    # print(data[\"x_\" + category].shape)  # (10699/3567/3567, 12, 170, 3)\n",
    "    data[\"y_\" + category] = cat_data[\"y\"]\n",
    "    # print(data[\"y_\" + category].shape)  # (10699/3567/3567, 12, 170, 1)\n",
    "\n",
    "# class\n",
    "scaler = StandardScaler(\n",
    "        # https://blog.csdn.net/g944468183/article/details/124473886\n",
    "        mean = data[\"x_train\"][..., 0].mean(),  # 均以训练集为基准\n",
    "        std  = data[\"x_train\"][..., 0].std()\n",
    "    )\n",
    "# print(data[\"x_train\"][..., 0].shape)  # (10699, 12, 170)\n",
    "# print(data[\"x_train\"][..., 0].mean())  # 229.85893440655073\n",
    "# print(data[\"x_train\"][..., 0].std())   # 145.62268077938813\n",
    "# Data format\n",
    "for category in [\"train\", \"val\", \"test\"] :\n",
    "    data[\"x_\" + category][..., 0] = scaler.transform(data[\"x_\" + category][..., 0])  # 对三个集合的x部分进行缩放\n",
    "\n",
    "random_train = torch.arange(int(data[\"x_train\"].shape[0]))  # 生成一个从0到data[\"x_train\"].shape[0]的序列\n",
    "# print(int(data[\"x_train\"].shape[0]))  # 10699\n",
    "random_train = torch.randperm(random_train.size(0))  # .shape[0]与.size(0)等价，第零维的大小\n",
    "# print(random_train)\n",
    "data[\"x_train\"] = data[\"x_train\"][random_train, ...]  # 四维的第一维重排\n",
    "data[\"y_train\"] = data[\"y_train\"][random_train, ...]\n",
    "\n",
    "random_val = torch.arange(int(data[\"x_val\"].shape[0]))\n",
    "random_val = torch.randperm(random_val.size(0))\n",
    "data[\"x_val\"] = data[\"x_val\"][random_val, ...]\n",
    "data[\"y_val\"] = data[\"y_val\"][random_val, ...]\n",
    "\n",
    "# random_test = torch.arange(int(data['x_test'].shape[0]))\n",
    "# random_test = torch.randperm(random_test.size(0))\n",
    "# data['x_test'] = data['x_test'][random_test, ...]\n",
    "# data['y_test'] = data['y_test'][random_test, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "409721ce-8555-4618-8e3d-a6b411d196ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[]是自己做出来的\n",
    "# print(data)\n",
    "\n",
    "train_batch_size = args.batch_size\n",
    "valid_batch_size = args.batch_size\n",
    "test_batch_size  = args.batch_size\n",
    "# class\n",
    "data[\"train_loader\"] = DataLoader(data[\"x_train\"], data[\"y_train\"], train_batch_size)\n",
    "data[\"val_loader\"]   = DataLoader(data[\"x_val\"]  , data[\"y_val\"]  , valid_batch_size)\n",
    "data[\"test_loader\"]  = DataLoader(data[\"x_test\"] , data[\"y_test\"] ,  test_batch_size)\n",
    "data[\"scaler\"]       = scaler\n",
    "\n",
    "# print(data)  # 浪费内存？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb9622e-1e20-4be5-815f-3c9b1d8dc621",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path) :\n\u001b[1;32m     15\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(path)\n\u001b[0;32m---> 17\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgranularity\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart training...\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mtrainer.__init__\u001b[0;34m(self, scaler, input_dim, num_nodes, channels, dropout, lrate, wdecay, device, granularity)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m      6\u001b[0m     scaler, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     granularity, \n\u001b[1;32m     15\u001b[0m ) :\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m STIDGCN(device, input_dim, num_nodes, channels, granularity, dropout)\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m Ranger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlrate, weight_decay\u001b[38;5;241m=\u001b[39mwdecay)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "loss     = 9999999\n",
    "test_log = 999999\n",
    "epochs_since_best_mae = 0\n",
    "path = args.save + args.dataset + \"/\"\n",
    "\n",
    "his_loss    = []\n",
    "val_time    = []\n",
    "train_time  = []\n",
    "result      = []\n",
    "test_result = []\n",
    "\n",
    "# print(args)\n",
    "\n",
    "if not os.path.exists(path) :\n",
    "    os.makedirs(path)\n",
    "\n",
    "engine = trainer(\n",
    "    scaler, \n",
    "    args.input_dim, \n",
    "    num_nodes, \n",
    "    channels, \n",
    "    args.dropout, \n",
    "    args.learning_rate, \n",
    "    args.weight_decay, \n",
    "    device, \n",
    "    granularity\n",
    ")\n",
    "\n",
    "print(\"start training...\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c8e17-eb80-4c66-90cd-5c0167cb3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(1, args.epochs + 1) :\n",
    "#     train_loss  = []\n",
    "#     train_mape  = []\n",
    "#     train_rmse  = []\n",
    "#     train_wmape = []\n",
    "\n",
    "#     t1 = time.time()\n",
    "\n",
    "#     # dataloader['train_loader'].shuffle()\n",
    "#     for iter, (x, y) in enumerate(dataloader[\"train_loader\"].get_iterator()) :  # 返回批数据\n",
    "\n",
    "#         trainx = torch.Tensor(x).to(device)\n",
    "#         trainx = trainx.transpose(1, 3)\n",
    "#         trainy = torch.Tensor(y).to(device)\n",
    "#         trainy = trainy.transpose(1, 3)\n",
    "\n",
    "#         metrics = engine.train(trainx, trainy[:, 0, :, :])\n",
    "#         train_loss.append(metrics[0])\n",
    "#         train_mape.append(metrics[1])\n",
    "#         train_rmse.append(metrics[2])\n",
    "#         train_wmape.append(metrics[3])\n",
    "\n",
    "#         if iter % args.print_every == 0 :\n",
    "#             log = \"Iter: {:03d}, \\\n",
    "#                    Train Loss: {:.4f}, \\\n",
    "#                    Train RMSE: {:.4f}, \\\n",
    "#                    Train MAPE: {:.4f}, \\\n",
    "#                    Train WMAPE: {:.4f}\"\n",
    "#             print(\n",
    "#                 log.format(\n",
    "#                     iter, \n",
    "#                     train_loss[-1], \n",
    "#                     train_rmse[-1], \n",
    "#                     train_mape[-1], \n",
    "#                     train_wmape[-1], \n",
    "#                 ), \n",
    "#                 flush=True, \n",
    "#             )\n",
    "\n",
    "#     t2 = time.time()\n",
    "#     log = \"Epoch: {:03d}, Training Time: {:.4f} secs\"\n",
    "#     print(log.format(i, (t2 - t1)))\n",
    "#     train_time.append(t2 - t1)\n",
    "\n",
    "#     valid_loss  = []\n",
    "#     valid_mape  = []\n",
    "#     valid_wmape = []\n",
    "#     valid_rmse  = []\n",
    "\n",
    "#     # \n",
    "#     s1 = time.time()\n",
    "\n",
    "#     for iter, (x, y) in enumerate(dataloader[\"val_loader\"].get_iterator()) :\n",
    "\n",
    "#         testx = torch.Tensor(x).to(device)\n",
    "#         testx = testx.transpose(1, 3)\n",
    "#         testy = torch.Tensor(y).to(device)\n",
    "#         testy = testy.transpose(1, 3)\n",
    "\n",
    "#         metrics = engine.eval(testx, testy[:, 0, :, :])\n",
    "#         valid_loss .append(metrics[0])\n",
    "#         valid_mape .append(metrics[1])\n",
    "#         valid_rmse .append(metrics[2])\n",
    "#         valid_wmape.append(metrics[3])\n",
    "\n",
    "#     s2 = time.time()\n",
    "\n",
    "#     log = \"Epoch: {:03d}, Inference Time: {:.4f} secs\"\n",
    "#     print(log.format(i, (s2 - s1)))\n",
    "#     val_time.append(s2 - s1)\n",
    "\n",
    "#     mtrain_loss  = np.mean(train_loss)\n",
    "#     mtrain_mape  = np.mean(train_mape)\n",
    "#     mtrain_wmape = np.mean(train_wmape)\n",
    "#     mtrain_rmse  = np.mean(train_rmse)\n",
    "\n",
    "#     mvalid_loss  = np.mean(valid_loss)\n",
    "#     mvalid_mape  = np.mean(valid_mape)\n",
    "#     mvalid_wmape = np.mean(valid_wmape)\n",
    "#     mvalid_rmse  = np.mean(valid_rmse)\n",
    "\n",
    "#     his_loss.append(mvalid_loss)\n",
    "#     train_m = dict(\n",
    "#         train_loss =np.mean(train_loss), \n",
    "#         train_rmse =np.mean(train_rmse), \n",
    "#         train_mape =np.mean(train_mape), \n",
    "#         train_wmape=np.mean(train_wmape), \n",
    "\n",
    "#         valid_loss =np.mean(valid_loss), \n",
    "#         valid_rmse =np.mean(valid_rmse), \n",
    "#         valid_mape =np.mean(valid_mape), \n",
    "#         valid_wmape=np.mean(valid_wmape), \n",
    "#     )\n",
    "#     train_m = pd.Series(train_m)\n",
    "#     result.append(train_m)\n",
    "\n",
    "#     log = \"Epoch: {:03d}, \\\n",
    "#            Train Loss: {:.4f}, \\\n",
    "#            Train RMSE: {:.4f}, \\\n",
    "#            Train MAPE: {:.4f}, \\\n",
    "#            Train WMAPE: {:.4f}, \"\n",
    "#     print(\n",
    "#         log.format(i, mtrain_loss, mtrain_rmse, mtrain_mape, mtrain_wmape), \n",
    "#         flush=True, \n",
    "#     )\n",
    "#     log = \"Epoch: {:03d}, \\\n",
    "#            Valid Loss: {:.4f}, \\\n",
    "#            Valid RMSE: {:.4f}, \\\n",
    "#            Valid MAPE: {:.4f}, \\\n",
    "#            Valid WMAPE: {:.4f}\"\n",
    "#     print(\n",
    "#         log.format(i, mvalid_loss, mvalid_rmse, mvalid_mape, mvalid_wmape), \n",
    "#         flush=True, \n",
    "#     )\n",
    "\n",
    "#     if mvalid_loss < loss :\n",
    "#         print(\"### Update tasks appear ###\")\n",
    "\n",
    "#         if i < 100 :\n",
    "#             # It is not necessary to print the results of the test set \n",
    "#             # when epoch is less than 100, because the model has not yet converged.\n",
    "#             loss = mvalid_loss\n",
    "#             torch.save(engine.model.state_dict(), path + \"best_model.pth\")\n",
    "#             bestid = i\n",
    "#             epochs_since_best_mae = 0\n",
    "#             print(\"Updating! Valid Loss:\", mvalid_loss, end=\", \")\n",
    "#             print(\"epoch: \", i)\n",
    "\n",
    "#         elif i > 100 :\n",
    "#             outputs = []\n",
    "#             realy = torch.Tensor(dataloader[\"y_test\"]).to(device)\n",
    "#             realy = realy.transpose(1, 3)[:, 0, :, :]\n",
    "\n",
    "#             for iter, (x, y) in enumerate(dataloader[\"test_loader\"].get_iterator()) :\n",
    "#                 testx = torch.Tensor(x).to(device)\n",
    "#                 testx = testx.transpose(1, 3)\n",
    "#                 with torch.no_grad() :\n",
    "#                     preds = engine.model(testx).transpose(1, 3)\n",
    "#                 outputs.append(preds.squeeze())\n",
    "\n",
    "#             yhat = torch.cat(outputs, dim=0)\n",
    "#             yhat = yhat[: realy.size(0), ...]\n",
    "\n",
    "#             amae   = []\n",
    "#             amape  = []\n",
    "#             awmape = []\n",
    "#             armse  = []\n",
    "#             test_m = []\n",
    "\n",
    "#             for j in range(12) :\n",
    "\n",
    "#                 pred = scaler.inverse_transform(yhat[:, :, j])\n",
    "#                 real = realy[:, :, j]\n",
    "#                 metrics = util.metric(pred, real)\n",
    "#                 log = \"Evaluate best model on test data for horizon {:d}, \\\n",
    "#                     Test MAE  : {:.4f}, \\\n",
    "#                     Test RMSE : {:.4f}, \\\n",
    "#                     Test MAPE : {:.4f}, \\\n",
    "#                     Test WMAPE: {:.4f}\"\n",
    "#                 print(\n",
    "#                     log.format(\n",
    "#                         j + 1, metrics[0], metrics[2], metrics[1], metrics[3]\n",
    "#                     )\n",
    "#                 )\n",
    "\n",
    "#                 test_m = dict(\n",
    "#                     test_loss =np.mean(metrics[0]), \n",
    "#                     test_rmse =np.mean(metrics[2]), \n",
    "#                     test_mape =np.mean(metrics[1]), \n",
    "#                     test_wmape=np.mean(metrics[3]), \n",
    "#                 )\n",
    "#                 test_m = pd.Series(test_m)\n",
    "\n",
    "#                 amae  .append(metrics[0])\n",
    "#                 amape .append(metrics[1])\n",
    "#                 armse .append(metrics[2])\n",
    "#                 awmape.append(metrics[3])\n",
    "\n",
    "#             log = \"On average over 12 horizons, \\\n",
    "#                    Test MAE  : {:.4f}, \\\n",
    "#                    Test RMSE : {:.4f}, \\\n",
    "#                    Test MAPE : {:.4f}, \\\n",
    "#                    Test WMAPE: {:.4f}\"\n",
    "#             print(\n",
    "#                 log.format(\n",
    "#                     np.mean(amae), np.mean(armse), np.mean(amape), np.mean(awmape)\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#             if np.mean(amae) < test_log :\n",
    "#                 test_log = np.mean(amae)\n",
    "#                 loss = mvalid_loss\n",
    "#                 torch.save(engine.model.state_dict(), path + \"best_model.pth\")\n",
    "#                 epochs_since_best_mae = 0\n",
    "#                 print(\"Test low! Updating! Test Loss :\", np.mean(amae), end=\", \")\n",
    "#                 print(\"Test low! Updating! Valid Loss:\", mvalid_loss  , end=\", \")\n",
    "#                 bestid = i\n",
    "#                 print(\"epoch: \", i)\n",
    "#             else :\n",
    "#                 epochs_since_best_mae += 1\n",
    "#                 print(\"No update\")\n",
    "\n",
    "#     else :\n",
    "#         epochs_since_best_mae += 1\n",
    "#         print(\"No update\")\n",
    "\n",
    "#     train_csv = pd.DataFrame(result)\n",
    "#     train_csv.round(8).to_csv(f\"{path}/train.csv\")\n",
    "#     if epochs_since_best_mae >= args.es_patience and i >= 300 :\n",
    "#         break\n",
    "\n",
    "# print(\"Average Training Time : {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
    "# print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
    "\n",
    "# print(\"Training ends\")\n",
    "# print(\"The epoch of the best result：\", bestid)\n",
    "# print(\"The valid loss of the best model\", str(round(his_loss[bestid - 1], 4)))\n",
    "\n",
    "# engine.model.load_state_dict(torch.load(path + \"best_model.pth\"))\n",
    "# outputs = []\n",
    "# realy = torch.Tensor(dataloader[\"y_test\"]).to(device)\n",
    "# realy = realy.transpose(1, 3)[:, 0, :, :]\n",
    "\n",
    "# for iter, (x, y) in enumerate(dataloader[\"test_loader\"].get_iterator()) :\n",
    "#     testx = torch.Tensor(x).to(device)\n",
    "#     testx = testx.transpose(1, 3)\n",
    "#     with torch.no_grad() :\n",
    "#         preds = engine.model(testx).transpose(1, 3)\n",
    "#     outputs.append(preds.squeeze())\n",
    "\n",
    "# yhat = torch.cat(outputs, dim=0)\n",
    "# yhat = yhat[: realy.size(0), ...]\n",
    "\n",
    "# amae   = []\n",
    "# amape  = []\n",
    "# armse  = []\n",
    "# awmape = []\n",
    "\n",
    "# test_m = []\n",
    "\n",
    "# for i in range(12) :\n",
    "#     pred = scaler.inverse_transform(yhat[:, :, i])\n",
    "#     real = realy[:, :, i]\n",
    "#     metrics = util.metric(pred, real)\n",
    "#     log = \"Evaluate best model on test data for horizon {:d}, \\\n",
    "#            Test MAE: {:.4f}, \\\n",
    "#            Test RMSE: {:.4f}, \\\n",
    "#            Test MAPE: {:.4f}, \\\n",
    "#            Test WMAPE: {:.4f}\"\n",
    "#     print(log.format(i + 1, metrics[0], metrics[2], metrics[1], metrics[3]))\n",
    "\n",
    "#     test_m = dict(\n",
    "#         test_loss =np.mean(metrics[0]), \n",
    "#         test_rmse =np.mean(metrics[2]), \n",
    "#         test_mape =np.mean(metrics[1]), \n",
    "#         test_wmape=np.mean(metrics[3]), \n",
    "#     )\n",
    "#     test_m = pd.Series(test_m)\n",
    "#     test_result.append(test_m)\n",
    "\n",
    "#     amae  .append(metrics[0])\n",
    "#     amape .append(metrics[1])\n",
    "#     armse .append(metrics[2])\n",
    "#     awmape.append(metrics[3])\n",
    "\n",
    "# log = \"On average over 12 horizons, \\\n",
    "#        Test MAE: {:.4f}, \\\n",
    "#        Test RMSE: {:.4f}, \\\n",
    "#        Test MAPE: {:.4f}, \\\n",
    "#        Test WMAPE: {:.4f}\"\n",
    "# print(log.format(np.mean(amae), np.mean(armse), np.mean(amape), np.mean(awmape)))\n",
    "\n",
    "# test_m = dict(\n",
    "#     test_loss =np.mean(amae), \n",
    "#     test_rmse =np.mean(armse), \n",
    "#     test_mape =np.mean(amape), \n",
    "#     test_wmape=np.mean(awmape), \n",
    "# )\n",
    "# test_m = pd.Series(test_m)\n",
    "# test_result.append(test_m)\n",
    "\n",
    "# test_csv = pd.DataFrame(test_result)\n",
    "# test_csv.round(8).to_csv(f\"{path}/test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STGNN_STIDGCN",
   "language": "python",
   "name": "stgnn_stidgcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
