{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd55038-43bc-4612-b2fe-36a2df8bf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "\n",
    "\n",
    "################################################################\n",
    "# https://github.com/LiuAoyu1998/STIDGCN\n",
    "\n",
    "# .log文件有模型结构\n",
    "################################################################\n",
    "# conda create -n STGNN_STIDGCN\n",
    "# conda activate STGNN_STIDGCN\n",
    "\n",
    "# conda install python=3.8\n",
    "\n",
    "# https://pytorch.org/get-started/previous-versions/\n",
    "# conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "\n",
    "\n",
    "################################################################\n",
    "# pip install pandas\n",
    "# pip install scipy\n",
    "\n",
    "\n",
    "################################################################\n",
    "# conda install ipykernel\n",
    "# conda install platformdirs\n",
    "# pip3 install ipywidgets\n",
    "# pip3 install --upgrade jupyter_core jupyter_client\n",
    "\n",
    "# python -m ipykernel install --user --name STGNN_STIDGCN\n",
    "\n",
    "\n",
    "################################################################\n",
    "# train.py    cuda:0\n",
    "\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b9ec6b-05f9-419d-8d96-8d7b344ebaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # 数据分析\n",
    "import random\n",
    "import time\n",
    "\n",
    "import util\n",
    "from util import *\n",
    "from model import STIDGCN\n",
    "from ranger21 import Ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb32efac-984b-4250-96dc-2861c95b8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--device\",     type=str, default=\"cuda:0\", help=\"\")\n",
    "\n",
    "# (10699/3567/3567, 12, 170, 3)\n",
    "parser.add_argument(\"--dataset\",    type=str, default=\"PEMS08\", help=\"data path\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64,       help=\"batch size\")\n",
    "# Nodes\n",
    "parser.add_argument(\"--input_dim\",  type=int, default=3,        help=\"number of input_dim\")\n",
    "# windows\n",
    "\n",
    "# model parametres\n",
    "parser.add_argument(\"--dropout\",       type=float, default=0.1,    help=\"dropout rate\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001,  help=\"learning rate\")\n",
    "parser.add_argument(\"--weight_decay\",  type=float, default=0.0001, help=\"weight decay rate\")\n",
    "\n",
    "# training parametres\n",
    "parser.add_argument(\"--epochs\",      type=int, default=500, help=\"\")\n",
    "parser.add_argument(\"--print_every\", type=int, default=50,  help=\"\")\n",
    "parser.add_argument(\"--save\",        type=str, default=\"./logs/\" + str(time.strftime(\"%Y-%m-%d-%H:%M:%S\")) + \"-\", help=\"save path\")\n",
    "parser.add_argument(\"--expid\",       type=int, default=1,   help=\"experiment id\")\n",
    "parser.add_argument(\"--es_patience\", type=int, default=100, help=\"quit if no improvement after this many iterations\")\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab5b56f-51c6-4b82-a8cd-a0550cff08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it(seed) :\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda117bf-9c5a-44f2-aed7-969f16814983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer :\n",
    "\n",
    "\n",
    "    def __init__(self, device, num_nodes, input_dim, channels, day_granularity, \n",
    "                 dropout, lrate, wdecay, scaler) :\n",
    "\n",
    "        self.model = STIDGCN(device, num_nodes, input_dim, channels, day_granularity, dropout)\n",
    "        self.model.to(device)\n",
    "\n",
    "        # self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        self.optimizer = Ranger(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "\n",
    "        self.loss   = util.MAE_torch\n",
    "        self.scaler = scaler\n",
    "        self.clip   = 5\n",
    "\n",
    "        print(\"The number of parameters: {}\".format(self.model.param_num()))\n",
    "        # print(self.model)\n",
    "\n",
    "\n",
    "    def train(self, input, real_val) :\n",
    "        # [64, 3, 170, 12]\n",
    "        # [64, 3, 170, 12]\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output = self.model(input)\n",
    "        output = output.transpose(1, 3)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "\n",
    "        real = torch.unsqueeze(real_val, dim=1)\n",
    "\n",
    "        loss = self.loss(predict, real, 0.0)\n",
    "        loss.backward()\n",
    "\n",
    "        if self.clip is not None :  # 5\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        mape  = util.MAPE_torch (predict, real, 0.0).item()\n",
    "        rmse  = util.RMSE_torch (predict, real, 0.0).item()\n",
    "        wmape = util.WMAPE_torch(predict, real, 0.0).item()\n",
    "\n",
    "        return loss.item(), mape, rmse, wmape\n",
    "\n",
    "\n",
    "    def eval(self, input, real_val) :\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        output = self.model(input)\n",
    "        output = output.transpose(1, 3)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "\n",
    "        real = torch.unsqueeze(real_val, dim=1)\n",
    "\n",
    "        loss = self.loss(predict, real, 0.0)\n",
    "\n",
    "        mape  = util.MAPE_torch (predict, real, 0.0).item()\n",
    "        rmse  = util.RMSE_torch (predict, real, 0.0).item()\n",
    "        wmape = util.WMAPE_torch(predict, real, 0.0).item()\n",
    "\n",
    "        return loss.item(), mape, rmse, wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d680393-2fd2-464f-95fc-8c012872ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform shuffle on the dataset\n"
     ]
    }
   ],
   "source": [
    "seed_it(6666)\n",
    "\n",
    "# [N, D, T] = 170*3*12\n",
    "# [N, D, T] = 170*96*288\n",
    "dataset  = \"PEMS08\"\n",
    "data_dir = \"data//\" + dataset\n",
    "num_nodes = 170\n",
    "# features = 3\n",
    "# win\n",
    "channels = 96\n",
    "day_granularity = 288\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "dataloader = util.load_dataset(data_dir, args.batch_size, args.batch_size, args.batch_size)\n",
    "\n",
    "scaler = dataloader[\"scaler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb9622e-1e20-4be5-815f-3c9b1d8dc621",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m test_result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday_granularity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                 \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart training...\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtrainer.__init__\u001b[0;34m(self, device, num_nodes, input_dim, channels, day_granularity, dropout, lrate, wdecay, scaler)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, device, num_nodes, input_dim, channels, day_granularity, \n\u001b[1;32m      5\u001b[0m              dropout, lrate, wdecay, scaler) :\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m STIDGCN(device, num_nodes, input_dim, channels, day_granularity, dropout)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m Ranger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlrate, weight_decay\u001b[38;5;241m=\u001b[39mwdecay)\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/STGNN_STIDGCN/lib/python3.8/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "# \n",
    "test_log = 999999\n",
    "epochs_since_best_mae = 0\n",
    "path = args.save + args.dataset + \"/\"\n",
    "if not os.path.exists(path) :\n",
    "    os.makedirs(path)\n",
    "\n",
    "# \n",
    "loss        = 9999999\n",
    "his_loss    = []\n",
    "train_time  = []\n",
    "val_time    = []\n",
    "result      = []\n",
    "test_result = []\n",
    "\n",
    "# \n",
    "engine = trainer(device, num_nodes, args.input_dim, channels, day_granularity, \n",
    "                 args.dropout, args.learning_rate, args.weight_decay, scaler)\n",
    "\n",
    "print(\"start training...\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9e30f-0088-4dd4-b0cd-6d1e077f8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: [64, 12, 170, 3]\n",
    "# STIDGCN(\n",
    "#   (Temb): TemporalEmbedding()\n",
    "#   (start_conv): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1))\n",
    "#   (tree): IDGCN_Tree(\n",
    "#     (IDGCN1): IDGCN(\n",
    "#       (split): Splitting()\n",
    "#       (conv1): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv2): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv3): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv4): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (dgcn): DGCN(\n",
    "#         (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#         (generator): Graph_Generator(\n",
    "#           (fc): Linear(in_features=2, out_features=1, bias=True)\n",
    "#         )\n",
    "#         (gcn): Diffusion_GCN(\n",
    "#           (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#           (dropout): Dropout(p=0.1, inplace=False)\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#     (IDGCN2): IDGCN(\n",
    "#       (split): Splitting()\n",
    "#       (conv1): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv2): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv3): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv4): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (dgcn): DGCN(\n",
    "#         (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#         (generator): Graph_Generator(\n",
    "#           (fc): Linear(in_features=2, out_features=1, bias=True)\n",
    "#         )\n",
    "#         (gcn): Diffusion_GCN(\n",
    "#           (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#           (dropout): Dropout(p=0.1, inplace=False)\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#     (IDGCN3): IDGCN(\n",
    "#       (split): Splitting()\n",
    "#       (conv1): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv2): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv3): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (conv4): Sequential(\n",
    "#         (0): ReplicationPad2d((3, 3, 0, 0))\n",
    "#         (1): Conv2d(192, 192, kernel_size=(1, 5), stride=(1, 1))\n",
    "#         (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "#         (3): Dropout(p=0.1, inplace=False)\n",
    "#         (4): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1))\n",
    "#         (5): Tanh()\n",
    "#       )\n",
    "#       (dgcn): DGCN(\n",
    "#         (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#         (generator): Graph_Generator(\n",
    "#           (fc): Linear(in_features=2, out_features=1, bias=True)\n",
    "#         )\n",
    "#         (gcn): Diffusion_GCN(\n",
    "#           (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#           (dropout): Dropout(p=0.1, inplace=False)\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#   )\n",
    "#   (glu): GLU(\n",
    "#     (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#     (conv2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#     (conv3): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "#     (dropout): Dropout(p=0.1, inplace=False)\n",
    "#   )\n",
    "#   (regression_layer): Conv2d(192, 12, kernel_size=(1, 12), stride=(1, 1))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d15ef4-b60a-492b-a606-658ec69083c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, args.epochs + 1) :\n",
    "\n",
    "    train_loss  = []\n",
    "    train_mape  = []\n",
    "    train_rmse  = []\n",
    "    train_wmape = []\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    for iter, (x, y) in enumerate(dataloader[\"train_loader\"].get_iterator()) :  # 枚举返回批序号、批数据\n",
    "\n",
    "        trainx = torch.Tensor(x).to(device)  # [64, 12, 170, 3]\n",
    "        trainx = trainx.transpose(1, 3)      # [64, 3, 170, 12]\n",
    "        trainy = torch.Tensor(y).to(device)  # [64, 12, 170, 1]\n",
    "        trainy = trainy.transpose(1, 3)      # [64, 1, 170, 12]\n",
    "\n",
    "        print(trainy[:, 0, :, :].shape)\n",
    "        metrics = engine.train(trainx, trainy[:, 0, :, :])\n",
    "        train_loss.append(metrics[0])\n",
    "        train_mape.append(metrics[1])\n",
    "        train_rmse.append(metrics[2])\n",
    "        train_wmape.append(metrics[3])\n",
    "\n",
    "        if iter % args.print_every == 0 :\n",
    "            log = \"Iter: {:03d}, \\\n",
    "                   Train Loss: {:.4f}, \\\n",
    "                   Train RMSE: {:.4f}, \\\n",
    "                   Train MAPE: {:.4f}, \\\n",
    "                   Train WMAPE: {:.4f}\"\n",
    "            print(\n",
    "                log.format(\n",
    "                    iter, \n",
    "                    train_loss[-1], \n",
    "                    train_rmse[-1], \n",
    "                    train_mape[-1], \n",
    "                    train_wmape[-1], \n",
    "                ), \n",
    "                flush=True, \n",
    "            )\n",
    "\n",
    "    t2 = time.time()\n",
    "    log = \"Epoch: {:03d}, Training Time: {:.4f} secs\"\n",
    "    print(log.format(i, (t2 - t1)))\n",
    "    train_time.append(t2 - t1)\n",
    "\n",
    "    valid_loss  = []\n",
    "    valid_mape  = []\n",
    "    valid_wmape = []\n",
    "    valid_rmse  = []\n",
    "\n",
    "    # \n",
    "    s1 = time.time()\n",
    "\n",
    "    for iter, (x, y) in enumerate(dataloader[\"val_loader\"].get_iterator()) :\n",
    "\n",
    "        testx = torch.Tensor(x).to(device)\n",
    "        testx = testx.transpose(1, 3)\n",
    "        testy = torch.Tensor(y).to(device)\n",
    "        testy = testy.transpose(1, 3)\n",
    "\n",
    "        metrics = engine.eval(testx, testy[:, 0, :, :])\n",
    "        valid_loss .append(metrics[0])\n",
    "        valid_mape .append(metrics[1])\n",
    "        valid_rmse .append(metrics[2])\n",
    "        valid_wmape.append(metrics[3])\n",
    "\n",
    "    s2 = time.time()\n",
    "\n",
    "    log = \"Epoch: {:03d}, Inference Time: {:.4f} secs\"\n",
    "    print(log.format(i, (s2 - s1)))\n",
    "    val_time.append(s2 - s1)\n",
    "\n",
    "    mtrain_loss  = np.mean(train_loss)\n",
    "    mtrain_mape  = np.mean(train_mape)\n",
    "    mtrain_wmape = np.mean(train_wmape)\n",
    "    mtrain_rmse  = np.mean(train_rmse)\n",
    "\n",
    "    mvalid_loss  = np.mean(valid_loss)\n",
    "    mvalid_mape  = np.mean(valid_mape)\n",
    "    mvalid_wmape = np.mean(valid_wmape)\n",
    "    mvalid_rmse  = np.mean(valid_rmse)\n",
    "\n",
    "    his_loss.append(mvalid_loss)\n",
    "    train_m = dict(\n",
    "        train_loss =np.mean(train_loss), \n",
    "        train_rmse =np.mean(train_rmse), \n",
    "        train_mape =np.mean(train_mape), \n",
    "        train_wmape=np.mean(train_wmape), \n",
    "\n",
    "        valid_loss =np.mean(valid_loss), \n",
    "        valid_rmse =np.mean(valid_rmse), \n",
    "        valid_mape =np.mean(valid_mape), \n",
    "        valid_wmape=np.mean(valid_wmape), \n",
    "    )\n",
    "    train_m = pd.Series(train_m)\n",
    "    result.append(train_m)\n",
    "\n",
    "    log = \"Epoch: {:03d}, \\\n",
    "           Train Loss: {:.4f}, \\\n",
    "           Train RMSE: {:.4f}, \\\n",
    "           Train MAPE: {:.4f}, \\\n",
    "           Train WMAPE: {:.4f}, \"\n",
    "    print(\n",
    "        log.format(i, mtrain_loss, mtrain_rmse, mtrain_mape, mtrain_wmape), \n",
    "        flush=True, \n",
    "    )\n",
    "    log = \"Epoch: {:03d}, \\\n",
    "           Valid Loss: {:.4f}, \\\n",
    "           Valid RMSE: {:.4f}, \\\n",
    "           Valid MAPE: {:.4f}, \\\n",
    "           Valid WMAPE: {:.4f}\"\n",
    "    print(\n",
    "        log.format(i, mvalid_loss, mvalid_rmse, mvalid_mape, mvalid_wmape), \n",
    "        flush=True, \n",
    "    )\n",
    "\n",
    "    if mvalid_loss < loss :\n",
    "        print(\"### Update tasks appear ###\")\n",
    "\n",
    "        if i < 100 :\n",
    "            # It is not necessary to print the results of the test set \n",
    "            # when epoch is less than 100, because the model has not yet converged.\n",
    "            loss = mvalid_loss\n",
    "            torch.save(engine.model.state_dict(), path + \"best_model.pth\")\n",
    "            bestid = i\n",
    "            epochs_since_best_mae = 0\n",
    "            print(\"Updating! Valid Loss:\", mvalid_loss, end=\", \")\n",
    "            print(\"epoch: \", i)\n",
    "\n",
    "        elif i > 100 :\n",
    "            outputs = []\n",
    "            realy = torch.Tensor(dataloader[\"y_test\"]).to(device)\n",
    "            realy = realy.transpose(1, 3)[:, 0, :, :]\n",
    "\n",
    "            for iter, (x, y) in enumerate(dataloader[\"test_loader\"].get_iterator()) :\n",
    "                testx = torch.Tensor(x).to(device)\n",
    "                testx = testx.transpose(1, 3)\n",
    "                with torch.no_grad() :\n",
    "                    preds = engine.model(testx).transpose(1, 3)\n",
    "                outputs.append(preds.squeeze())\n",
    "\n",
    "            yhat = torch.cat(outputs, dim=0)\n",
    "            yhat = yhat[: realy.size(0), ...]\n",
    "\n",
    "            amae   = []\n",
    "            amape  = []\n",
    "            awmape = []\n",
    "            armse  = []\n",
    "            test_m = []\n",
    "\n",
    "            for j in range(12) :\n",
    "\n",
    "                pred = scaler.inverse_transform(yhat[:, :, j])\n",
    "                real = realy[:, :, j]\n",
    "                metrics = util.metric(pred, real)\n",
    "                log = \"Evaluate best model on test data for horizon {:d}, \\\n",
    "                    Test MAE  : {:.4f}, \\\n",
    "                    Test RMSE : {:.4f}, \\\n",
    "                    Test MAPE : {:.4f}, \\\n",
    "                    Test WMAPE: {:.4f}\"\n",
    "                print(\n",
    "                    log.format(\n",
    "                        j + 1, metrics[0], metrics[2], metrics[1], metrics[3]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                test_m = dict(\n",
    "                    test_loss =np.mean(metrics[0]), \n",
    "                    test_rmse =np.mean(metrics[2]), \n",
    "                    test_mape =np.mean(metrics[1]), \n",
    "                    test_wmape=np.mean(metrics[3]), \n",
    "                )\n",
    "                test_m = pd.Series(test_m)\n",
    "\n",
    "                amae  .append(metrics[0])\n",
    "                amape .append(metrics[1])\n",
    "                armse .append(metrics[2])\n",
    "                awmape.append(metrics[3])\n",
    "\n",
    "            log = \"On average over 12 horizons, \\\n",
    "                   Test MAE  : {:.4f}, \\\n",
    "                   Test RMSE : {:.4f}, \\\n",
    "                   Test MAPE : {:.4f}, \\\n",
    "                   Test WMAPE: {:.4f}\"\n",
    "            print(\n",
    "                log.format(\n",
    "                    np.mean(amae), np.mean(armse), np.mean(amape), np.mean(awmape)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if np.mean(amae) < test_log :\n",
    "                test_log = np.mean(amae)\n",
    "                loss = mvalid_loss\n",
    "                torch.save(engine.model.state_dict(), path + \"best_model.pth\")\n",
    "                epochs_since_best_mae = 0\n",
    "                print(\"Test low! Updating! Test Loss :\", np.mean(amae), end=\", \")\n",
    "                print(\"Test low! Updating! Valid Loss:\", mvalid_loss  , end=\", \")\n",
    "                bestid = i\n",
    "                print(\"epoch: \", i)\n",
    "            else :\n",
    "                epochs_since_best_mae += 1\n",
    "                print(\"No update\")\n",
    "\n",
    "    else :\n",
    "        epochs_since_best_mae += 1\n",
    "        print(\"No update\")\n",
    "\n",
    "    train_csv = pd.DataFrame(result)\n",
    "    train_csv.round(8).to_csv(f\"{path}/train.csv\")\n",
    "    if epochs_since_best_mae >= args.es_patience and i >= 300 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c8e17-eb80-4c66-90cd-5c0167cb3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(\"Average Training Time : {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
    "# print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
    "\n",
    "# print(\"Training ends\")\n",
    "# print(\"The epoch of the best result：\", bestid)\n",
    "# print(\"The valid loss of the best model\", str(round(his_loss[bestid - 1], 4)))\n",
    "\n",
    "# engine.model.load_state_dict(torch.load(path + \"best_model.pth\"))\n",
    "# outputs = []\n",
    "# realy = torch.Tensor(dataloader[\"y_test\"]).to(device)\n",
    "# realy = realy.transpose(1, 3)[:, 0, :, :]\n",
    "\n",
    "# for iter, (x, y) in enumerate(dataloader[\"test_loader\"].get_iterator()) :\n",
    "#     testx = torch.Tensor(x).to(device)\n",
    "#     testx = testx.transpose(1, 3)\n",
    "#     with torch.no_grad() :\n",
    "#         preds = engine.model(testx).transpose(1, 3)\n",
    "#     outputs.append(preds.squeeze())\n",
    "\n",
    "# yhat = torch.cat(outputs, dim=0)\n",
    "# yhat = yhat[: realy.size(0), ...]\n",
    "\n",
    "# amae   = []\n",
    "# amape  = []\n",
    "# armse  = []\n",
    "# awmape = []\n",
    "\n",
    "# test_m = []\n",
    "\n",
    "# for i in range(12) :\n",
    "#     pred = scaler.inverse_transform(yhat[:, :, i])\n",
    "#     real = realy[:, :, i]\n",
    "#     metrics = util.metric(pred, real)\n",
    "#     log = \"Evaluate best model on test data for horizon {:d}, \\\n",
    "#            Test MAE: {:.4f}, \\\n",
    "#            Test RMSE: {:.4f}, \\\n",
    "#            Test MAPE: {:.4f}, \\\n",
    "#            Test WMAPE: {:.4f}\"\n",
    "#     print(log.format(i + 1, metrics[0], metrics[2], metrics[1], metrics[3]))\n",
    "\n",
    "#     test_m = dict(\n",
    "#         test_loss =np.mean(metrics[0]), \n",
    "#         test_rmse =np.mean(metrics[2]), \n",
    "#         test_mape =np.mean(metrics[1]), \n",
    "#         test_wmape=np.mean(metrics[3]), \n",
    "#     )\n",
    "#     test_m = pd.Series(test_m)\n",
    "#     test_result.append(test_m)\n",
    "\n",
    "#     amae  .append(metrics[0])\n",
    "#     amape .append(metrics[1])\n",
    "#     armse .append(metrics[2])\n",
    "#     awmape.append(metrics[3])\n",
    "\n",
    "# log = \"On average over 12 horizons, \\\n",
    "#        Test MAE: {:.4f}, \\\n",
    "#        Test RMSE: {:.4f}, \\\n",
    "#        Test MAPE: {:.4f}, \\\n",
    "#        Test WMAPE: {:.4f}\"\n",
    "# print(log.format(np.mean(amae), np.mean(armse), np.mean(amape), np.mean(awmape)))\n",
    "\n",
    "# test_m = dict(\n",
    "#     test_loss =np.mean(amae), \n",
    "#     test_rmse =np.mean(armse), \n",
    "#     test_mape =np.mean(amape), \n",
    "#     test_wmape=np.mean(awmape), \n",
    "# )\n",
    "# test_m = pd.Series(test_m)\n",
    "# test_result.append(test_m)\n",
    "\n",
    "# test_csv = pd.DataFrame(test_result)\n",
    "# test_csv.round(8).to_csv(f\"{path}/test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STGNN_STIDGCN",
   "language": "python",
   "name": "stgnn_stidgcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
